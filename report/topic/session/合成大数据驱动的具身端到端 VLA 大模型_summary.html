
        <!DOCTYPE html>
        <html lang="zh-TW">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>合成大数据驱动的具身端到端 Vla 大模型</title>
            <style>
                body {
                    font-family: 'Roboto', 'Microsoft JhengHei', sans-serif;
                    background-color: #f8f9fa;
                    color: #333;
                    margin: 0;
                    padding: 0;
                    line-height: 1.6;
                }
                header {
                    text-align: center;
                    padding: 30px 20px;
                    background: linear-gradient(135deg, #4b6cb7, #182848);
                    color: #fff;
                    position: relative;
                    box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
                    border-radius: 10px 10px 0 0;
                }
                .video-link {
                    display: inline-block;
                    margin: 10px 0;
                    padding: 8px 15px;
                    background-color: rgba(255, 255, 255, 0.2);
                    color: #fff;
                    text-decoration: none;
                    border-radius: 5px;
                    font-weight: 500;
                    transition: all 0.3s ease;
                    border: 1px solid rgba(255, 255, 255, 0.3);
                }
                .video-link:hover {
                    background-color: rgba(255, 255, 255, 0.3);
                    transform: translateY(-2px);
                    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
                }
                .video-link i {
                    margin-right: 5px;
                }
                header h1 {
                    font-size: 2.2rem;
                    margin: 0 0 10px 0;
                    letter-spacing: 0.5px;
                }
                header p {
                    font-size: 1.1rem;
                    margin: 0;
                    opacity: 0.9;
                }
                header img {
                    max-width: 35%;
                    height: auto;
                    margin-bottom: 15px;
                }
                main {
                    max-width: 900px;
                    margin: 30px auto;
                    padding: 0 20px;
                }
                .content-container {
                    background-color: #fff;
                    border-radius: 10px;
                    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);
                    padding: 30px;
                    margin-bottom: 30px;
                }
                .section-block {
                    background-color: #fff;
                    padding: 25px;
                    border-radius: 8px;
                    margin-bottom: 25px;
                    border-left: 5px solid #4b6cb7;
                    box-shadow: 0 3px 10px rgba(0, 0, 0, 0.05);
                    transition: transform 0.2s ease, box-shadow 0.2s ease;
                }
                .section-block:hover {
                    transform: translateY(-3px);
                    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
                }
                /* 為不同段落設置不同的邊框顏色和背景 */
                .section-block.conclusion {
                    border-left-color: #e74c3c;
                    background-color: #fff9f9;
                }
                .section-block.meeting-topic {
                    border-left-color: #3498db;
                    background-color: #f7fbff;
                }
                .section-block.tech-points {
                    border-left-color: #2ecc71;
                    background-color: #f7fff9;
                }
                .section-block.decisions {
                    border-left-color: #f39c12;
                    background-color: #fffaf5;
                }
                .section-block.timeline {
                    border-left-color: #9b59b6;
                    background-color: #faf5ff;
                }
                .section-block.challenges {
                    border-left-color: #e67e22;
                    background-color: #fff8f2;
                }
                .section-block.action-plan {
                    border-left-color: #1abc9c;
                    background-color: #f5fffc;
                }
                .section-title {
                    font-size: 1.5rem;
                    margin-top: 0;
                    margin-bottom: 20px;
                    padding-bottom: 10px;
                    border-bottom: 2px solid #4b6cb7;
                    display: inline-block;
                }
                /* 為不同段落標題設置對應的顏色 */
                .section-block.conclusion .section-title {
                    color: #e74c3c;
                    border-bottom-color: #e74c3c;
                }
                .section-block.meeting-topic .section-title {
                    color: #3498db;
                    border-bottom-color: #3498db;
                }
                .section-block.tech-points .section-title {
                    color: #2ecc71;
                    border-bottom-color: #2ecc71;
                }
                .section-block.decisions .section-title {
                    color: #f39c12;
                    border-bottom-color: #f39c12;
                }
                .section-block.timeline .section-title {
                    color: #9b59b6;
                    border-bottom-color: #9b59b6;
                }
                .section-block.challenges .section-title {
                    color: #e67e22;
                    border-bottom-color: #e67e22;
                }
                .section-block.action-plan .section-title {
                    color: #1abc9c;
                    border-bottom-color: #1abc9c;
                }
                .content-container ul {
                    padding-left: 20px;
                }
                .content-container li {
                    margin-bottom: 10px;
                    position: relative;
                    list-style-type: none;
                    padding-left: 25px;
                }
                .content-container li::before {
                    content: "•";
                    position: absolute;
                    left: 0;
                    color: #4b6cb7;
                    font-size: 1.2rem;
                    font-weight: bold;
                }
                .section-block.conclusion li::before { color: #e74c3c; }
                .section-block.meeting-topic li::before { color: #3498db; }
                .section-block.tech-points li::before { color: #2ecc71; }
                .section-block.decisions li::before { color: #f39c12; }
                .section-block.timeline li::before { color: #9b59b6; }
                .section-block.challenges li::before { color: #e67e22; }
                .section-block.action-plan li::before { color: #1abc9c; }
                
                .content-container p {
                    margin-bottom: 15px;
                    line-height: 1.7;
                }
                .content-container strong {
                    color: #2c3e50;
                    font-weight: bold;
                }
                footer {
                    text-align: center;
                    padding: 25px 20px;
                    background-color: #f0f2f5;
                    color: #666;
                    border-radius: 0 0 10px 10px;
                    border-top: 1px solid #e0e0e0;
                }
                footer img {
                    max-width: 35%;
                    height: auto;
                    margin-bottom: 15px;
                }
                footer p {
                    margin: 5px 0;
                    font-size: 0.95rem;
                }
                footer a {
                    color: #4b6cb7;
                    text-decoration: none;
                    font-weight: bold;
                }
                footer a:hover {
                    text-decoration: underline;
                }
            </style>
        </head>
        <body>
            <header>
                <img src="https://i.imgur.com/0LXUWvj.png" alt="會議摘要圖示">
                <h1>合成大数据驱动的具身端到端 Vla 大模型</h1>
                <a href="https://www.nvidia.com/gtc/session-catalog/?search=%E5%90%88%E6%88%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%85%B7%E8%BA%AB%E7%AB%AF%E5%88%B0%E7%AB%AF%20VLA%20%E5%A4%A7%E6%A8%A1%E5%9E%8B&search=%E5%90%88%E6%88%90%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E7%9A%84%E5%85%B7%E8%BA%AB%E7%AB%AF%E5%88%B0%E7%AB%AF+VLA+%E5%A4%A7%E6%A8%A1%E5%9E%8B&tab.catalogallsessionstab=16566177511100015Kus#/session/1726155424858001HJ8h" class="video-link" target="_blank"><i>▶</i> 會議影片超連結</a>
            </header>
            <main>
                <div class="content-container">
                    <div class="section-block default"><h1>合成大數據驅動的具身端到端 VLA 大模型</h1></div><div class="section-block conclusion"><h2 class="section-title">Key Takeaways</h2><p>本次會議主要介紹 Galbot 公司在具身通用機器人領域的最新進展，重點探討了如何利用合成數據來驅動端到端的視覺語言動作（VLA）大模型，以賦能通用機器人。</p><h3>重點摘要：</h3><ul><li>通用機器人相較於特定用途機器人的革命性意義，以及其在各個領域的應用前景。</li><li>端到端 VLA 模型在視覺感知、任務規劃和動作生成方面的優勢，以及其與其他大型模型的比較。</li><li>數據收集是 VLA 模型發展的瓶頸，以及合成數據在解決數據瓶頸方面的潛力。</li><li>Galbot 公司在合成數據方面的努力，包括 Gaparnet、Dexgrasp Net 和模擬深度傳感器。</li><li>基於合成數據訓練的視覺語言導航（VLN）模型 NAVID 和 NAVID-4D，以及其在真實環境中的應用。</li><li>基於合成數據訓練的抓取 VLA 模型，以及其在不同環境和物體上的泛化能力。</li><li>合成數據預訓練和少量真實數據後訓練相結合的訓練範式，以及其在降低數據成本和提高模型性能方面的優勢。</li></ul><h3>Topic:</h3><ul><li>具身智能</li><li>通用機器人</li><li>視覺語言動作模型</li><li>合成數據</li></ul></div><div class="section-block meeting-topic"><h2 class="section-title">會議主題</h2><p>會議主要探討了如何利用合成數據來訓練具身端到端 VLA 大模型，以賦能通用機器人，使其能夠理解人類指令並執行各種任務，包括導航和抓取。</p></div><div class="section-block tech-points"><h2 class="section-title">主要技術點</h2><ul><li><strong>端到端 VLA 模型：</strong> 該模型以語言、視覺和其他感知信號作為輸入，輸出動作，能夠處理視覺感知、任務規劃和動作生成。</li><li><strong>合成數據：</strong> 利用計算機圖形學技術生成大量標註數據，用於訓練 VLA 模型，解決真實數據收集成本高昂的問題。</li><li><strong>Gaparnet：</strong> 一個包含大量可操作關節物體的數據集，用於訓練 VLA 模型理解物體的結構和功能。</li><li><strong>Dexgrasp Net：</strong> 一個包含百萬級靈巧抓取數據的數據集，用於訓練 VLA 模型學習靈巧抓取技能。</li><li><strong>NAVID 和 NAVID-4D：</strong> 基於合成數據訓練的視覺語言導航模型，能夠理解人類指令並在複雜環境中導航。NAVID-4D 增加了深度信息作為輸入，提高了空間推理能力。</li><li><strong>抓取 VLA 模型：</strong> 基於合成數據訓練的抓取模型，能夠在不同環境和物體上執行抓取任務，並具有良好的泛化能力。</li><li><strong>Sim-to-Real：</strong> 將在模擬環境中訓練的模型遷移到真實環境中，以降低數據成本和提高模型性能。</li><li><strong>預訓練和後訓練：</strong> 首先在合成數據上進行預訓練，然後在少量真實數據上進行後訓練，以提高模型在真實環境中的性能。</li></ul></div><div class="section-block decisions"><h2 class="section-title">決策與共識</h2><ul><li>合成數據是解決 VLA 模型數據瓶頸的有效途徑。</li><li>預訓練和後訓練相結合的訓練範式能夠降低數據成本和提高模型性能。</li><li>Galbot 公司將繼續投入合成數據的研發，以推動通用機器人的發展。</li></ul></div><div class="section-block timeline"><h2 class="section-title">時間規劃與里程碑</h2><ul><li>2023 年 5 月，Galbot 公司成立。</li><li>2023 年，發表 Gaparnet 論文。</li><li>2024 年，發表 NAVID 論文。</li><li>2025 年，發表 NAVID-4D 論文。</li><li>持續開發和部署基於 VLA 模型的通用機器人，應用於零售、醫療和工業等領域。</li></ul></div><div class="section-block challenges"><h2 class="section-title">未解決的技術挑戰</h2><ul><li>如何進一步提高合成數據的真實性，以縮小模擬環境和真實環境之間的差距。</li><li>如何設計更有效的後訓練方法，以利用少量真實數據提高模型性能。</li><li>如何將 VLA 模型應用於更複雜的任務，例如操作和協作。</li></ul></div><div class="section-block action-plan"><h2 class="section-title">後續行動計劃</h2><ul><li>繼續擴充和完善合成數據集，包括 Gaparnet 和 Dexgrasp Net。</li><li>開發更先進的 VLA 模型，提高其在不同任務和環境中的泛化能力。</li><li>將 VLA 模型應用於更多實際場景，例如零售、醫療和工業。</li><li>與其他公司和研究機構合作，共同推動通用機器人的發展。</li></ul></div>
                </div>
            </main>
            <footer>
                <img src="https://i.imgur.com/0LXUWvj.png" alt="會議摘要圖示">
                <p>此摘要由 AI 輔助生成</p>
                <p>如有任何問題或需要更多詳細資訊，請聯繫 ITR 小組</p>
            </footer>
        </body>
        </html>
        