# LLM 2-bit 後量化的加速與部署實踐
[會議影片連結](https://www.nvidia.com/gtc/session-catalog/?search=LLM%202-bit%20%E5%90%8E%E9%87%8F%E5%8C%96%E7%9A%84%E5%8A%A0%E9%80%9F%E4%B8%8E%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5&tab.catalogallsessionstab=16566177511100015Kus#/session/1727624187326001lFnY)
# LLM 2-bit 後量化的加速與部署實踐

## Key Takeaways
本次會議主要分享了字節跳動語音團隊在大語音模型下，針對兩比特權重壓縮方案（DecoupleQ）以及基於 TRT-LM 定制的兩比特 Kernel 的實現和優化。
### 重點摘要：
*   介紹了 DecoupleQ 的兩比特量化方案的實現原理。
*   分享了基於 TRT-LM 的兩比特算子實現。
*   闡述了兩比特在字節語音內部的部署方案。
### Topic: 量化、模型壓縮、TRT-LM、部署優化

## 會議主題
會議主要探討了如何通過兩比特量化方案（DecoupleQ）壓縮大語音模型，並基於 TRT-LM 進行定制化 Kernel 的實現和優化，最終在字節跳動語音場景下進行部署。

## 主要技術點
*   **DecoupleQ 量化算法：** 將模型參數分解為整數部分 W 和浮點部分 S、Z，將量化問題轉化為有約束的優化問題，不再關注量化的各種細節和區域，而只需要關注求解這個優化問題。
*   **TRT-LM 兩比特算子實現：** 分為預處理階段和 Kernel 運行階段。預處理階段將量化工具產出的兩比特模型權重轉換成 Kernel 支持的格式，包括 K 維度上的 Transpose、數據布局轉換（NK -> N/4 K/64 4 64 或 NK -> N/8 K/64 8 64）以及將有符號數轉換成無符號數。Kernel 運行階段從 Global Memory 加載權重到 Shared Memory，再到寄存器，然後在寄存器中將數據轉換成 BF16，再反量化，最後做 BF16 的 MMA 矩陣乘法。
*   **W2 到 FP16 的快速轉換邏輯：** 通過 LOB3 指令與 0x003003 做運算得到第 0 個和第 1 個元素，再與 0x64006400 做或運算，將元素轉換成加 1024 之後 FP16 對應的數據表示，最後減去 1026 得到原始數據對應的 FP16 表示。
*   **部署方案選擇：** 根據部署硬件的計算密度和業務的特定場景（低延遲或高吞吐）選擇不同的部署方案。

## 決策與共識
*   採用 DecoupleQ 量化算法進行模型壓縮。
*   基於 TRT-LM 定制兩比特算子，並進行預處理和優化。
*   根據業務特點和硬件特性選擇合適的部署方案。

## 時間規劃與里程碑
*   通過量化，目前數據已經可以做到單卡的 7 到 8 個小時就可以做到無損。

## 未解決的技術挑戰
*   無。

## 後續行動計劃
*   將 TRT-LM 當做一個算子庫接入到內部的高性能推理引擎中。
*   使用 DecoupleQ 這樣的 W2A16G64 的配置進行量化，在語音 ASR 的 W2E2 指標上做到基本無損。
