# 构建以 Megatron-Core 为核心的大语言模型训练加速生态
[會議影片連結](https://www.nvidia.com/gtc/session-catalog/?search=%E6%9E%84%E5%BB%BA%E4%BB%A5%20Megatron-Core%20%E4%B8%BA%E6%A0%B8%E5%BF%83%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F%E7%94%9F%E6%80%81&tab.catalogallsessionstab=16566177511100015Kus#/session/1727454091122001Qxbv)
# 構建以 Megatron-Core 為核心的大語言模型訓練加速生態

## Key Takeaways
本次會議主要介紹了阿里云人工智能平台PAI團隊與英偉達Megatron Core團隊在大語言模型及多模態大模型訓練方面的合作與優化工作。重點介紹了PiMagatron Patch的核心定位與功能，以及在性能優化、功能擴展方面的更新，包括分布式優化器卸載、多模態大模型分布式訓練支持、以及DeepSeek V3相關的訓練實踐。

### Topic:
*   大語言模型訓練
*   多模態大模型訓練
*   性能優化
*   分布式訓練

## 會議主題
會議主要探討了如何基於英偉達Megatron Core框架，通過PiMagatron Patch工具庫，更方便地進行大語言模型和多模態大模型的訓練。重點包括顯存優化、多模態大模型支持、以及DeepSeek V3的訓練實踐。

## 主要技術點
*   **分布式優化器CPU卸載：** 將優化器狀態和參數更新計算放置到CPU上進行，以減少GPU顯存佔用。通過主優化器和子優化器分離的設計，實現優化器卸載加載的統一調度，並提供卸載比例變化等靈活能力。
*   **Naive Optimizer Overlapping：** 通過利用Pytorch的CUDA Stream機制，實現通信與計算的重疊，將CPU GPU間的數據拷貝時間盡可能地掩蓋在CPU的計算之下，以減緩性能損失。
*   **多模態大模型支持：** 針對千問2VL等多模態大模型，解決了並行的模型權重轉換、多模態數據的高效加載和動態分辨率的支持、視覺編碼器和大語言模型解碼器的計算負載均衡、以及長序列情況下的顯存消耗等問題。
*   **並行模型權重轉換：** 針對Huggen Face到Megatron的權重轉換問題，提出了一種正確的轉換流程，保證Attention的正確性。
*   **多模態數據加載：** 擴展了內置的DataLoader代碼，使其能夠基於輸入的原始多模態數據構造用於動態分辨率訓練的圖像切片序列以及位置信息，並支持可自定義prompt多輪對話，包含任意數量圖像或視頻的對話樣本的數據處理。
*   **動態分辨率支持：** 允許模型依據輸入圖像的大小，將其編碼成不定數量的視覺特征，提升模型對於高分辨率圖像的細節捕捉能力，同時優化低分辨率圖像的推理性能。
*   **計算負載均衡：** 通過支持基於非均勻切分策略間的負載均衡，以及拓展模型以模型實現，以支持虛擬流水線並行特性，解決了視覺編碼器與大語言模型解碼器的前向速度差異，以及每個MicroBatch實際的前向Token數不一致導致的負載不均衡問題。
*   **DeepSeek V3訓練實踐：** 在DeepSeek V2的基礎上，進一步在算法功能和AI Infra方面和V3進行了對齊，包括MOE相關的負載均衡方法、MTP支持、FP8混合精度訓練等。

## 決策與共識
*   通過CPU Offloading技術降低顯存需求，支持更大模型的訓練。
*   針對多模態大模型，解決數據加載、權重轉換、計算負載均衡等問題，提升訓練效率。
*   在DeepSeek V3的訓練中，持續進行算法功能和AI Infra方面的優化。

## 時間規劃與里程碑
*   持續優化PiMagatron Patch工具庫，提供更方便易用的接口和功能。
*   探索深度推理模型的訓練過程，包括和強化學習框架的對接，以及完整的SFT和強化學習的訓練流程。
*   及時更新到社區，與廣大開發者共享最新的技術成果。

## 未解決的技術挑戰
*   FP8混合精度訓練的性能和收斂性還需要進一步優化。
*   需要探索更多降顯存和加速技術，以支持更大規模的模型訓練。

## 後續行動計劃
*   繼續優化MOE相關的負載均衡方法和MTP支持。
*   完成微調完整流程的優化。
*   進一步優化FP8混合精度訓練功能。
*   嘗試其他一些降顯存和加速技術，比如FSDP加CPU Offloading的降顯存技術，以及高性能的AutoAuto通信算子。
*   探索深度推理模型的訓練過程，包括和強化學習框架的一些對接，以及完整的SFT和強化學習的訓練流程。
