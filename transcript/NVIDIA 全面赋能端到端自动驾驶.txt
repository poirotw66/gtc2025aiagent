嗨 大家好我是NVIDIA的John Shaw那这个topic呢我们目前分为了四个二斤的然后我们首先进入第一部分World Model的起源然后首先对于这个World Model来说我们第一次接触这个词应该是2018年Nips这篇文章叫做 Recurrent World Model Facility Evaluation那这篇文章呢它会把三个模块包括Veremonial包括RN包括这个Control这三个模块给结合在一起那Veremonial Model呢它整个起到一个对世界的这个感知包括理解那RN呢它主要起到一个对于当前状态然后预测未来状态这样一个功能然后最后呢这个Control模块呢它主要是把前面的一些feature包括这个token给转化成我们汽车或者说转化成我们记忆人所能理解的这种actions那整个这个文章呢并没有对这个Word Model进行一个明确定义但是它做了这样一个类比类比于我们人脑认知科学中的Mental Model那这个Mental Model呢它把这个对于世界的这个认知然后推理包括决策这个过程呢串在一起然后就像我们刚才提到这个World Model嗯对于World的这个Percept包括Future Predict然后包括最后Take Actions完全是E-Match的嗯那同时呢这个Mental Model呢它本身也包含呃两个主要的部分一个是Mental Representation嗯它是把我们五官的一些输入然后转化成我们人脑的一些神经元信号对呢这个过程呢其实就是有点像呃我们用的那个TokenTether它会把各种各样的这种呃视觉的输入一起做一个inviting提取相关的token呃然后同时呢这个Mental Model然后它也包含另一个模块叫做Mental Simulation然后这个模块呢嗯这部分呢它其实就像我们人脑的呃刚才提到的三个过程然后它会把我们的一些神经信号然后在人脑中做一个想象然后来想象未来会发生什么然后这也是我们把这个World Model所用在这种呃包括任何的后训练场景包括自动驾驶啊包括机器人然后所要包含的这两部分对呃然后我们进行下一部分就是讲一下呃这个World Model在这个当前的这种自动驾驶模型上的一个应用吧呃然后其实World Model在自动驾驶上它有一些些不同是因为它的这个输入输出会做一些改变呃比如说我们输入的话它会变成这种呃包括CAMERA包括LIDAS这种Sensor的输入呃然后同时呢它也会输入一些呃Ctrl模块包括我给一些Actions给一些呃TRAJETRIES然后做用这些作为输入然后同时呢我也可以给它一些其他的Condition比如说当前的这种Bounding Box啊当前的这些呃车道线啊Lanes啊呃对然后用这些做一个呃整个布局的这个控制让它能够生成呃更贴近现实情况的呃这样一个视频对整个World Model它其实在这里面表示呃还是一个以这种生成视频为主的这样一个模型那最后呢Prompt的话我们会给它一些呃整个场景的这种描述然后包括我们也可以做一些OBJET LEVEL的这种描述呃然后输出的话它其实也有一些不同啊就是除了刚才最主流的这种直接生成视频的这种形式呃我们还可以给它多加几个HEAD包括生成一些呃Planning啊包括生成一些Action呢呃对然后包括生成一些呃呃他车的这种Motion Prediction对这些都是可以的呃那通过这种自动驾驶场景的适配其实我们呃当前最主要面对的问题就是嗯就是我们要保持这个是生成视频的一个真实性那怎么样保证这个真实性呢一方面是要做到一个呃跨相机的一致性因为我们自动驾驶它有很多个传感器那有Sensor有LiDAR这种跨传感器的一致性是非常重要的呃然后其次的话是要做到一个时序上的一致性呃对呃因为自动驾驶的话我们还是希望它能够生成一段的视频的但是呃当前的这个World Mode往往生成不了那么多视频所以说一般都通过嗯一些呃Slide的window的这种形式呃然后但这在这种生成的方式中我们要加各种各样的条件去保证它持续上的一致性对来增加整个视频的跟场景的match程度一个真实性下面给大家介绍两个呃自动驾驶World Model非常经典的工作一个是这个Drive Dreamer一个是Guy1然后这两个工作呃其实输入嗯条件跟刚才那个讲的综数那里面差不多呃都是有这个Test Prompt啊包括input的呃reference image啊包括一些ADMAP的条件啊bounding box的条件啊呃对还有会输入一些呃Action作为条件然后来以期望这种模型能够生成未来的一些轨迹然后包括呃未来的一些视频呃对那呃其实讲到这里面这两个模型最主要的不同一个是Based on diffusion的一个是呃Based on这种autoregressive的呃那我们cosmos的话它其实也有这种两种架构的模型呃那从这种模型的输入输出来看呃这种world model在自动驾驶场景的应用主要分为两个方面一部分是呃我可以去生成未来的一些视频这意味着我们可以构造一些场景然后来让来让这种world model去生成呃然后同时呢它有的模型它也会生成一些呃未来的这种轨迹然后这个其实也是起到了一个辅入端端模型的这样一个作用那所以呃后面有一些研究的话它主要focus在两个方面一个是说嗯我会去生成一些counter case然后来让自动驾驶去使用然后另一个方面是说呃我也会去生成一些嗯未来的这种视频的预测或者说一些轨迹的预测然后来让下游的这种呃检验模型来检验这个呃是否合适或者说是否符合我们的需求然后来去选择这个最呃评分最高的或者说呃最呃最不可能产生这种交通事故的这样一个轨迹下面给大家介绍一下呃cosmos因为大家cosmos是怎么在自动驾驶场景做赋能的首先cosmos它本身是一个我们呃非特格的一个平台然后它包括各个模块这些模块在其他的呃topic 上也有介绍那我们今天呢主要focus在这种呃world model on这种呃av scenario我们做自动驾驶的post training的话包含几个方面一方面是我们要在自驾数据上做这种post training训练呃各种各样distribution的数据呃然后其次呢我们要保证它的生成时长呃然后呢我们要支持多传染器这个多传染器呃最直观的是这个multi camera的support然后最后呢呃我们会支持这个global control condition我们用text prompt然后同时呢也支持layout level的condition我们看几个demo这个是城区和高速场景的这个demo呃我们可以看到其实在城区场景的话呃他出了一个车祸撞了一个车然后camera会出现明显的抖动然后我们可以看一下一些auto distribution的呃这样一个scenario的场景呃包括火火灾然后包括呃在雪天然后通过这种木桩对然后这些都是base model里面没有的呃所以说呃我们当前的这个world model他其实可以生成一些counter case供下游使用这也是我们最大的一个use case然后同时呢我们的world model也可以follow一些trajectory的condition然后来生成这种不同的转向的视频ok最后的话我们会展示world model的另一个用法就是当我们的这个reconstruction呃重建的视频质量不高时候就比如说我们的这个那我们的一个就是我们的那个下游户的重建呢那我们的这个视频质量不高的时候就比如说我们做一些noviu的重建那我们的world model可以呃作为一个fixer然后把这个视频做一些修复上述就是我们world model最主要的两个use case一个是counter case的生成一个是呃重建过程中的fixer那有了这两个用力的话呃我们整个world model就可以在自动驾驶生成上面呃赋能我们的端端算法下面有请这个tara猪为我们介绍下一个topic神经重建生成大家好呃呃我是来自英伟达的高级系统软件工程师tyler呃今天很荣幸向大家介绍一下nvidia的神经重建引擎呃neural reconstruction engine简称NRE呃今天主要呃聊两个内容第一是呃我们为什么要用NRE呃第二是呃当前版本的NRE具备了什么样的能力要回答我们为什么要使用NRE呃我们首先要呃简单来说它是基于NERV神经辐射场呃还有3D Gaussian呃两种三维场景表示的呃三维重建和渲染软件呃它可以让我们从呃真实采集的相机二维图像和激光点云中呃我们可以重新渲染出具有很强真实感的二维图像以及点云然后下面这张图呢展示了NRE的一个主要的使用场景呃自动驾驶的这个呃避环仿真呃我们可以将呃呃感知算法甚至说呃基于传感器输入的呃自动驾驶模型呃接入到我们重建好的这个三维场景中进行仿真测试呃嗯NRE可以渲染出呃当前呃位置的这个呃呃传感器数据比如说这个相机图像还有这个激光点呃激光雷达点影呃自动驾驶模型呢呃接受这个传感器数据并输出呃它下意识刻的气测的位置呃然后NRIE又可以根据新的这种未知的输入输出新的传感器数据然后循环就完成了仿真得益于这种可为场景的表达还有基于真实数据的重建NRIE相比于这种传统的类似于游戏引擎的这种仿真系统具有更强的真实性和这种可扩展性那么为了更好支持闭环仿真NRIE提供了怎样的能力呢首先第一点是对各类传感器的准确建模NRIE支持多种不同类型的相机机变模型比如DV中的Pin Home模型还有Fish Eye模型然后我们可以设置不同的相机内参同时我们也建模了相机的滚动快门以准确捕捉相机自身运动还有目标运动对相机成像结果的影响这对自动驾驶场景的重建是非常重要因为量产相机通常有不能被忽视的快门时间然后在很多的场景中相机也就是自车和场景中其他目标的相对速度是比较快的因此滚动快门所产生的运动模糊也非常明显通过准确的建模NRIE可以有效克服这种运动效应对重建的负面影响然后NRIE也支持对激光雷达建模我们支持不同的激光雷达的发射模式以及对反射强度还有射线丢失等等这种射线的性质的预测让我们可以产生相对完整和准确的点下面这个视频呢展示了对同一个场景使用不同程度激变的相机进行渲染的一个结果准确的传感器建模是高质量重建和渲染的一个基础另外很重要的一点是NRIE支持多传感器同时重建比如环式摄像头前式摄像头和后式摄像头同时重建一个更大的重建后的三维场景并且NRIE仅需一次重建训练就可以同时渲染出高质量的激光雷达点下面的这个视频呢展示了NRIE生成的图像还有点云和真实图像点云的一个对比最左边一列的是点云的鸟看图第二列是自车斜后方视角的这么一个点云的映射图最后一列是前视视角的相机的图像上面是真实的图像然后下面就是这个渲染出来的这个图像你可以看到注意到一点是在渲染出来的图像中真实图像被拍到了这个自车部分我们并没有把它重建出来最后一点随着产品的进化NRIE现在也不仅仅只有重建的能力我们也将生成是AI继承到NRIE中以提供更好的新视角的渲染结果这个问题是这样因为重建本身它是从真实数据比在真实数据没有覆盖到的区域或者说这个相机没有拍到的地方其实重建的结果并不准确但是仿真所产生的新视角很可能会偏离原始视角比如说原来的这个自车在某一根车道上比执行士它有可能在仿真环境中它会选择一个便道到另外一个车道因此偏离了这个原始视角很多然后这样我们看到重建的这个场景它在渲染之后它可能也存在瑕疵我们称之为围影左边的视频展现了一个常见的新视角围影原始数据是沿着这个道路前进并且相机的朝向是比较水平的我们在渲染的时候选择停在某个轨迹点然后抬高摄像头并且沿着Z转旋转我们也可以看到在这个摄像头没有拍到的偏上方的这些区域在镜头上方的这些区域它出现了很多天空植物还有这些建筑出现了很多模糊为了解决这样的问题呢我们引入了一种基于Diffusion Model的生成模型我称它为Fixer它可以对有瑕疵的渲染图像进行修复得到更加干净准确的新视角图像右边的这个视频呢就展示了我们修复之后的形容大家可以看到模型把这个没有看到的部分没有看到的天空植物还有建筑物给它想象出来我们同时提供了两种使用这个Fixer的方法一种是渲染后处理每次渲染都进行实时的修复另一种是在训练的过程中使用Fixer对训练图片进行修复然后重建的质量内化成模型的一部分然后可根据需求灵活的选择不同的方式最后一页Slide展示的是NRE的大概的一个软件站NRE也是在NVIDIA的生态上成长起来的软件它利用了包括Cuda、Slum、Optics等在内的计算和渲染的底层能力练起了一个重建和渲染的平台结合生成式AI向客户提供了一种自动驾驶仿真的新方式这就是关于NRE的全部内容谢谢大家谢谢大家好 谢谢大家今天我会介绍我们在自驾领域我们DevTech Team做的一些训练的优化今天的介绍的分成三部分第一部分是图片训练的PIPLINE第二部分是视频训练的PIPLINE第三部分就是我们怎么把Loss计算Batch起来然后首先我们开始第一部分我们的Motivation是这个样子的在自驾模型里面我们的Perception Model或者说是在E2E里面的Perception Modular它都消耗了大量的计算资源在此呢图片和视频的加载以及预处理非常关键是因为这个模型呢它要接收多录视频的输入比方说六录七录八录所以呢根据这样子的一个情况呢我们基于达利我们设计了一个Auto Model Training的它的最佳实践我们希望我们的Auto客户呢比较容易的重用我们的工作他们也可以基于我们的框架然后开发他们自己的一些加速算子一些预处理的算子然后来加速他们自己的模型训练首先呢我们从一个简单的图片训练开始然后我们又达利实现然后和原来的基于纯CPU的进行对比然后看一看效果那这个是基于一个纯CPU的BV Formal的训练我们可以看到这边有点空白所以其实是模型训练在等这个数据加载然后在我们用GPU加速这个加载以及预处理过程之后我们就会发现模型训练以及数据的加载和预处理是完全Pipeline起来了然后我们的GPU利用率呢达到最高首先呢来看一看我们的设计其实是我们这个是基于达利的首先呢我们有一个达利我们有一个大力External Source然后呢把我们的数据读进来然后读的格式就是Sample Data Group然后这个Sample Data Group是我们根据这个自驾这边它的数据的特点来设计的比方说我具有图片我还是多录的图片我具有图片每录属录图片呢也都有它的一些超餐然后我还有雷达我还有各种各样一些这种有分层的数据所以我们是按照这个特点处理的然后每一个OP它的输入都是Sample Data Group它的输出也是所以不论你有多少个OP你都可以把他们集连起来然后最终呢输出一个Sample Data Group然后转成Dict然后送入我们的模型训练脚本然后完成这个训练这个是一个比较compact的图跟刚刚一样就是读数据然后把这个Pre-Processing的这些OP叠起来然后最后输出那我们介绍一下我们对这个Image Training Pipeline做的一些优化首先呢它有达里斯大优化比方说笔图片以及解码它是可以有一部分放在GPU上进行的所以它可以是所以呢它这个是用Hardware Decode所以它可以释放一些就是解放一些裤子扣然后还有呢它的很多算子一些内置的算子其实既有CPU version也有GPU version然后呢大力还支持一些Customer算子就是你可以自己写CPU和GPU然后把它们导成一个大力的Customer算子然后你也可以写Number就是一个Python Level的东西然后呢在这个算子之间如果这两个算子的数据是没有互相依赖的然后大力会自动把它们并行起来并且大力这个过程呢跟训练它是并行的所以我们可以隐藏一些overhead然后呢我们还可以根据这个performance来选哪些OP在CPU上运行哪些OP在GPU上运行根据这个performance来看那我们基于这些特性呢我们实现了一个example的pipeline我们用GPU加速了我的JPG的解码以及A5 transform然后我们还做了一些Customer的一些优化比方说对于Gaussian我们也优化了然后我们还做了这个vectorize的处理对于boundingbox来说然后我们还实现了一些自定义的算子所以下面呢我们可以看一下我们的performanceevaluation首先呢我们的batch size是128我们有64个CPU然后我们用了DDP然后每个GPU上它的batch size是一样的那这是我们的一个结果所以对于所以其实是比我们的reference pipeline也就是纯CPU的GPU托识实现呢我们有一个5.17倍的这个加速所以我们现在来看就是经过我们的优化之后我们的模型训练和数据预处理是可以完全的这个pipeline hide得住那下一部分呢就是我们的视频训练其实我们视频的训练就是我们的背景是这个样子的然后现在大部分自家模型呢它的输入是图片但是随着我的这个训练数据的增加然后这个对我的存储以及我的带宽都是一个很大的很大的然后呢视频的压缩率其实是比图片压缩率要高的这些是一个简单的例子就是用视频我们可以节省80%到90%的存储但是呢如果我们想要采大这个视频训练我们想达成的是我们有更少的存储以及我们有更少的带宽但是呢我们对这个真的提取然后以及数据预处理我们就会有更高的要求因为我们的目标是一样的就是模型训练可以把这些加载以及预处的过程可以他们俩可以完全的pipeline起来并且我们还要尽量的少消耗GPU的显存因为我们希望把更多的GPU的显存用在这个模型训练存模型的一些Tensor以及Wates的上面那这个就是我们现在的设计首先啊video的解码啊就是我们叫on demand video decoding也就是你给我一个视频你给我一个frame ID我尽可能少解就是少少做解码运算然后把我的期望的这个帧给解出来所以我们会有一个Demax的现成也会有Decoder的现成然后呢对于我期望的帧我只解它最小的解码代源也就是一个GOP然后我只存它我想要的那一帧其他的帧我都不放在显存里面这个呢就是实现了我解最我解码最少所以保证了我一个速度然后这边呢其实是说我只存我需要的帧所以其实是可以减少显存的消耗然后呢再把我解出来的帧送去做预处理以及增强所以这部分其实是跟我们的图片训练的Pipeline是一样的所以我们在视频里面我们就不强调这一部分然后呢这个是我们最终的一个interface就是interface长这个样子就是你告诉我你想解哪些视频然后你的frame ID我就可以把它解出来然后它解出来之后其实就跟读进来的图片是一样的然后基于因为我们刚刚说了我们对于图片我们对于视频训练是有要求的就是它的速度有要求所以我们做了很多的优化比方说重用这个NVDecoder我们还可以用Async的就是EAP的Demaxer然后以及GPUMemoryPool去重用这个GPUMemory我还可以跳过一些不需要的帧所以最终呢我们解一帧的时间是89.7毫秒它实现了训练和数据加载以及处理的一个完全的隐藏那这也是我们最终的timeline的一个show就是它们是完全隐藏起来那我们来到了今天的最后一部分就是我们想把这个loss计算bathing起来这个background就这样这边是stream PTR然后这边是UniAD然后在这些模型里面它有些是目标检测然后有一些是E2E它们的loss计算都占了非常大一段时间大概30%左右并且它们都是在CPU上的然后它们还都是persample的那我们的一个直观的想法就是我们能不能把它们batch起来然后放在GPU上然后它的困难其实就是我的这个ground truth也就是user loss其实是prediction和ground truth他们是ground truth是nonuniform的所以我们就设计了一个叫regbatch它可以handle这个uniform的batch然后呢它这个regbatch呢它可以和各种的loss都适配起来所以就是你不管是什么loss我的regbatch都可以把它batch起来它既可以是protug loss它也甚至可以有weight还有mask所以呢我们就设计了一个dataformat然后呢这个dataformat存是按照predtouch按照tensor来存的所以它其实是uniformsize所以我们需要padding我们还需要一些mask这个是regbatch的一个例子我有数据我有mask然后呢我还有一个additional的元素叫samplesize其实就是告诉你这个data里面每一行它有哪些这个它有几个有效的元素它这个有效的呢它一直是存在左边的然后padding是在右边他们其实这两个看起来有点冗余但是在有一些计算里面他们会提高这个计算的效率这些其实我们就列了我们的一些hub function比方说怎么转成reg tensor然后怎么去index这个reg tensor然后怎么去生成mask然后以及怎么去这个去map然后从这个reg tensor map到另外一个index这些其实都详细的列了这些op然后还有就是说你有一些tensor它是persample的我怎么把它转成batching的其实是有一个combine以及split的然后反之怎么转回来有combine还有一个split的操作所以最终呢我们把这个batching用在streamptr上面然后仅靠这个batch的loss呢我们就取得了1.43倍的一个speedup对所以今天是今天这是今天所有的内容谢谢大家谢谢大家各位听众大家好我是这个章节的讲师Tony我给大家带来的是关于在shall平台上的端车模型部署以及性能优化shall是我们针对车载应用推出的新一代算力平台是基于blackwell架构提供1000clops的ip8int8算力那么采用了第五代的tensor code那么提供了32兆的l2cage同时针对大圆模型类的应用做了深度的优化提供了2000tops的ip4的算力那么另外在这基础上针对性的进行了软件的优化可以实现更高效的flash attention那么Tensor RT10的推力引擎呢同样提供了更好的图优化还提供了像l2tiling training的这样一个可以针对memory bound的算值进行性能优化的技术同时灵活的GPU调度策略使得我们的用户可以通过硬隔离的MIG技术或者通过软隔离的GPU schedule的技术来实现高效的端到端和VLM模型的这样一个高效部署那么我们先快速回顾一下自动驾驶算法演进的三个阶段那么从最先可以应对结构化道路的传统卷积RN类网络那么这一类网络对算力的要求并不高可以部署在ZEWIA平台到最近几年涌现的BV Transformer类的网络用来应对无图的这样一个设计场景可以部署在All Ring的算力平台到最近最新涌现的端到端VLM的算法设计那么这样的端到端VLM的算法设计可以应对此前没有发生过的一些交通场景从而实现场景理解那么它对算力的要求也更高所以我们的SAL就应运而生从BV Transformer到端到端VLM从BV Transformer到端到端VLM这样一个跨越为什么对算力的要求提升了这么多呢那么首先我们可以从BV Transformer的设计中看到它的一个整体的结构那么左边是基于小鹏此前公开的一些信息可以推测出来相应的一个算法结构首先我们从传感器进来的图像和激光雷达所以激光雷达数据会进入一个基于传统的卷积或者Transformer-based backbone再转换成BV视角下的Feature再经过一轮持续的这样一个处理最终经过Transformer-decoder的处理到最终拿到我们规划预测需要使用的感知结果那么这样一个链路呢到最新端到端和VLM中我们可以看到实际上它已经成为整个端到端模型中的一个处境端到端VLM在此前理想去年TTC上提供的公开资料可以看到快慢系统的设计非常巧妙的解决了此前我们提到的场景理解的问题那么我们的端到端的模型可以用来做环境的感知包括动性态场景的这样一个感知还有可以去做目标的轨迹的预测和自车的行车轨迹的一个规划同时呢这样一个系统作为快系统再配合VLM作为慢系统来实现一个更高智能的一个规划效果那么我们可以从这样一个眼镜路线中看到端到端的设计中的一些特点从当前比较流行的像Spot Drive的设计中我们可以看到端到端模型带来的一些新挑战那么它包含一个两阶段的这样一个设计或者是单阶段的设计那这样的设计里面会包含主要两个功能一部分功能是用来做感知和在线的建图定位另外一部分是用来做他车的预测和自车的轨迹规划那么这样一个设计呢里面会引入哪些新的特点呢相比此前的卷积包括BUA Transformers设计我们可以看到在这个设计中它会更加的依赖Transformer类的结构那么另外呢为了融合多模态的信息它在这个结构中会引入更多的CellCart来连接不同组件来实现相应的一个信息的融合针对这样的一个模型的设计那么对我们的算力平台有了更高的挑战我们的Salt针对端到端提供了一系列的软件上的推理的优化那么针对我们前面提到的这样几个特点首先Transformer类的结构在Salt的Tensor RT中引入了Blackware的Flesh Attention针对性的进行加速那么针对前面提到的很多的Short Cut结构我们在新的Tensor RT10中提供了新一代的编译引擎可以针对性的进行更高效的这样一个图优化处理那么模型变得更大变得更深这样就会对Memir Bond类的算子产生优化的一些需求那么我们在Salt Tensor RT上同样推出了L2Tailing Training技术可以实现针对Memir Bond算子的一个性能提升那么最终我们还可以看到在端到端模型中因为它不同的模型输入和量刚之间的不同会导致整个模型的精度很难控制那么这样对模型的精度控制在Tensor RT10中我们提供了像IP8和IP16的整个新的精度通过这种新的混合精度使用类型像先前的Int8加IP8的混合精度能够实现更高的精度模型达到最终理想的部署效果前面介绍了针对M2M的模型设计Tensor RT引入的一些新的技术那么这里有给出我们在一些当前比较流行的网络设计中的一个推理效果我们可以看到像Buv Fusion, Buv Formal, Sparse 4D和Sparse Drive这一系列的模型上我们在Drive OS 7.0.2也就是我们当前最新的Tensor RT10的基于Blackwell架构的重新设计后的Tensor RT10带来的性能的提升是非常明显的有很多模型已经可以实现两倍的早先一代的All Rain的性能那么前面给出的是一个性能的Overview这里是我们当前Drive平台的Model Room中已经包含了一些典型设计那么我们的Drive Model Room是一套综合的解采方案可以同时提供Training和Inference的端到端的一个端到端的一个从训练到最终部署的一套软件解决方案这一套方案呢可以实现我们在不同的模型设计中都能拿到很好的精度和性能的一个平衡那么包括后续我们还会在其中引入L2Tatting Training以及IP8相应的一些支持前面介绍的是关于端到端网络的相应的一些Tenshin RT测的一些优化那么针对最终的部署优化和VM在一起工作时我们还有针对性的像MIG和GPU Scheduling的技术供用户选择来去实现软件的支援的一个分布那么通过MIG技术我们可以实现EDAS和大元模型域的一个支援的硬格力那么可以通过拆分对应的L2CacheTPC的数目包括Bandwise的一个带宽来去实现整个算力平台的一个支援划分那么最终呢我们还提供了GPU Scheduler技术可以实现一个软件层面的隔离来去使应用能够在单个时间片中独占的使用GPU上的资源那么这样可以实现最高效的一个支援的调度和应用