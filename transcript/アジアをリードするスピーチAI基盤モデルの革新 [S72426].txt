皆さんこんにちは 本日はアジアをリードするスピーチAI基盤モデルの革新というテーマで皆さんにお話しさせていただきたいなと思いますはい ちょっと講演を始める前にですね 私どういうものなのかというところを自己紹介させていただきたいなと思うんですけれども私はもともとアメリカのアカデミアで生成AIの研究をやっていた研究者になりまして博士課程自体はですね アメリカのコーネル大学というところで研究をやっておりまして専門分野としては自然言語処理とコンピュータービジョン 言語と画像の分野の研究をやっておりましたその後ですね アメリカから日本に戻ってくるような形で日本が誇るスーパーコンピューターアフガクというスパコンを使ってですね日本語に特化したLLM 日本語に特化した生成AIを研究開発するというプロジェクトを立ち上げた一人となりましたこちらのプロジェクトではですね 例えば1万個のCPUの分散並列学習を行い日本語のLLMを学習する そういったことをやってきて今どういう立場でこの場に立たせて 講演させていただいているのかというところなんですけれどもこのフガクLLMというプロジェクトがあった後にですね私は言葉テクノロジーズという会社を共同創業しました言葉テクノロジーズでは何をやっているかというと音声AIにもいっぱい計算資源 GPUを使うという考え方を持ち込んで大規模に開発をしようということをやっておりまして弊社例えば計算省さん NEDOさんがやられているジェニアックの事業の第1.5回 第2回に連続採択していただいて大規模にGPUを使いながら 音声の生成AIを開発しております私ここの言葉テクノロジーズでは 共同創業者兼CEOという形でやらせていただいております今日のトピックに非常に関連するところとしてこの言葉テクノロジーズという企業は 具体的に何をやっているのかというところをお話しさせていただきたいんですが先ほど申し上げたところとも 被るところはあるんですが私たちは音声の生成AIの力で 言語の壁を打ち崩すというテーマで日々活動しております具体的にどういうところを やっているのかというと基本的には音声の生成AIの 研究開発をやっていてそれを特にAIの同時通訳という技術に パッケージングして提供しているような会社ですこのAIの同時通訳というところでは 超高速で例えば私が日本語でしゃべっているときにそれを英語のテキストであったりだとか英語の音声に同時に通訳してくれる 技術を提供していたりこれが例えば日本語 英語だけじゃなくて 中国語 韓国語 ベトナム語そういったところに展開できてこれが例えばiOSアップで提供できるようなそういったことをやっていたりしますこの同時通訳という技術が我々言語の壁を打ち崩すという テーマでやっているのでメインではあるんですが我々音声の生成AIの基盤のモデルを 作っている会社でもありますのでベースのテクノロジーとしては例えば音声の書き起こしですね入ってきた音声を クリーンなテキストに書き起こしてあげるこういった技術でも 非常に強いところはございまして例えば弊社のモデルというのはハギングフェイスという AIのモデルをシェアするプラットフォームで一部オープンソースとされていたりするんですがこういったところでは 例えば累計50万回以上ダウンロードされていたり例えば弊社のモデルはですね実用的なところで言うとNVIDIA様の AIサミットジャパンというところでもご運用いただいたりしておりますさらにですね 弊社のモデル 汎用的なところで言うと音声合成 音声チャットボットこういったところでも 非常に高い技術を持っていましてデベロッパーの方々に評価いただいたときでも大手と比べて非常に高い評価を いただいていたりだとかこういったところが大手企業 商業的に導入されたりしております今回のトークではですね技術的なところはどんどん この後話していければと思うんですけれどもそもそもこういう言葉テクノロジーズがどういった経緯でできたかというところが非常に我々の研究開発にも 結びついているところが多いとそういったところで ちょっとこだけ言葉テクノロジーズのお話をさせていただきたいんですけれども我々の企業自体はですね2023年の7月に創業して今 東京の大手町に研究開発拠点を構えて研究をしているという体制になっております共同創業者としてはですね本日お話しさせていただいている 私小島と加西というものが2人で立ち上げた企業となっております加西も小島もですね2人とももともとアメリカのアカデミアで規制性愛の研究をやっていたものになりまして私の自己紹介は先ほどさせていただいたんですが加西の方もですねシアトルに入れられます ワシントン大学というところから博士号を取得していてその後 トヨタ工業大学 資格語と校というところでも表弁を取っておりました私たち2人ともですねもともとはアメリカのアカデミアで生成愛の研究をやっていたんですが冒頭申し上げた富学LLMというプロジェクトがきっかけに日本に帰ってきて研究活動をスタートしまして富学LLMってどういうプロジェクトだったかというと日本国内 当時生成愛の研究があまりなされていなかった現状もあって日本国が誇るスーパーコンピューター富学を活用して日本語に特化した生成愛を研究開発していこうということをやりましてそういった経緯もありまして弊社はもともと研究開発職が非常に強いチームを構成したんですが大きく分けて弊社内には2つのタイプの専門家がございましてこの富学LLMのプロジェクトから派生したようなスーパーコンピューターの専門家と加西は私のようなAIのトップの専門家がタッグを組んで研究開発をしているのが弊社の言葉テクノロジーズとなりますそういったところもありまして弊社は着実に発展を遂げておりまして会社を作った当初は六本木の東京ミッドダウンというところがあるんですけれどもそこの前にオフィスを構えていて2人しか従業員がいなかったんですが今は大手町にオフィスを移転しまして大体20人弱ぐらいの規模になっておりましてそういったところもあって例えばForbes JapanさんであったりだとかNHKさんで取り上げていただくようなところまでは来ておりますこういったところを踏まえてですね今日メインでお話ししたいテーマとしてはこういった技術開発チームトラクションを得ている弊社が開発しているスピーチ基盤モデルの技術のお話をしていきたいなというふうに思っておりますちょっとそのスピーチ基盤技術のお話をする前にちょっと生成AI全般のお話をさせていただきたいなということを思っておりまして生成AI全般のお話をするとどういうことかというとここでちょっと言及させていただきたいのは今のAIの生成AIの全ての発展はこの大規模言語モデルのスケーリング則に依存しているということがあるということをちょっと言及させていただきたいなと思いますスケーリング則って聞き慣れている方も聞き慣れていない方もいらっしゃるかなと思うんですけれども具体的にどういうことかこの図を使って説明していきたいなと思いますこの図で表されているものというのはこれ様々なタスクにおける生成AIのパフォーマンスになっていて例えばそれこそ生成AIが算数の問題解けますかとか他言語のペルシャ語のクエスチョンアンサリング質問の問題を答えられますかとかあと生成AIがですね読解力が必要とされるようなタスクでどのようなパフォーマンスが達成できますかそういったものをそれぞれの図で表しているんですがX軸がですね生成AIの学習に使われたGPU計算資源の量Y軸がですねそれぞれのタスクにおける生成AIのパフォーマンスとなっておりますこの図を見て明確に言えるのは生成AIのパフォーマンスって計算資源GPUが一定の量を使われるまではそんなにそれぞれのタスクできないんですただその計算資源GPUの量が一定の量学習で使われるようになると一気にこういう突如変異的にいろんなタスクでパフォーマンスを発揮できるようになるとそういった現象が観察されておりましてこれのことを往々にしてスケーリング則と我々は呼んでいますこのスケーリング則が当初オープンAI社中心に発展してきたテキストメインの生成AIで主にこの発展を導いていた考え方だと我々このテキストのAILLMと呼ばせていただくんですがLLMの発展で起こっていたこのスケーリング則をそのままスピーチAIに持ち込んでしまおうというフィロソフィーの下研究開発をやっておりますスケーリング則 先ほどのこの前のスライドではですねそれぞれのタスクにおけるAIのパフォーマンスと使う計算資源 スーパーコンピューターGPUのリソースの比率をお見せしていたんですが同じような法則がですね学習するAIのモデルの大きさ学習に活用されるデータの量そういったところにも比例するような形でどんどんタスクにおける性能が良くなっていくと我々はですね 開発のフィロソフィーとしてスページのAIのフィールドでどんどんスーパーコンピューターのリソースを増やしていってAIのモデルの大きさも大きくして使う音声データのデータも大きくしていくことでどんどん性能の良いモデルを作ろうということを日々取り組んで開発しております実際ちょっと計算資源のところを補足させていただきたいなと思っていてこれ不学LLMの話も踏まえながらちょっとお話ししていきたいなと思っているんですが我々の不学LLMをやった当初はですね日本国内 不学っていうスパコンを先ほどお話ししてこれご存知の方もご存知じゃない方もいらっしゃっているかなと思うんですけど不学って当時日本国内にあった世界で一番早かったスーパーコンピューター2022年当時でも世界で2番目に早かったスーパーコンピューターなんですね我々企業当初はこういったスパコンのリソースを活用して研究開発をやっていて実際不学LLMの前もですねフランスとかでハギングフェイス社がですねフランス政府が持っているスパコンを活用して研究開発をしていたんですがそれと比べてもかなり大きな量で研究開発をできていたとこの不学っていうスパコンは非常に規模の大きいものでこれでAIの研究開発をやるっていうのもなかなかインパクトのあるものができるようなものではあったんですが不学一つだけ弱点があったんですそれ何かというとこれってCPUと呼ばれるハードウェアを使っていてAIで汎用的に使われるGPUを搭載したスパコンではなかったんですよねだからやっぱりそのAIの研究開発をこのインフラでやろうとしようとするとなかなか分散並列学習と呼ばれるソフトウェアのアルゴリズム上の工夫であったりとかそういったところに時間がかかってしまってなかなか思ったように研究開発のスピードが加速できなかったとそれでも参加しなさった方々の多大な努力のおかげでプロジェクト自体は成立したとただですね 我々これ2022年当時こういった計算資源の基盤を使っていたんですが今はもう全く違った計算資源のインフラを使っていて具体的にどういうところを使っているかというとネド様計算省さんがやられているジニアクという補助金があるんですねこの枠組みに弊社第1.5回第2回連続採択いただいておりましてここで何ができるようになったかというとNVIDIAさんのGPUに大量にアクセスできるようになったんですね大量にアクセスできるようになったら具体的に何にアクセスできるようになったかというとNVIDIAさんが開発なさっているH100というデータセンターの最先端GPUに大規模にアクセスできるようになったと我々今現状の研究開発はですねこのH100のGPUを大量に使うことによってスピーチAIのモデルの研究開発もスケールアップして学習できるそういった体制で進めておりますこういった計算資源を踏まえて次ちょっとモデルのところをお話ししたくて実際こういった計算資源 GPUがあったとして具体的に何を作るのかというお話をしたいなと思いますまずこのスピーチ音声の領域我々のような企業が出てくる前はどういうアプローチが取られていたのかというと音声が関わるタスク 何でもいいんですよ音声チャットボットであったりだとか我々が取り組んでいるような同時通訳こういったものをやろうとしていたときにどういうアプローチが使われていたかというとパイプラインアプローチというものが使われていましたこれ何かというと例えば入ってきた音声を一回に同時通訳のコンテクストだと入ってきた音声を一回それを書き起こしてあげてテキストにするとそのテキストをテキストオンリーのLLMに入れてあげて出てきた翻訳を読み上げてあげて同時通訳なり通訳するとこういった複数のモデルを組み合わせることによって一つのAIシステムを作るというパイプラインのアプローチを今までしてきたんですただ我々が取っているアプローチというのはエンドとエンドと呼ばれるアプローチになりますこれ何かというと音声をAIのモデルに直接入れてしまってAIのモデルが音声を直接出すというアプローチをやっているとこれをやると何が嬉しいかというと2つあります1つは我々のような同時通訳のように非常にリアルタイム性が必要とされるアプリケーションだとこういったアプローチの方が遅延を減らせるよねとそれ2つ目はですね1つ目嬉しいことがあってむしろこっちの方が重要かもしれないとこういうアプローチを取るとNVIDIAさんのGPUのインフラのようなところでいっぱいデータを使って大規模に学習しようとするとスケールしやすいとこの2点の嬉しさがあるので我々としてはエンドとエンドのアプローチを採用して研究開発を進めているということをやっていますちょっとこれだけだとやっぱりテクニカルなところだと情報量が足りないよねちょっと満足できないよねとおっしゃる方も多いかなと思うので実際我々がどういったモデルを開発しているのかもう少しだけ踏み込んでお話ししたいなと思います具体的にはですね我々が開発しているものっていうのはテキストオンリーのLLMと似たようなオートリグレッシブトランスフォーマー左から右にどんどんトークンを出力していくよというモデルになっていますこれトークンって言ってもトークンって何かっていうと例えばテキストの単語とかだとI have a catみたいな文章があったとしたらこのIとかhaveとかあとがcatみたいな単語をそれぞれのインデックスにしてあげてそれでテキストを表現するというものがトークンなんですテキストだとこういったことができるっていうのは皆さん直感的に分かるかなと思うんですけど音声でそれをどうやるかというと音声も実は似たようなことができるんですね音声もですねトークナイザーというものがあって音声って基本的に波形データなんですけど波形からこういったトークンにマッピングできるようなモデルがあるとこんなモデルを活用して音声を波形からトークンデータに変換してあげるとそうするとこのトークン化された音声の上でもオートリグレッシブトランスフォーマーが学習できますよねっていうのが我々のフィロソフィーなんですがただこれをやってしまうと一つ問題なのがI have a catってテキストのトークンで表すと大体2から3トークンぐらいで済むんですねただこれを同じ音声でI have a catって音声データをトークン化しようとするとこれが大体100トークンぐらいに基本的にはなってしまうとこうすると学習もしにくいしインフラとしてインフラとしてインフラとしてシステムと運用する場合も非常にコストがかかってしまうとなので我々の研究開発のミソとしてはこの音声をトークン化するときにどんどん圧縮してあげてそうすることで学習も高速化できるし学習高速化できることで開発サイクルも向上して長く学習できるようになるしこれを実際にアプリケーションシステムとして運用するときもどんどん早く運用できるので嬉しいよねとそういったところが我々技術開発のミソとしてやっているというふうにお話しできればいいかなと思います実際ここからはですねちょっとどんどんデモをやっていこうかなと思っていてこういったフィロソフィーで開発できるモデルが具体的にどういったことができるのかなと実際に聞いていただきたいなと思いますまず1個目のデモはですねこういうオートリグレッシブトランスフォーマーを使ったスピーチAIモデルが日本語のドメインでどれだけナチュラルで感情豊かでいろんな表現ができる音声を生成できるか実際に聞いていただきたいなと思います言葉テクノロジーズのミッションは音声基盤モデルを作ることです私たちのモデルはこのように流暢な日本語を生成できますさらにこのように生成する音声のスタイルをコントロールすることも可能です言葉のモデルは世界最強の日本語音声生成モデルです今お聞きいただいた音声というのはですね全て弊社のスピーチAIモデルによって生成された音声になりますお聞きいただいた通りただ単に流暢な日本語を話せるだけではなくて私の声真似ができたりスタイルでお真似をしたり感情をコントロールできたりそういったことができるようになっていますこれも我々のスピーチAIモデルというのは非常にパワフルなものですのでこういった声真似とかスタイルの変更をどうやっているかというと大体5秒ぐらいの音声サンプルを与えるだけでそういうスタイルとか声を真似することができると非常にパワフルなテクノロジーになっていますこのパワフルなテクノロジーをバックアップしているのが冒頭申し上げたスケーリングの考え方でGPUをいっぱい使ってあげてデータもモデルもそれに合わせてスケールアップしてあげることでこのような非常に高い性能が出ていくとそういった現状になっております次のデモも非常に似たようなデモではあるんですけれどもこういったことを突き詰めていくと何ができるようになるかと先ほどの日本語の音声生成だったんですがこれ多言語でも私の声で話せるようになります聞いていただきたいなと思います言葉テクノロジーズのミッションは音声基盤モデルを作ることです韓国語の話の言葉を語ることです韓国語の話の言葉を語ることですそれから本で言葉を語ることです今お聞きいただいた通り私の声を真似するような形で日本語だけじゃなくて英語 ドイツ語 韓国語 ベトナム語をしゃべるようなことができるとこのようなですね一つのパーソナにおける多言語の生成みたいなところもですねやはりスケーリングの考え方が寄与するところが大きくてこういったところのデータも多言語にどんどんスケーリしてあげましょうそれに応じてモデルも大きくしてあげましょうそういったところの成果としてこういう多言語のドメインでも流暢な音声がどんどん生成できるようになっていくとこういったテクノロジーは非常にパワフルでありつつ使い方によっては非常に難しいところもあるテクノロジーかなとは思っているので我々としては責任を持ってこういったものがアプリケーションで運用できるところと運用できないところを切り分けて提供していくということを考えておりますここで実は多言語音声生成というのをお見せしたのはですねちょっと次のスライドのデモにつなげるところが大きくありましてここで今お見せしたいところとしてはですね我々が開発しているスピーチAI基盤モデルの一番最新の成果こういったものを使うことで同時通訳をやってしまうAIの同時通訳をやってしまうそれがどのぐらいのレベルになっているのかというのを実際ビデオを見て皆さんに理解していただければなと思いますじゃあ早速1分半ぐらいのデモになるんですが見ていただきたいなと思います私は日本語ですごいランダムなことを言っているんですが私がさて日本語を話すと英語が大体2から30秒くらいの遅延でこのような形で出てくるんですが私が喋ると私の声でこうやってパッと翻訳が出てきてこの sugar 蘇原を持って知らないことだと思いますおそらく私のテראの経験をしているのが警視庁のポテン挑矛期状でパテン挑戦中は今後ハングランタル対策の勿害をどんどん変えていくようなことを私の者とシェアしているlexe-ya違反地はレオフレーチチェンジボールボール沒前を知りません私はちょっと逆をやってみたいなお望みをます僕は transmit私たちはこのソフトの話をトラスティングサービスにインターネットの話を私たちのことについてですその後に日本語の話を日本語の話を日本語の話を日本語の話を日本語の話を日本語の話を日本語の話を私たちは私たちは私たちは私たちはこのシステムを展開することに社会にするどうもありがとうございますいかがだったでしょうかそうですねでもビデオの中でお見せしたこととしては私が日本語で話した時に私の声で英語で音がパッと出てくる通訳がパッと同時に出てくると同時といっても人間の同時通訳と同じなので大体日英だと2から3秒ぐらいの遅延で出てくると私が逆の言語を話した時も当然同時通訳ができるようなシステムが現状もうすでに弊社の中でできておりますこれこういったそうですねシステムこのビデオの中であるようなものはおそらく今世の中どこを見てもこのレベルでできるプロダクトシステムというのはほとんどないんじゃないかなというレベルまで来ておりまして今日へ延日ができているんですがデモで見せていないところでは日韓官日日中中日そういったところもサポートできるようなところまで来ております今このデモビデオの中では割と私の声真似はしていたんですがフラットで感情のないような声で通訳が出てくるとただ今後の弊社の技術的マイルストーンとして私が例えば嬉しいような感情で物事を話した時に通訳も嬉しいような感情で話してくれる怒ったような感情で行った時にはその感情も反映するような形で話してくれるあとはですねイントネーションとかトーンそういったものも私が日本語で話したことが多言語でそれを忠実に再現するような形で同時通訳を行っていくそういった研究開発を今弊社では行っております我々こういったシステムを構築して大規模なGPUを活用するような形で構築しているんですがどのように提供していくのかというのも非常にミソかなと思っておりまして弊社としては大きく分けて2つこのような技術を提供していくような形になります1つはですねiOSのアプリのような形で皆さんが実際我々のテクノロジーを手に取ってお使いいただけるような形ですねこれは音声からテキストへの同通もありますし音声から音声への同通もあるとこれをお使いにいただくことによって実際にどのように言語の壁が壊れていくのかというのを実感していただきたいともう1つはですねこういったものをAPIとして提供していくことによって例えばウェブカンファレンスコールセンターゲームのライブストリーミングトリジショナルメディアのライブストリーミングそういったところでより社会的インパクトが大きな課題に同時通訳を差し込むことによって世の中を変えていくとそういったところを我々としては展開していきたいなというふうに思っておりますそうですねこういったところを主に突き進めつつ次の技術的な課題がどういうところなのかというのもちょっと同時に触れておきたいなと思っておりまして我々としてはこのスピーチAIの基盤モデルというのをどんどん同時通訳に特化させることによってここにおけるインパクトを増大させたいなと思っておりますそうしたときに具体的にどういったところが技術開発のポイントになってくるのかというと1点目は先ほど申し上げたところと関連するところも多いんですがどんどんやっぱりユーザーのユーザー体験を向上していこうというところが大きなところなのかなと思っております我々が開発するエンドとエンドの音声モデルというのはその特徴から声真似 感情 イントネーションそういったところを非常に流暢にコントロールできますし元の音声から出力される通訳の音声のマッピングこういったフィーチャーのマッピングみたいなところも上手にできるので実と絡めてどんどん研究開発していければいいなと思っております2点目はですね我々やっぱり日本に研究開発のチームを置いて創業メンバーも日本同時通訳ということを考えるとやっぱり日本語に日本語における日本における市場が非常に大きな分野だと思いますのでこれを日本語市場により特化させるような形でどんどん展開していきたいなと思っております我々狙っているところとしては日本語の同時通訳においてはとりあえずまずは精度を世界一にして同時通訳精度と知恵のバランスでも人間の同時の方々と同じぐらいそれを超えられるぐらいの精度を提供できていければいいかなと思っておりますあと2つ技術的に非常にエクサイティングだなと思っているところをお話しさせていただきたいんですがこれは生成AIでも音声スピーチのAIでも同時通訳では全く同じところであるんですがやっぱり業界特化とパーソナライゼーションというのは非常に重要なところなのかなと思っておりまして業界特化だと同時通訳はやはり非常に分かりやすいところがあるかなと思っておりまして例えば医療なら医療に特化した通訳法律なら法律に特化した通訳テックならテックにフォーカスした通訳それぞれのところでやっぱり人間の通訳の方がやられるところだとなかなかお作法とかがあって難しいとこれをAIなら業界特化をする形でいろんな分野を柔軟的にカバーできるような形で通訳できるとそういったところを例えばラグであったりだとか辞書登録そういったアプローチを含めてサポートできるようになるということを業界特化のところでは考えておりますこれをやろうとしたときにちょっと苦毒はなってしまうんですがやっぱりインフラでスケールされたようなAIモデルデータをいっぱい見てポストトレーニングを積んだようなモデルではないとこういうことができないのでそこでもやっぱりスケーリングのフレーバーが入ってくるのかなと思っておりますそれに加えてですねパーソナライゼーションのような話は業界特化もいいんだけどそれを例えば個人の輸出ケース個人の輸出ケースとか個人が使う音声に特化させていくことでよりそれぞれの人のユーザー体験に合わせて精度も上げていきましょうという話もあってですねこれは例えばiPhoneとかにデプロイされたときにデータセンターとエッジがうまく連携するような形でAIのモデルをアップデートしてセキュリティもこの辺も担保したような形でどんどん特化していきましょうみたいな話があるのでそういったところで今後技術開発が行われていくようなことになるのかなというふうに思っておりますこれも本当に私の話の最後のスライドになるんですけれどもこういったところを踏まえて言葉にとってやっぱり先に進むこういったスピーチAIモデルを作るときに欠かせないのが今後も計算資源GPUになっていくのかなと思っておりますこういったGPU具体的に弊社はどういったところに使っているのかというと大きく分けて3つのところに使っております1つ目はですねAIのモデルの学習を行っていくときに使っていくもう1つはこういったときこういった開発したモデルをですねサービスとして展開していくときに推論のところで使っていくと最後はですね学習を行うときに音声のデータを前処理する必要があるんですこういうデータ準備前処理のところでもGPUを使っていくと大きく分けた3つ3点ありまして現状でそれぞれのこの用途とでどれだけGPUを使っているかというと今日の時点では今日の時点では学習に90%システムの推論に1%データ準備に9%そういった比率で使っておりますただこれがあと3ヶ月後4ヶ月後3ヶ月後4ヶ月後と失礼いたしますこれ6ヶ月後ですね6ヶ月後の数字をエスティメイトしたんですが6ヶ月後どのような形になっていくかというと学習においてのGPUは40から50%推論が実は学習と同じぐらいの量までスケールアップするかなと思っていてそれが40から50%データ準備は5%ぐらいという割合にGPUのユースケースとしても変わってくるかなと思っておりますこれを踏まえた時に学習で使うようなGPUと推論で使うようなGPUのインフラって全く異なるようなところがありまして例えば学習であれば我々としては最新の強力なデータセンター用なGPUをマルチノード環境で使えるような環境であれば大規模の音声のAIを研究開発していけるようなとこういったそうですねデータセンターGPUというのはリージョンとしては日本がベストなんだけど日本じゃなくてもできるよねみたいな観点があるとただ推論は全く違ったようなところがありまして推論もですね最新のGPUがあればあるほど嬉しいとただそれ以上にですね我々世のような同時通訳のアプリケーションを展開しようとするとリアルタイム性知恵を少なくしていくということが非常に重要なのでそういった時に何が重要になってくるかというとやっぱりネットワークのアクセスの冷天使遅延をできるだけ減らしてあげるとこういったことを考えた時に何が必要かというと弊社のようなアプリケーションが日本のデータセンター一番データセンターネットワークの遅延が少ないところで運用できる環境が必要なのでこういったところでNVIDIAさんの推論向けのGPUを大量に使わせていただくことになるのかなというふうに考えておりますこれが私の最後のスライドなんですけれども言葉テクノロジーとしてはですねスピーチAIこういったところに革命を持たせらせていけるのはやっぱりスケーリングの考え方かなと思っていてスケーリングしようとした時に一番重要になってくるのはGPUのインフラだとGPUのインフラを考えた時にどこを考えていかなくちゃいけないかというとそれが学習に使うGPUもあるしインフラのスケーリングというところだと推論で使うGPUのスケーリングという話もあるのでこういったところを考えつつ引き続きNVIDIAさんのGPUのインフラを活用させていただきながらスピーチAIの研究開発サービス運用をますます進めていきたいなと思っている次第です私のお話はここまでになります弊社の情報発信我々が開発しているようなスピーチAIの情報発信は弊社の公式Xだったりだとかもし何かお問い合わせがあったらお問い合わせフォームの方からもお問い合わせしていただくことが可能ですのでぜひ何かあったらよろしくお願いしますそれではご静聴いただきましてありがとうございましたありがとうございました