 Music Thank you. Thank you. Thank you. Thank you. Thank you. It is what we will be covering today. First, I'll introduce the Grace Hopper Superchip and why it represents a breakthrough for LLM training. Then I'll focus on the use of the NVIDIA Insight Systems Profiler. And next, Karin will focus on the practical part of this presentation and show the output of a profiling session. Finally, we will conclude with best practices and some guidelines. Let's start by understanding the challenges of the additional accelerated systems. The point is that modern applications require both URI and parallel processing, making both CPU and GPU supercritical to this. On one side, we have the CPU, which is designed for serial processing. It excels at handling complex sequential tasks with precision and efficiency. On the other hand, we have the GPU, a powerhouse of power processing. It's designed to take on massive amounts of data simultaneously, making it ideal for tasks like deep learning and generative AI. However, these two powerhouses need to work in harmony. We need an architecture that is heterogeneous, accelerated and coherently coupled to truly unlock the potential of the hardware. If we reduce the bottlenecks and optimize the flow of data between the CPU and GPU, we can achieve levels of performance once talked to be impossible. Although we need the CPU and GPU to work in tandem, the traditional CPU-GPU communication via PCIe creates bottlenecks. The GPU can't access CPU memory directly and memory transfer speeds aren't fast enough to keep up with the large datasets without bottlenecking the GPU. We need an architecture with fast data transfers, coherent memory and simplified programming model. That's the reason why NVIDIA has developed the Grace Hopper Superchip to address these challenges head-on. It's built for the new era of accelerated computing and generative AI. It offers excellent performance across CPU, GPU or memory intensive applications, easy deployment and scalability, and it's super optimal for performance and total cost of ownership ratio. This maximizes data center utilization and power efficiency. Let's ask you a bit into more detail here. In the Grace Hopper Superchip, both the CPU and the GPU have their own memory system. While Grace is optimized for low latency code, Hopper is for high bandwidth and throughput. They are linked with this 900 GB per second NVLink chip to chip, which gives Hopper coherent and high-speed access to Grace memory and vice versa. So essentially, Grace can read Hopper's memory and Hopper can read Grace memory directly without moving any data. So this allows developers to create applications that utilize both processors using a single unified address space. So collectively, all these features enable faster training times, improved performance and the ability to handle larger and complex deep learning models. So this architecture translates into real-world performance gains. As shown in this slide, GH can deliver impressive training and inference performance for models up to 175 billion parameters and beyond. In this presentation, we are showcasing two tools. The NEMO framework for model training and the N-Site systems for the actual profiling. Let me give you a brief overview of both. So training a large language model is a super complex task, which requires an end-to-end solution. And NEMO provides exactly that. Integrates tools for data curation, optimized training, model customization and large-scale deployment. Let's take a closer look to all these components. So the NEMO framework can effectively extract, deduplicates, filters information from massive amounts of unstructured data. This is, of course, fundamental to utilize high-quality and relevant data at scale. Can efficiently use GPU resources across tens of thousands of nodes, leveraging advanced parallel techniques. By dividing the model and the training data, NEMO enables maximum throughput and significantly minimizes training time. And once your foundational model is trained, it can be easily adapted to a variety of tasks through fine-tuning, domain adaptation and guardrails to avoid unappropriate responses. NEMO supports the latest and most advanced techniques for fine-tuning, customization, alignment and reinforcement learning. So finally, NEMO integrates seamlessly with the NVIDIA Triton Inference Server to accelerate the inference performance, delivering cutting-edge accuracy, low-latency and high throughput. NEMO is a system-wide performance analysis tool, which is designed to identify optimization opportunities into your code to identify bottlenecks. NEMO allows you to visualize millions of events on a timeline, pinpoint gaps in CPU and GPU utilization and balance workloads across multiple processors. N-Sign systems provide a command-on-line interface for profiling. N-Sign√©es tracks. the NCSI's profile command tool, which is the main tool of this suite. It can be used to start a profiling session, decide which APIs to profile, including, of course, MBTX tags to mark regions into your code, and produce statistics and generate a file report at the end. Let me pass the floor to my colleague Karin for the second part of this presentation, including some experiments. Thank you, Giuseppe. Hi, everyone. So before we get started, a quick introduction about myself. My name is Karin Sevignani, and I'm a senior solutions architect at NVIDIA. I am currently leading the NVIDIA AI Technology Center initiative in the UK, where I collaborate with academics and researchers by helping them make the most out of NVIDIA technologies, both hardware and software. My expertise is in natural language processing, where I have a PhD in conversational AI. Let's now start with our experimental session. Today's experiments use a GH200 machine with one node and one GPU, running at full capacity with no power caps. The hardware includes a 72 core grace CPU running at 3.4 gigahertz, 480 gigabytes of RAM, a hopper CPU with 96 gigabytes of high bandwidth memory, and the InfiniBand for fast data transfer. On the software side, we're using Linux with a custom kernel, CUDA drivers, Mellanox, and NUGCC. Slurm handles job scheduling and singularity containers ensure portability. It's a robust setup designed to handle intensive profiling and experimentation efficiently. Let us walk through the steps to set up the environment for profiling. First, you'll need to pull the internal Nemo singularity image for GH locally. Then, grab a node in interactive mode using the a solid command specifying one node and a two hour time limit. Once inside the interactive session, run the container with the singularity run command to launch the Nemo nightly image. After setting up the container, you'll download the necessary components, the Lama 2 7 billion model, the Databricks Dolly 15k dataset, and the Nemo framework codebase. These steps ensure everything is in place for running your experiments smoothly. Here's the task for our profiling session for the day. We're profiling the fine tuning of a 7 billion parameter model with LoRa on a single GPU. But why are we using LoRa? It's a highly efficient method for fine tuning large models, as it reduces the number of trainable parameters, making the process faster and less resource intensive. For this case, a Lama 7 billion parameter model will not fit on just one GPU for fine tuning. Hence, we're using LoRa. We're following the Nemo playbook for setup and monitoring key metrics like CUDA, CUDA, and KUBLUS using the NSYS profile command. This captures detailed performance data to optimize our workflow. Here are the most interesting switches for this workload. We have Y360. This sets the profiling duration to 360 seconds or 6 minutes. We have D720, which specifies the delay before profiling starts in seconds. Here it was for 720 seconds or 12 minutes before beginning the profiling process. This can be useful if you want to skip the initial setup phases and focus on profiling the main workload. We have trace, DA monitor CUDA, QDNN, and Kublas and more. And finally, we have the O switch that sets the output file name for the profiling report. Let's now have a look at a quick video. So we're going to walk through the process of profiling a job from setup to execution. This video demonstrates the steps involved in setting up the environment, running the job, and analyzing the output. As you can see here, we start by allocating an interactive node and navigating to the relevant directory. This is where our profiling tools and training frameworks are configured. Let's set all the environmental variables and enable our Nemo container data to the data to the data to the data to the data to the data to the data to the data to the data to the data. We can see that we have already downloaded all the necessities for this to work. Our Nemo code, the Databricks dataset, and finally, the Lama 7 billion parameter model. Let's now set some job related environmental variables, such as the path for the model, the various datasets that we're using today, and the parallelism hyperparameters. For simplicity, we are using a single GPU, hence we don't have to parallelize anything. Let's now launch our fine tuning job. During this space, profiling tools are actively monitoring performance metrics such as runtime, memory usage, and GPU utilization. These insights are critical for identifying bottlenecks and optimizing performance. Once the job is finished and we see that the NCIS report file has been indeed written, we can copy over onto our local machine. We can copy over to the NCIS. Let's upload the file into NCIS systems so that we can have a look at it. This process will take some time, but we have sped things up today here. Let's now take a look at things a little bit more closely. Let's go ahead and see. Here's what we've got from our first profiling session. After copying the NCIS rep file to your local machine and loading it into NCIS systems, you'll see this timeline view. It gives us a detailed breakdown of CPU and GPU activity during the run. At the top, we can see how all 72 CPU cores were utilized along with the processes and threads below. The bars represent how busy each core is over time. Consistent activity suggests that this workload is well distributed across the course. The main process, Python 3, is responsible for running our workload. Let's work our way down, analyzing each section one by one. Let's start with kernels. These are the computational tasks running on the GPU. The most time-consuming kernel is the SM90XMMA one, which performs matrix multiplications, a key operation in deep learning. It takes up 49.9% of GPU time, indicating it's a major contributor to the workload. Other kernels, like elementwise kernel and vectorized elementwise kernel, handle elementwise operations, consuming smaller portions of GPU time. Then we have memory. Memory usage is minimal here, just 0.5%, suggesting that memory bandwidth isn't a bottleneck in this run. This indicates that this workload is indeed compute bound and not memory bound. Examples of compute bound processes are floating point arithmetic and intensive mathematical operations, like matrix multiplications. Whereas examples of memory bound processes are large scale data copying algorithms that require frequent memory lookups. Here's a detailed view of resource utilization for the main thread. The main thread, or PT main thread here, is the central thread coordinating the workload. Its activity shows interactions with libraries like NICO, peer communication, and MBTX for custom annotations. The green part at the top represents GPU activity, and while it shows high utilization overall, we can see some gray spaces indicating idle GPU periods. These idle times might be caused by delays in data processing on the CPU, or insufficient overlap between computation and communication. Below the GPU activity, we see colorful blocks representing CPU threads, which are responsible for tasks like preparing data and launching kernels. The orange bars indicate CUDA memory transfers between the CPU and the GPU. These transfers are sparse, which is a good sign. It means memory transfers are not dominating execution time. Asmost data already resides on the GPU, which is ideal for deep learning workloads. Let's zoom in on the Autograd engine in PyTorch, which handles automatic differentiation for computing gradients during backpropagation. The green sections in the timeline represent active computation or execution, while the brown dotted sections indicate thread preemption, context switching, or waiting for resources. You'll also notice blocks labeled pthreadcon wait, which show that application threads are blocked and waiting for a condition variable to be signaled. This often happens when the CPU is waiting for tasks running on the GPU to finish. Understanding these wait periods is critical for optimizing performance. Reducing them could improve overall throughput by ensuring a better synchronization between CPU and GPU tasks. Reducing them could improve the data and user-to-end data. Let's now have a look at a different run, where we increased the NumWorkers hyperparameter from 0 to 4, which enables multiple data loading threads. To go a little more into the specifics of it, NumWorkers controls how many sub-processes are used to load data in parallel while training. When NumWorkers is set to 0, the main process handles all the data loading, which can become a bottleneck, especially for larger datasets or when batch preparation is complex. By increasing it to 4, we allow multiple workers to load the data concurrently, significantly reducing data loading time and ensuring that the GPU remains active without waiting for batches to be prepared. This change has significantly improved performance on three different levels. Number one, the GPU utilization is now more consistent with fewer idle gaps in kernel activity. This means the GPU is staying busy with computations rather than waiting for data. Number two, the memory section shows regular patterns of activity, indicating better pipelining of data to the GPU. This ensures that data is being transferred efficiently and minimizes delays. And number three, we can notice several improvements on the main thread activity. The main thread now has more consistent green regions, showing active computations. There's less time spent in blocking states like the thread can wait, meaning threads aren't waiting as often for resources or synchronization signals, reflecting better intel-leaving of computation and data movement, hence a smoother execution. By making this simple change to NumWorkers, we've already seen significant improvements in resource utilization and overall performance. So let's now have a look at the uploading. But what is uploading? CPU of loading of activations is a technique where intermediate activation tensors, typically stored in GPU memory during model training or inference, are temporarily moved to CPU memory. This is especially useful for large models that require more GPU memory than is available, as it frees up space for other operations like forward and backward passes. By uploading activations to the CPU, we can handle larger batch sizes or train bigger models without running out of GPU memory. Uploading reduces GPU memory usage, enabling larger models or batch sizes to be trained on limited memory GPUs. What are the downsides of uploading? This comes at the cost of increased synchronization overhead and reduced GPU utilization. While uploading is beneficial for memory-constrained environments, it may not scale well if CPU resources become a bottleneck, or if bandwidth limits are reached. We can see from the Ansight system timeline that there is an increased CPU activity due to the activation of loading stage, while we notice several and more frequent gaps in GPU kernel execution, which is likely due to the increased synchronization between CPU and GPU. As our last experiment, we ran the same script leveraging the unified memory on Grace Hopper. Here, we compare the performance of two workloads, LoRa fine-tuning on the left and full Llama 7b fine-tuning on the right. Both leverage unified memory, but their behavior and resource utilization differ significantly. With LoRa, we can see consistent GPU utilization and minimal memory migration, as only 1.1% of memory operations involve unified memory. This shows that LoRa's parameter efficient approach keeps most data on the GPU reducing overhead. In contrast, the full fine-tuning workload shows frequent idle GPU periods and significant memory migration, with 9.8% unified memory activity and heavy host-to-device transfers. The comparison highlights how LoRa optimizes resource, while full fine-tuning leveraging the unified memory handles larger workloads with varying efficiency depending on memory access patterns. Although leveraging the unified memory on GH200 allowed us to fit a larger model onto one single chip, which was impossible before, using unified memory is not as straightforward for scaling deep learning workflows. We saw on the right side that the GPU is frequently idle due to the significant memory migration. We saw the training job on the right side that we saw in the right side of the GPU. We saw the training job on the right side of the GPU. Let's introduce some additional best practices that we can follow to optimize our workflows. Optimization is all about improving efficiency, reducing bottlenecks, and achieving better performance with the resources that we already have. We also have a lot of different methods that we can use to optimize our training. We also have a lot of different methods that we can use to optimize our training. Other than offloading and leveraging unified memory, another way to optimize training workflows can be by using mixed precision training, which combines lower precision formats like FP16 with higher precision formats like FP32. The goal here is to optimize performance without sacrificing accuracy. Mixed precision training offers three key benefits. An increased training speed, as lower precision formats like FP16 require fewer computational resources, allowing faster processing. Memory efficiency, as lower precision formats reduce memory usage, enabling larger batch sizes or larger models to fit into GPU memory. And finally, maintained accuracy. Loss scaling techniques are used to preserve small gradient values during backpropagation, avoiding underflow issues and ensuring the model converges as expected. To conclude, we have FP8 training. FP8 takes mixed precision training a step further by using an even lower precision format than FP16, while maintaining accuracy through advanced loss scaling and numerical stability techniques. FP8 takes a step further by using FP8. While FP8, with FP8, we see even greater improvements in speed and memory efficiency. This allows us to train larger models or process more data in less time, making it ideal for cutting edge applications like large language models and transformer-based architectures. Hopper GPUs sensor cores are fully optimized for FP8 operations, ensuring that we can push the boundaries of both performance and scalability. Here's why FP8 is so exciting. Improved performance. As FP8 reduces computational overhead, meaning your forward and backward passes run faster. On Hopper GPUs, this translates to significantly higher throughput compared to FP16 or BF16. Reduced memory usage. As FP8 cut memory requirement in half compared to FP16 or BF16. This means that you can train larger models or use bigger batch sizes without worrying about running out of memory. We have cost efficiency. By saving memory and speeding up computation, FP8 reduces the number of GPUs you need for large-scale training, making it more cost-effective for big projects. And finally, we have comparable accuracy. So despite using lower precision, FP8 achieves accuracy within 1% of BF16 baselines across various tasks. This is possible thanks to advanced numerical stability techniques built into Hopper GPUs and NVIDIA software. Now, there are two types of FP8 formats that we can choose from. E5M2, which has a wider dynamic range and is great for encoding infinites, NANs and zeros. E4M3, which trades some dynamic range for higher precision, offering more granularity between values. These options give you flexibility depending on your model's needs, whether you prioritize range or precision. To conclude, we've seen an overview of the NVIDIA GRACE Hopper Superchips and why it is useful for large language model trainings. We went through different experiments with LORA fine tuning of a 7 billion parameter model. We analyzed each experiment using NVIDIA Ansight systems, learning how to interpret some of the colors and allocations on the timelines. And finally, we talked about a few techniques that can be used to further optimize model training. Before we wrap up, I want to leave you with one final thought. Everything we've discussed today is about one thing, pushing the boundaries of what's possible in AI and machine learning. The tools and techniques that we've experienced aren't just about solving today's challenges. They're about preparing for the future. As models grow larger and more complex, and as demand for efficiency and scalability increases, innovations like this will be what enables us to stay ahead. So, as you go back to your own work, I encourage you to experiment with these optimizations, explore how they can fit into your workflows, and think about how they can help you achieve more, faster and smarter. Thank you so much for your time and attention today. If you have any questions or want to dive deeper into anything we've covered, we'd be happy to address them in the chat or via email. I also want to thank my colleague Giuseppe for his part of the presentation.LYI Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.