大家好,我是来自职业跳动C的语音团队的成为在团队内主要负责模型的性能分析、优化以及实现以及我的同事郭毅主要是负责模型的量化、裁剪、振流等方向那么首先感谢大家来听这次的会议这次会议主要由我和郭毅来给大家分享下我们在大语音模型下的两比特传重压缩方案以及基于TRT-LM定制的两比特科诺的一个实现和优化OK,那我们开始今天的会议我们会从三个主题给大家分享第一个是两比特量化方案的实现原理也就是我们抵抗包Q的一个实现的一个方案第二个是基于TRT-LM的一个两比特算子实现第三个是两比特在字节语音内部的一个部署那么首先由郭毅同学给大家分享一下DECAPOQ的两比特量化方案的实现原理好的,我们先简单介绍一下量化的背景量化只在减小神经网络模型的数字进度表时它将一个高进度的浮点性转化为低进度的整形或者浮点性从能够节省存储IO以及加快计算效率一般来说,在大模型的解码阶段每次只解码一个token从使得GPU的计算没有饱和模型解码的overhead就主要集中在IO部分因此,WitOling的量化就有着重要的研究价值在这次分享中,我们只研究两比特WitOling的量化,即W2A16每个Wit占据两个比特的元素Acclification保持为16比特在FP16转化为Intel2的过程我们称之为量化过程Intel2转化为FP16的过程我们称之为反量化的过程在Transformer的Decode阶段Linear Layers的IO通常占据着较大的overhead我们于是重点量化这一块对于Linear Layer的位置量化到低比特以后它既能加快IO,也能降低模型的存储一个Linear Layer表示为如下形式Y等于XW0加B其中,W0是已经训练好的高精度的weightX是输入,B0是BIOSY是Z层的输出为了反量化的方便我们关注于线形均匀量化于是从FP16到Intel2的一个简单印象是如下的准线性表示这里的S和Z是需要有量化算法来求解的系数它就是我们通常所说的Skale和Zero Point然后再加上这个Round Clip就最终将weight的数字确定在了-2,-1,-1的表示范围内也就是Intel2的表示范围这个式子就代表着经典的量化过程然后反量化的时候我们只需要用S乘以W hat再加Z得到Wθ我们可以看到在数字上面Wθ是接近于W0的我们接下来所要介绍的DecoupleQ量化算法它不再关注这个量化公式而只需要关注这个反量化公式前面我们介绍了量化的基本背景下面我们重点介绍DecoupleQ量化算法首先我们构建这个公式所代表的优惯目标函数这个优惯目标函数的意思也很明显就是希望量化前后这个Linear层的输出的RLL要尽可能的小这里没有了BIOS是因为BIOS的计算和存储在整个过程中的粘比特别小所以我们一般不量化BIOS而这个式子经过简单的变形就得到右边这个形式这里的H等于X转值乘以X当交准数据量足够大的时候H大概率是一个正定矩阵当然我们也可以对它的对角性元素全部加上一个小量以确保后面数字计算的稳定性然后我们把前面那个反量化式的式子带入进去以后就得到下面这个式子一般来说在per channel量化中wtta的每一行是独立进行优化的每一行都有它自己的S和Z所以我们只需要关注其中某一行即可然后这个优化目标就可以写成下面这个式子我们重点看一下这个元素条件这个元素条件是对任意的I取值从1到输入维度wi-1下于等于0fuwi-2下于等于0wi所以整数这个元素条件就是说对于w的每一个元素wi它的取值范围都必须是大于等于fu2下于等于1然后属于整数也就是取值是fu1-01这四个数然后对于S和Z没有任何元素然后这个优化目标函数就可以重新地简单写成下面这个式子这里的W乘以S加Z其实就是反联合过程而这个B就是这个W0所对应的那一行从这个式子我们可以看出这个式子它是一个严格的一个2函数然后这个2函数它在优化的时候它对S和Z没有任何约数它是一个严格的2函数然后对于W来说它是带有一些理想约数的在求解的时候可能会比较麻烦我们构建了这样一个优化目标以后我们就可以抛弃传统的量化范式不用再关注量化的各种细节和区域而只专注于求解这个优化问题就行这是Decalbq相较于传统量化算法的最大的差异我们这里把W、S、Z都当成是独立的优化变量只是W的取值受限于-2、-1、-1这四个数S和Z的取值没有任何约数直观来看这个优化目标很难得到一个全局最右解但我们可以尝试一些局部最右解在我们的工作中我们使用交替叠代的方法来求解W和S、Z在求解S、Z的时候它是一个关于S、Z的5约数的二次函数所以可以直接求导令导数等于0然后立即得到最右解在求解W的时候我们把S、Z固定起来作为一个离散问题来求解比如使用GPTQ里面的求解方法或者其他的力度投影梯度效价的方法然后我们求解了S、Z和W以后然后我们交替的把这个过程交替的迪在几次就能够得到一个局部最右解当一个block内的所有linear layers全部被量化完毕以后这个block的参数就可以分解为整数部分W和辅点部分S、Z了然后我们可以在一个block单位内继续优化在我们的工作中我们固定住整数部分W使用梯度效价来调整S和Z优化目标是量化前后的block输出差异最小化在这里需要强调一下我们这一步固定住的整数部分W是因为它占据着绝对多数参数量如果打开训练就很容易过离合并且离散问题训练起来也不是很稳定我们这次的分享是专注于PTQ但事实上作为Bolus如果下游指任务的有标签数据机是存在的我们可以用它在整个模型层面来Fenture我们所有的辅点部分来进一步提升模型效果这个是一个非常清凉级的训练因为我们固定住了占据绝对多数参数量的整数部分止Fenture浮点部分这一步其实有着很广泛的应用比如我们可以在任何开源量化模型上面用自己的数据机去对模型进行微调微调的时候我们都可以固定已经量化的整数部分止微调浮点部分SJ从我们前面介绍的DecubleQ量化算法中我们可以分析总结出它的highlightsDecubleQ将模型参数分解为整数部分W和浮点部分SC然后将一个神经网络的量化问题转化为有约数的优化问题这么做的好处是很明显的它不再需要关注量化了各种细节和区域而只需要关注这么求解这个优化问题下面这种表格展示了DecubleQ在NAMA模型上的测试效果我们可以看出在各种量化配置下面DecubleQ均能达到SOT的效果接下来由我来给大家继续介绍下两比特的Wid-only量化的Know的实现方案W2A16的算子和TRT-ALM中的其他的Wid-only的算子基本保持一致主要分两阶段预处理阶段和Know运行阶段那么在预处理阶段主要是把量化工具产出的两比特的模型的Wid转换成Know支持的Wid的格式模型产出是Int8的数据格式一个8比特存储的是一个2比特的数据在预处理阶段需要对数据做Pack即8比特存储4个2比特的数据以减少显存占用在Know的运行阶段会先从Global Memory中加载2比特的Wid到Shared Memory再从Shared Memory中加载2比特的数据到寄存器然后在寄存器中对数据转换成BF16再反量化最后再做BF16的MMA的矩阵乘法那么在实际部署中可以将预处理的过程集成到量化产区的工具中这样模型存储的大小可以减少4倍可以减少部署的时间接下来就要开始分享三个预处理阶段的过程一个是需要在K维度上做Transpose最左边是我从英伟达的官网检取的MMA.M16N8K16的Wid在一个Warp内的线程中线程的寄存器中对应的数据在一个Warp处理8乘16个数据每个线程加载4个BF16的数据一共两个32位的寄存器对T0线程来说需要01894个数那么对于W4来说因为一个32位的寄存器可以存储8个4比特的数据因此我们需要在K维度上重排使其在Dquant之后可以保持和BF16的MMA的寄存器中的数据排布保持一致中间展示了W4的数据排布方式通过把数据Receive为-1-442这样的排布之后再对中间两维做Transpose就可以使T0线程获取到的数据包括018916172425这8个数和原始的BF16的MMA拿到的寄存器的数据是一致的这里W4的一次Load可以在K维度上做4次MMA对于W2在处理上依然是FollowW4的实现不过一个32位的寄存器可以保存16个2比特的数据因此需要把数据Receive为-1-8-4-2之后再对中间两维做TransposeW2的一次Load可以做8次MMA第二个预处理是将数据从NK的布局转换成为N除4K除64464这样的一个布局这里是因为一个ThreadBlock的K是按照64来做分块的为了让一次Load满足一个Catline对应的128byte的大小W4的Weight最后的两维转换成4乘4和64是因为4乘以64再乘以W4的W4Weight所占用的byte数也就是0.5然后等于128byte那么同样的对于W2来说需要把NK的布局转换成N除8K除64864那么8乘以64乘以W2所占用的字节数也就是0.25也等于128byte那么第三个预处理是包括了将所有将有符号数转换成无符号数对于W2来说就是将一个-2到1的一个数据范围加上一个2变成一个0到2也就转换成一个无符号数这里是为了减少反量化过程中符号的处理另外同时会将一个8bit的大小压缩成一个2bit也就是一个8bit存储4个2bit的数据并且对数据做既有位置的一个重排这里是将下标为偶数位的放在一起下标是奇数位的放在一起这里重排是为了方便后面反量化过程中可以并行处理在介绍完预处理之后再介绍下Know运营时的一个关键的销改及数据加载的方式我们从Global Memory到Share Memory再到寄存器的一个修改我们从Global Memory到Share Memory的存储W4和W2的存储是一致的即每个县程读取16个字节每8个县程fit一次catch line对应的128bit的大小每一行需要做一次重排那么重排的公式也放在这里这里跟其他的跟之前Catlas的一个标准的计目的方式是一样的那么这里是为了方便后续从Share Memory中漏的是减少Share Memory的Bank冲突另外从Share Memory中漏的数据时对于W4的算子来说前面提到会对W8做一次重排并且在K维度上的CTI的分块大小是64那么这里64个数据是对应着32个字节那么两行之间间隔正好是32字节所以T0县程从偏移0位置开始访问那么T1县程从偏移32字节的位置开始访问对于W2算子来说64个数据是16个字节两行之间间隔16字节所以T0县程从偏移0开始访问T1县程从偏移16个字节的位置开始访问这里主要是修改了Catlas原始的这个W2的这个间隔的排布的访问的方式最后再介绍一下W2A26反量昂化过程中的一个难点也就是W2到FP16的快速转换的逻辑这里只贴了到FP16到BFP16也是类似的一个逻辑另外这里的截图只保留了前四个数的反量化的代码是因为其余的是相同的逻辑所以把代码忽略掉了这里是主要是为了方便我们快速的理解代码那么前面介绍了在一个32比特的数据中存储了16个2比特的无符号的数据并且对机偶做了一次重排那么在反量化的时候首先会通过LOB3指令与0x003003做运算得到第0个和第1个元素从这里也就明白了在预处率阶段需要做机有重排的原因是为了这里做并行的反量化即一个指令可以做两个数的反量化然后再与0x64006400做或运算将第0个和第1个元素转换成了加1024之后FP16对应的数据表示另外前面因为把数据加2得到了无符号的数码所以这里需要通过sub.f16x2指令来减去1026得到原始的原始数据的对应的FP16的一个表示这里的FP16的1026对应的是0x6402和6402那么到这里我们就将一个W2的数据转换成了FP16后续只需要再乘以scale并加上0point就得到了反量化之后的数据这一页主要是分享在英伟达模款GPU上针对LAMA和MIXTURAL 8x7B的GIM的不同进度下的蛋白质的一个性能主要包括W8A16 W4A16 W2A16这三个Witonly算子的一个延迟那么横坐标表示不同GIM的尺寸以及不同进度的算子重坐标表示算子的latency单位是微秒左边这张图是LAMA的7B 13B 30B和65B模型下对应的GIM的一个性能其中W8A16采用Puchano的量化方式W4A16和W2A16则采用细粒度的量化方案主要是在K纬度上是Groupwise并且这里的Groupwise是128在N纬度上是Puchano可以看到在该款GPU上W2A16相对于W4A16有1.04倍到1.86倍的一个性能提升并且从这张表上可以看到Wit的size越大加速比越高右边这张图是MixTro8×7B的一个性以及Group GIM的一个算子性能其中的G2409628672表示的是Group GIM的Group是24096表示的是K28672表示的是N算子的量化方案和前面的Lama的那个W8W4W2都保持一致我们可以看到在MixTro模型上W2A16相比于W4A16也有1.03倍到1.83倍的一个加速比接下来分享下低比特算子在字节语音场景下的部署的方案在选择部署方案的时候我们可以根据部署的硬件的计算密度和业务的特定的场景比如低延迟或者高吞吐等特点选择不同的部署方案图表中绘制的英伟达模款GPU的FP16 W8A16 W4A16以及W2A16这四个算子的BitSize和理论吞吐的关系局限横坐标对应的是BitSize横坐标对应的是BitSize横坐标是对应算子的理论吞吐单位是TFlops这里因为矩阵乘法的NK这里因为对于低延迟算子来说我们一般矩阵乘法的NK是远大于BitSize的因此GIM的计算密度可以简化为2m乘以1除以Wit数据类型所占用的字节数从图表中可以看到对于W4A16来说当BitSize等于34时处于memory bound的场景当BitSize大于34时则处于computer bound的场景另外当BitSize等于34时W2A16的理论性能是优于W4A16的算子的也就是这段画斜线的区域那么当BitSize在这段区域之内的时候我们可以采用W2A16的算子因此当实际业务的频发小于干预之时我们可以考虑使用W2A16进行部署那么通过这张表格我们可以帮助我们结合业务的特点比方说低延迟还是高等图或者硬件的一个特性来选择合适的部署方案那么最后就是我们因为我们前面已经分享了如何基于TRTRM定制W2A16的算子以及如何根据业务的特点和硬件特性去选择合适的部署的一个方案那么这一页主要是分享我们是如何使用TRTRM部署我们的一个模型那么在我们团队内部呢主要是将TRTRM当做一个算子库接入到我们内部的高性能的推定引擎中那么最上层是我们的API层包括了Client和Solar端的一个API下一层是调度层主要是做Batching的管理显存的管理以及一些Offload upload这样的一些逻辑再往下则主要是模型和Layer层模型基本包含了目前主流的一些模型结构那么最后一层则是Kernel层其中就包含了TRTRM算子的一个接入包括了WitOnly的算子以及混合量化的算子右边的表格表示了使用我们的DecoupleQ这样的一个W2A16G64的一个配置进行量化在两个任务上只需要25到37卡单卡只需要25到37小时就可以在语音ASR的W2E2指标上做到基本无损那么通过我们的量化目前这个数据已经可以做到单卡的7到8个小时就可以做到无所了那么以上的分享就是本次的一个全部内容了再次感谢大家来参加这次的分享这是本次的一个全部内容了再次感谢大家来参加这次的分享我们下期再见我们下期再见