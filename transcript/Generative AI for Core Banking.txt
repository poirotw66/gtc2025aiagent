 Hello and welcome to a presentation about Genitive AI for core banking on premise. Today I'm going to talk about something truly transformative. How Genitive AI is reshaping the core banking landscape. We will look into how AI is enhancing workflows, improving user experiences and making banking smarter and more efficient. We're witnessing a shift in how financial institutions operate. Genitive AI is no longer just a futuristic concept. It's actively optimizing banking workflows, refining users journeys and revolutionizing query handling. A head-driven automation can now interpret customer requests more intelligently, personalize financial interactions and provide instant contextual responses. This means faster processing, more intuitive sales service options and overall improved banking experience. Of course, bringing AI into core banking isn't simple. Developing and deploying the first Genitive AI solution for banking required overcoming major challenges, especially integration and security. Banks operate within highly regulated environments where data privacy, compliance and system security is critical. Today I will walk you through some of these challenges and solutions we implemented to address them. A key innovation that made this possible is our semantic layer. Think of it as a translator between complex banking data and AI models. Banking system stores vast amount of structured and unstructured data, often fragmented across multiple platforms. Our semantic layer standardizes and organizes this information, making it AI compatible. The result? More accurate and context-aware AI outputs. AI smoother integration process and banking systems that truly understands and responds to user needs in a meaningful way. None of this would be possible without robust technology. That's where NVIDIA plays a crucial role. NVIDIA's optimization techniques enhance model inference, making real-time AI interactions faster, more efficient and cost-effective. This level of performance ensures that banking AI solutions are not only smart, but also secure, scalable and ready for real-world financial applications. Teminus has actively embraced responsible AI, and this is our journey. In 2016-18, like many others, in the industry, we began with prescriptive and predictive analytics. This foundation allowed us to analyze past data and forecast future trends effectively. By 2019, we progressed with incorporating explainable AI into our solutions. This advancement brought transparency to our models and letting business analysts understand and trust AI-driven solutions. We began with AI-driven insights more deeply. Now, we are moving towards generative AI to further our commitment to responsible AI. This cutting-edge approach allows us to produce outputs that not only are innovative, also transparent and easily interpretable by using its users. These developments empower banks to deploy and implement scalable solutions faster and more efficiently. Additionally, we enhance Teminus internal operations by fostering an environment of transparency and accountability in AI usage. The core banking system is a beating heart of any bank. It consists of extremely complex databases with hundreds of thousands of tables, highly intricate relationships. Accessing this data is a complicated process. Currently, to extract data from a core banking system, you must connect it to costly analytics platforms and systems that only summarize certain parts of the data into reports. If you wish to go beyond these summaries, it requires assembling a team of experts who can translate business insights into SQL queries and navigate the database's complexity before returning information to the end user. This creates a significant barrier between business stakeholders and the data they need. Genative AI can offer a solution. However, according to Boston Consultant Group, only 10% of banks have made significant progress in applying Genative AI within their organizations. They have identified several main barriers. barriers to ensure security and safety. Even if a solution that interfaces with the core banking using Genative AI is developed, guaranteeing security and safety is crucial in the banking industry. As a barrier number two is integration with existing infrastructure. Many banks have legacy core banking systems that have been placed for decades. Integrating Genative AI solution directly with this complex system poses significant challenges. As a result, many institutions are still exploring how to overcome these obstacles. In the survey, only 34% respondents indicated that are underway in implementing Genative AI. However, we haven't completed that process yet. Only 10% said that have made significant progress in making Genative AI a reality. This is a main barrier that no one has overcome yet. We are uniquely positioned to address these significant challenges. While others are still thinking and planning, we have developed a solution. Our solution is only one that can plug directly into core banking system without any need for modifications. We can seamlessly integrate with the existing core banking system within the bank, ensuring minimal disruption. Moreover, we make sure that security and privacy are integral to everything we do. By prioritizing these critical aspects, we offer a solution that not only connects effortlessly, but also maintains the highest standards of safety essential for the banking industry. As depicted in this illustration, the traditional workflow without Genative AI demands more resources, it's time consuming, and do have multiple iterations required to fully understand and execute business requirements. In contrast, the Genative AI workflow requires significantly fewer resources and iterations. Our solution simulates the product managers planning process by providing insights into current product customer usage behaviors that are not readily available throughout the standard reports or analytics supplied by existing institutionalized analytics processes. We offer full transparency of SQL queries in a way that's understandable to non-technical users. This means that business users can immediately see if the queries are addressed effectively. Rather than just delivering the raw numbers, we provide data directly derived from business queries along with actual insights. Essentially, offering a support of 24-7 business analyst who explains what these numbers mean. This approach leads to significant fewer iterations, desired to put accuracy, and reduced effort. Our solution is scalable at the enterprise level. So, as your querying needs grow, there is no need to expand the team behind it. The system's ability to easily scale brings about transformational cost efficiencies. This acts as a job liberator, freeing up team members who were previously involved in these tasks to focus on more important strategic work. At the heart of our solution, there are three main components. First, we utilize large language models and we are entirely agnostic about which ones we use. We employ both open source and commercial models, offering this flexibility as an option for our customers. The second, we provide user interfaces and post-processing capabilities to ensure seamless integration and usability. Most importantly, we have developed what we call a semantic layer, which is proprietary of Temelus technology. The purpose of the semantic layer is to transform complex core banking databases into data that can be easily consumed, analyzed, and processed by the Genetive AI solution. The semantic layer is crucial because it bridges the gap between intricate banking data structures and the Genetive AI models. And they do efficient and effective analysis without the need for extensive modification to existing systems. Our semantic data model operates by establishing relationships between data as it is organized. This approach allows the data to carry interesting meaning without the need for human intervention or additional processing. Considering all the semantics of the words and resources material. The semantic layer provides a comprehensive overview of the data. We simply need the metadata associated with the core banking data. And then our solution works automatically. Here's an overview of how our solution works. The user logs into the user interface and provides a natural language query. Our system automatically converts it requested into SQL, which then executed on the database. The results are returned and used to generate actual insights. What is important here is that if the solution were done solely with the Genetive AI, the business users would receive only raw data. For example, just a number of customers who have certain product. The system would provide these basic figures without context. In a contrast, our solution offers enriched insights. We provide the customer with a business analyst level interpretation. As if a business analyst were sitting beside them 24-7. Summarizing the data and explaining everything in detail. This means the user doesn't just get numbers. We receive comprehensive insights that help them make informed decisions. What does this mean for banks and the entire financial industry? The solution will have significant impact on the financial sector. By enabling direct interactions with your data and extracting value insights. You can streamline your operations themes and actions. This empowerment allows for the automation of manual processes and improves responsiveness to specific customer needs. It will be able to summarize complex information into coherent narratives. Simplifying the creation of content in a consistent style along the way. This capability has the potential to revolutionize how banks operate in the years to come. By unlocking the immense value hidden within your data, you will be able to leverage it fully in your daily operations. Driving efficiency and innovation across your organization. What does this mean for banking business? For banking managers, this means that they have timely, ad hoc access to control data insights and narratives enhancing their ability to make informed decisions. For data analyst team, it enables faster generation of SQL code. Develop new complex analytics. This empowers sales and relational managers to provide timely and creative insights to clients. For customer service, it results in improved responses to customer queries leading to higher customer satisfaction. For bank clients, it offers easy and comprehensive answers to banking questions. How we position Gen.AI? Gen.AI can look enormous value for banking expertise and entire banking ecosystem. It enables the seamless sharing of the best practices across the industry and allow banks to offer hyper-personalized products to their customers. By leveraging Gen.AI, banks can tailor services to individual customer needs like never before, enhancing customer experience and driving innovation throughout the sector. By retrieving information instantly instead of wasting time searching for it, we can act on data immediately. We save significant amount of time and eliminate unnecessary work allowing the bank to focus its main attention on core objectives. Furthermore, this approach enhances the bank security and the auditing capabilities. Incorporating safe, responsible and traceable AI into banking and data security architecture ensures transparency and regulation and accountability. Factors that are extremely important for banking systems. In terms of benefits and delivery from business cases standpoint, we always prioritize delivering value. We ensure that every solution we provide offers complete value to our customers. We focus on value-based support to secure management buy-in within the bank. Our solution is exceptionally quick to implement and can be deployed in as little as 20-man days. Additionally, it comes with optimized GPU requirements developed in collaboration with NVIDIA, allowing for continuous optimization. Our scalable and modular approach adjusts seamlessly in changing business needs. We have always maintained an agnostic stance towards the large language models, ensuring we provide customers with the best-in-class options, giving them the flexibility they need. For our customers, we focus on enabling pivot towards greater productivity. By providing instant access to business insights, we can immediately address customers' needs and implement productivity changes to adopt accordingly. This capability offers us invaluable insights and act as a job liberator. Instead of having teams of pupils, spend significant time gathering and analyzing data, you can obtain the information instantly, receive comprehensive descriptions, and elevate your operations. The user experience is exceptional featuring fully human-like natural language interaction. Users no longer have to engage with rigid and highly specific SQL statements. This natural interaction allows us to scale efficiently and cover a wide range of use cases. Looking ahead, we aim to extend this functionality to areas such as financial crime detection, wealth management, and more. By doing so, we hope to unlock even greater value and innovation within the banking industry. This allows the banks with a generative AI-assisted product planning workflow. It allows to have significant onto the current product success potential, understand potential, marking size, and propensity, segment customer profile, turning down on usage behavior, and understanding usage and product trends. When we deploy the solution, we anticipate that customers will implement it on-premise within their core banking environment. There will be options to configure GPU clusters similar to those available in a cloud. This approach aligns with system-wide policies, ensuring that interactions are encrypted and subject to stringent authentication and authorization protocols. Importantly, we do not interact with live core banking systems. Instead, we operate on replicated core banking production databases that are deployed on-premise and made accessible to the TAML-List GenAI solution. Authentication will be managed using OpenAI ID and OAuthuth protocols. As user access to the databases is governed by ZACML and SMS policies. These policies specified by ZACML ensures that the user access is properly controlled, guaranteeing the most client aspects of the system, security and privacy. Regarding the first barrier we mentioned earlier, integrated with the bank's core system while guaranteeing security and privacy, we leverage existing solution rather than reinventing well. We directly interface with the robust security system that Amunus has established over the years. One system is called ZACML. It translates to Extendable Access Control Markup Language. ZACML supports the expression access control policies, allowing for fine-grained authorization measures. By utilizing ZACML, we ensure that our solution aligns with established security frameworks, providing a secure and standardized method for defining and enforcing access control policies. This illustration demonstrates how our security processes function in terms of accessing the application we are discussing. When implementing Genative AI as an alternative or running in parallel to an existing application, the solution acts as policy enforcement point. In this configuration, ZACML policies are defined on per rule per table basis. These policies and their associated obligations are applied equally in both scenarios. The user access the system throughout the Gen AI interface or using existing bank interface. By operating as policy enforcement point, our solution ensures that all access control policies are consistently enforced across different interfaces. This means that the same security measures, permissions and restrictions are in place regardless of how application is accessed. This approach maintains the integrity and security of the system while allowing flexibility of using Genative AI to enhance user interactions. In this diagram, we illustrate how authentication process involving an identity provider. The identity provider communicates with the Gen AI system by sending requests for authentication along with a token that includes user role. Upon receiving this, Gen AI system sends decision requests to the policy enforcement point. The policy decision point then determines the user role based on the provided information. Depending on the role identified, the system will either grant or deny access to the user. In essence, this role based authentication ensures that users are granted access only to the resources they are authorized to use, enhancing overall security and compliance of the system. Zero Trust Architecture, a secure framework based on the principle of Never Trust, always verified. This approach fundamentally changes how we think about security by assuming that threats can come from anywhere, both inside and outside the organization. By embracing Zero Trust Architecture, we are committed to delivering a secure, reliable and trustworthy AI solution. This approach not only protects sensitive data, but also builds confidence with our clients adhering to the highest standards of security, essential in financial industry. Our deployment architecture leverages cutting edge technologies to ensure efficiency, scalable and resilient operations. We use Kubernetes to deploy and manage applications at scale. Istio is serving as our service mesh. It addresses critical aspects like traffic management, security, observability and resilience. To enable serverless framework and event-driven architecture, we implement Knative. It simplifies deployment and routing, allowing us to build and run applications that can automatically scale based on demand. For deploying, monitoring and managing our machine learning models within a Kubernetes environment, utilize KServe. It streamlines the model serving process, making it efficient to handle machine learning workloads at scale. This deployment architecture ensures that our applications are robust, scalable and adaptable to evolving business needs while maintaining higher standards of security and performance. In this illustration, this illustration addresses the second generation of adoption barrier, integrating within the customer existing technology stack. To overcome this challenge, we deploy our UI front-end, API gateway, model management services and policy enforcement modules, are designed to interface directly with the customer's data. By having these components communicate seamlessly with the data layer, we ensure that our solution integrates smoothly without requiring significant changes to existing infrastructure. This approach allows our GN-AVI solution to function effectively within the customer's current technology environment, facilitating adoption and minimizing disruption. This diagram offers a comprehensive end-to-end overview of the entire process. It begins with the product release specification and details how we deploy on Kubernetes. It outlines the sequence deployment of the various large language models and the different genetic AI services we implement. This full-scale overview illustrates each step from development, deployment and showcasing how all components work together seamlessly. This diagram offers a specific approach to the user's data. In this diagram, we provide the process sequence. And starting this, the user begins by submitting the query. Then the system processes the query considering the database schema and identifying relevant entities. Then user roles and permissions are assessed using XAML policies to determine where a user is allowed to access. Then the query adjusted based on permissions is executed on a production replica of a product database. And the results are retrieved, sent for insights to the large language model. And the model generates insights and a comprehensive report is delivered to the user. If access is denied at any stage, the process stops ensuring security and compliance. So this concludes the presentation of the Temenos solution. And I'm handing over to Guilherme, who will speak about NVIDIA technology NIMS. NVIDIA NIMS were at the foundation of the collaboration with Temenos. NIMS is essentially a pre-built Docker container and associated Helm chart. And inside of this Docker container, you have engines that are optimized for model inference. Most of the open source models in the ecosystem are covered by NIMS. And so you can swap in and out between any model you'd like. NVIDIA NIMS is still a pre-built Docker node. NVIDIA NIMS is also a pre-built Docker container. So you can swap in and yeah. In specific with regards to Temenos, the advantages they found of using our technology for their specific deployment was that the deployment with Docker was very simple, made very simple. There was a seamless integration with their Kubernetes stack that you have seen in previous slides. LLM response and consistency versus VLLM was a key difference. So having the same question always get the same answer is not as trivial as you'd think it is. Latency for their specific level of concurrency sell between 30 to 40% improvements in latency versus VLLM. The ability to easily switch between different optimization profiles. So an M for a specific model might be optimized for latency or it might be optimized for throughput. So depending on your particular use case, you might want one or the other. So the ability to switch between these was really made working together more seamlessly. And finally, covering most and major open source models is a big advantage for not only this project to be able to swap in and out. You know, try a LLM model, then try a Mistral model, then try a Quen model. Switching between these seamlessly, both for testing, but also for future plans to extend these sort of generative AI workflows. The deployment with the deployment with the name is fairly simple. It's especially running on a local machine. It's a single Docker command that pulls the container from the NVIDIA cloud. And you see the text, what hardware is running on. So if it's an A100, H100, L40s, it figures out what hardware it is and picks the most optimized model inference engine for that specific hardware. If we don't have an optimized inference engine, it will build it locally with tensor RT, LLM on the spot. So if for some reason you're using some hardware and a model that's an edge case that we haven't covered yet. The name will still build an optimized engine, but on the fly rather than a pre-built one. And you could think of the interaction with NIMS as a very simple HTTP REST API. And this API is actually the sort of industry standard one that was made standard by OpenAI and now has been sort of adopted by VLLM and NIMS. So you can do your HTTP requests through the completions endpoint, and you can get things such as liveness, health checks and metrics through that same exact API. The two names used for this workflow covered in the presentation were the LLM3 SQL Coder 8B, which is a model that translates natural language into SQL code. And this model also supports structure generation. So if you want to make it such that your model's output always obeys, for example, a regex or a JSON schema, etc., you can enforce that with NIMS. And the second model used is a QAN 2.57B. So it's a model by Alibaba. And this is essentially used to explain the output of running the SQL query in a natural language to the user. So it adds this layer of explainability to the whole system and removes sort of some of that black box element that deep learning has had thus far. And it makes everything more interpretable. A little aside on structure generation with NIMS. So essentially, yeah, you can provide a JSON schema, regular expression, context-free grammar, etc. And if you have a sort of workflow where you need the model to obey particular configurations, what you can do is condition the output such that anything that doesn't obey your schema gets excluded from the space of possibilities of what the model will say. So in this diagram here, you can see a very simple example of limiting the model's outputs to be only one, two, three, four and five. So the probability for any token that is not one between one and five gets masked to zero. Obviously, this is a fairly trivial example, but this can be made possible for all types of context-free grammars. Finally, a simplified version of how the NIMS were integrated with the workflow that Mantis had presented previously in the presentation. So first you have the user query coming in. And this is a natural language query of sort of similar to what percentage of our clients under the age of 40 have an ISA account with our bank. And this first generates an intermediate representation with structured generation where the terminals have a defined JSON schema with extraction specifications that they want the model to obey for. So this produces this JSON output and then given this enforced output plus the SQL database schema, the model then generates the actual SQL query. This is run against SQL core banking database. Here we say PostgreSQL. It could be any other flavor SQL supported by this model as well. This produces the output. So in this case, made up number 15.3%. And then both the SQL and the output are passed along to the QAN 2.5 model, which provides an explanation of a natural language explanation of what the SQL query actually did so that users who might not be familiar with SQL can understand what the AI is doing. And so this is a simplified overview of how NIMS were used in the terminals workflow. And now back to Mantis. All right. So now we're going to look at the demo of the system in action. So here you can see the terminals GENV AI user interface. And this demo is being structured in two parts to showcase the capabilities and the features of the system itself, plus showing the safety and security aspects, depending on the user that uses the system. So first I'm going to be logged in as a branch manager called John, who has broader range of permissions and is able to track the platform more freely and has a built that ask different questions. And the second part to showcase the restrictions to accessing the data for users that have different roles, I'm going to look into the other user and ask the same question. And we're going to see how the system behaves in two different cases. So first of all, we're going to ask a simple question. And the question is provided in the input text field. And what's happening behind the scenes is an engine of the entire solution. Importantly, once the question is provided, the answer is going to be presented in plain English. And we're going to have in the first part, as we can see, insights being generated. And below the insights, we have representation of the data that's been retrieved from the database. So our question was, list the average transaction amount by occupation. And the results in our insights, as I mentioned, is not just plain raw numbers, but we getting information like it would be provided from analysts. We find out that the highest, the occupation that has highest average positive transaction amount is in first place doctors, followed by corporate services and followed by bank, bank and financial services. And the same data is represented in the chart that is provided below. And the system supports multiple flavors of the charts, depending on the data. And this can be changed based on user needs. So as you can see, I changed horizontal bars to linear chart. Also, for users who wants to access the data and see what actual data being retrieved for this question, there's option to show that data in tabular format. So this data actually is the real data that's been retrieved running on a database, the SQL statement that's been generated by one of large language models. And importantly, we expose all the SQL itself for transparency. For anyone who has access to the database or wants to do some tweaking in any shape or form, they can analyze the query SQL and see what's been generated. However, important aspect is for users who don't have knowledge of a SQL itself, we provide in plain English explanation what actually the SQL does. So this allows to use the system by the people who have a variety of expertise at different levels. Also, the platform allows to use the data outside the platform itself. So if you wanted to share the reports or share the information that you acquired through the system that someone else will store for later, you can simply export the report and the reports will be generated and downloaded in PDF format. So if we're going to click on report, we're going to see an example of a structure. So no surprises, it's the same information you have provided in UI. We have a question, the answer with these insights, SQL itself and SQL description in plain English, along with all the data and the graphical representation. So now once we know that the doctors have highest average positive transaction, we would like to dig deeper and understand what products arrangement might appeal to that profession. So we're going to ask another question. Which product arrangements appeal more for doctors? And as before, we can see on the left side that what's happening behind the scenes while the question being processed by different components of the system. And as before, we're not getting just plain text or plain raw numbers. We get the insights accompanied with plain English and numbers and statistics along with it. And we can get from insights that for doctors, most appealing product is current accounts. And it occurred six times in our demo database and is representing approximately 27% of the entries. In the second place, we have saving account and it appeared three times and it represents 13% of all occurrences. And not surprises, the same relevant information is accompanied and can be visible in a chart itself. So as before, the same information about the SQL, you can look at the information generated. And if you don't understand, you can read it out, what that SQL does for further interpretation. So this, as I mentioned from the beginning, concludes the first part of the demo when we show PowerUser, it drills to the specific topic and gets information and sites and able to make certain decisions or share information with someone else. So now I'm going to switch to the different user. And I'm already logged into the system. And as you can see, now I'm demo user. And I'm going to ask the same question as before. And the expectation is that it should go through the same process itself, but because of the restrictions of his role, the system policy enforcement service will intercept the request, the user's role, the user's role, and the user's role. And understand that this user is not allowed to see the data. It should prevent it. And exactly that's what happens. If the user asks a question and to get the answer for this question, it requires to access information that is not allowed, his request will be prevented with the polite message that he's not allowed to access the data. So this short demo demonstrates not only the power, flexibility, speed of genetic AI, but also data security aspects important for banking sector. So this is the end of our presentation. Thank you.