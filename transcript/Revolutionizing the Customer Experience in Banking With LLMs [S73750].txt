 Thank you. Thank you. Hello, everyone. Today, we will be sharing our journey with LLMs in a banking environment. And we will share our experience and whole journey, whole steps. This is Oktaş Şahin Olu from Ishbank. I'm working as a chief data scientist. And this is Çağlar from Ishbank. And I'm working for chief AI architect. Okay, let's start. If we go through our agenda, today, actually, we start from bottom and go to the up. And we will describe all the steps and what we have gone through. Let's start with the investigation phase and go through the future work. Okay. At the beginning, when the LLM are released in 2023, actually, we started an investigation. We started an investigation about on top of three subjects, which are technology, business requirements, and infrastructure. From technology perspective, we started to investigate the LLM structures, how they are working, what type of models behind them, and also we go through the open source community, hiding phase, libraries, implementation examples, and so on. From business perspective, we had lots of meetings with the business part and the corporate architecture department to investigate what type of use cases, potential use cases, that we can implement on top of this technology. And these three items in context, question, answer, summarization, and translation actually come into table, come onto table. And we started to focus on these three subjects. From infrastructure perspective, we had already GPU farm, lots of GPU servers. But with this new technology, actually, the GPUs that we have was the previous versions, and they need to be upgraded because of the new technologies coming with the LLMs, like quantization, D-types, B-flow, 16D-types, and that kind of stuff. And with the IT guys, we are, we actually planned new installations, new GPUs, and installations of these new resources on top of our development and production environments. We are using Kubernetes environments, so adding these resources to Kubernetes environments, we work with the IT guys. And after that, we started, I mean, we jumped into the next stage, which is the foundation model. Yeah, as a next step, foundation, we investigated all the released LLMs to choose a foundation model for us. To be able to evaluate these released LLMs, we have created a validation set. But, you know, the Turkish performance is very important for us, but also we would like to understand the capability of the LLMs. Because of that, we have created two different validation sets. One is Turkish and the other one is English. Because of that, we have created a validation set set for us, but with English data set, we tried to understand the capabilities of the models. We have chosen different types of models, as you can see here, and made a validation on all of them to choose one of them as a base model. what we have found after this evaluations we noticed that the turkish performance were poor because all of the lms released at that time are all pre-trained on english and the turkish performance were poor really poor because we didn't get received any relevant answers to our turkish questions but it's okay according to the english performance we have chosen a model uh it was the leader of the leaderboard at that time on hiking phase and we it is selected as our foundation model but we also noticed that the turkish fine tuning was needed because of that we have created we had to create that training data so let's jump to the training data part preparing the training data was not easy for us because not enough turkish data was a big challenge for us there are lots of training data in the open source community but uh most of them were in english uh because of that uh we started to uh gather all the turkish data that we can from web crawling open source data sets open source any kind of open source uh or data sets uh we gathered all this data and for uh different types of tasks uh that we noticed during our uh business uh meetings uh so these three tasks in in context question answer summarization and translation uh we uh focused on the data sets uh actually address these tasks but beside these data sets uh we also gathered unsupervised data to improve the turkish and one more thing that we have noticed during this uh process the translation data uh really improved our performance on turkish and fine tuning uh because all the models are pre-trained on top of english and giving the model the uh translation data from turkish to english and english to turkish really improved the turkish learning performance when we have a look at the when we have a look at the fine tuning part we have started another investigation so there are lots of things to do with the fine tuning so how we're gonna move forward so full parameter fine tuning versus adapter fine tuning quantization that we have to use we have to use or not multi-node multi-gpu training techniques and hugging phase training techniques and deep speed training techniques all these subjects uh were investigated and uh evaluated uh for fine tuning and let's continue with our decisions after this investigation quantization quantization you know the these models llm models are really big ones and the uh they're fitting in the gpu memory is a really headache because of that uh we evaluated the quantization but we noticed that the with the quantization the performance of the model uh decreased significantly in our use cases because uh as we all know the details of the weights with the quantization uh are reduced significantly uh because of that uh we have decided to uh go with the beefload 16 instead of the quantization but choosing this option actually come with that uh come with a problem uh now we need to think about the gpu memory utilization the next coming slide slides uh will be mentioning about these problems okay so another question was how do we go with the full fundamental fine tuning or adapter fine tuning as as i mentioned uh the in the previous uh slide uh the gpu memory is is a really big problem uh and from memory usage perspective uh adapter fine tuning uh has really uh significant advantage uh but secondly catastrophic forgetting is another thing that we need to uh avoid it uh because during the fine tuning uh we may break uh we may break uh the model or we may reduce the pre-training performance uh during the fine tuning process uh because of that we choose the adapter fine tuning instead of full parameter fine tuning but actually only the adapter fine tuning was not enough to utilize the gpu uh memory uh optimize the g memory usage uh because of that uh we have also investigated the deep speed technique with this deep speed technique uh for on top of multi gpu we have divided the model uh among the gpus but also we have divided the uh optimizer states and gradients uh among the gpus uh among the gpus uh and uh by this way actually uh we uh deal with the uh gpu memory utilization and it was a successful uh fine tuning uh after using all of these techniques uh together here uh we have used the deep speed zero state zero stage three approach okay after all this investigation and experiments uh we came to the execution point uh at this point we made our selections and we determine i determined our actions a foundation model was selected the adapter fine tuning selected a pytorch hugging phase and deep speed libraries will be used we have decided not to not use uh quantization because of that we used deep speed and as i mentioned in the training data part we have blended different types of data for different types of tasks because of that we had to develop our own data calculator for mixed data of different task types also we have created multiple instructions for different instructions for different tasks and also multiple instructions for the same tasks and all these instructions were generated and used in a balanced distributed manner when we have a look at our computer environment uh on top that the fine tuning will be running on top of that uh we performed our fine tuning on top of 18 server each has eight a 100 nvidia gpus with 80 gigabytes of ram and totally we used 144 a 100 standard gpus for fine tuning after fine tuning uh the first achievement uh the first uh outcome uh of our uh first fine tuning uh with the model a model a as a foundation model uh we have reached this amount of uh this amount of success for different tasks at the beginning only the in context question answer translation and summarization uh were our uh were in our focus uh classification and function calling were not in our focus at the beginning but as we all know every day new llm are being released and the performances of the lms increases day by day because of that during this journey uh after our fine tuning is finished we continue to investigate the new models which are released and evaluate them uh periodically and if we noticed a new model that is better than us then we switched to that model as our foundation model and during this journey we have switched our foundation model uh from model a to model b and currently we have achieved uh the amount of success that you have seen in the screen for different type of tasks and i would like also uh to mention about these numbers because these evaluations done by human scores and judge lm scores human scores and judge lm scores we are not using statistical measurement methods uh because they are not uh really working well for the free texts because of that uh we have uh human annotators so they all uh read the outputs and scores them and we took the majority and then we use the judge lms and we are using this course together to evaluate the performance and for question answer i would like to say that these numbers uh are the performance of the lm which means that the rack performance is excluded all questions have relevant context action and retrieval augmented generation is the essential part of the uh llm structures why we are using reg the major reason behind it is the minimizing the hallucinations and increase the answer accuracy and stability and i should generate the same answer because of the stability and the accuracy and by using reg also we don't have to train llm again and again for different uh type of domains uh because uh with the power of rack we are using the common sense llm and we are feeding the relevant context to the llm and this increase the application speed also and when the content of the domain is changed or updated we don't have to train our llm again and again on top of these reasons uh another major thing for us to choose the right uh we had already a retrieval model before the llm we already uh have implemented the retrieval model uh for intelligent document before the llms because of that we already had a retrieval model so we don't we didn't have to uh train and develop a retrieval model before the llm implementation this was another advantage for us when you need to implement a retrieval augmented generation so the domain knowledge should be documented and the business department should document their domain knowledge uh and you need the business engagement at this point uh and you as an artificial intelligence department and you need to provide your business uh how to prepare their document so we can be prepared how to documents for business units which includes the document type document structure and recommendations for reg besides this uh we prepared the validation set for question answering for each use cases to be able to measure the performance parsing and chunking uh development is another important point for reg implementation thanks to the half the documents that we have prepared uh parsing development was easier for us but the chunking uh is a was another challenge because the retrieval model has a maximum sequence length and you have to divide your contacts into chunks which should not reach the maximum sequence length of the retrieval model but uh slicing your uh document uh just according to the token size is not a good approach because uh the split point uh the split point may hit uh in the middle of a word or a sentences this means that your chunk adds with an incomplete sentences or words and the next chunk will begin with an incomplete word or sentences to prevent this uh first of all we divided uh uh the sentences we extract the sentences from the domain document and we chunk uh create our chunking the right thing according to the sentences and last uh thing that we have implemented to improve the uh right performance injecting extra information to chunks uh for example some sample uh some sample questions that will be answered by that chunk or some keywords or tags related with that chunk if you inject this information into that chunk if you think that might in hundreds and total assessment of uh what might be inserted so this fate that explained the journey that we had during selecting and fine-tuning the LLM models. And I will continue with the infrastructure platforms and frameworks that we use to run these LLMs and other GNI models. I want to start with a brief view of our infrastructure. The biggest challenge in building our infrastructure was due to regulations in our country. Banks are not allowed to use cloud services, so we had to build their on-prem infrastructure. So we started with setting up service with NVDA GPUs. First, we started with A100 and then added H100 to our infrastructure. And now this year we will grow our infra with H200. And on top of this infrastructure, we set up an Red Hat OpenShift cluster as a container platform. It's running on nodes with GPUs. And as an AI platform, we had an in-house developed platform. It started as an MLOps platform. And with the GenAI, we are now migrating it to the GenAI Ops platform. Also, a VectorDB is running on this infrastructure. And we are using it in LAC use cases that are being used at off-dimension previously. And as you all know, GPUs are scarce sources and they are expensive. So we have to utilize it as much as we can. So we decided to take the advantage of NVDA tools to optimize our GPU hardware. One of these tools is NIMP and it provides us a standard interface for inference APIs and it contains optimized libraries and LLM models for the best and most optimum use of GPUs. Another tool that we are using is MIC. MIC stands for Multi-Instance GPU and it helps us to divide a single GPU hardware to up to seven virtual GPUs. In fact, LLM models that we use run in more than one GPU, but we have other GNI models, smaller models that do not need the memory or compute power of the whole GPU. So we position them on virtual GPUs and we can take advantage of virtualization. One of the challenges that we faced when we are setting up these tools was the fact that we are working in an air-gapped environment. We have a lot of data that we have to do with the internet. And we worked with NVDA consultants to achieve setting them up in the air-gapped environment. I want to continue with the general general ops process, but first I want to start with MLOps. If you want to run AI in an enterprise environment, you have to set up a process and you have to define the standards to run them more effectively. For the machine learning models, MLOps was a solution for it. Four or five years ago, when GNI was not in the picture, we started with MLOps. And we decided to develop our own MLOps platform, which we call Mosaic. And Mosaic was covering all the steps of the MLOps pipeline, which starts with data preparation and also covers model development, model deployment and monitoring phases. It also handles authentication, role-based access control and auditing of all the transactions that are done on the platform. And now we are continuing with GNI ops. We have now extended our MLOps platform with GNI ops capabilities. I want to show the most important capabilities that we have added to our Mosaic platform. First challenge was to create a document data lake. Especially for RAC use cases, we have to form a data lake that should contain all the documents needed for the use cases. We had internal documents in many places like portals, file shares, etc. And we created the pipeline that collect these documents, divide them into chunks and insert them to VectorDB. Another capability was setting up a semantic document search for RAC. As Octai mentioned, we already had a model for RAC. We used it and put a vector database behind that model. And it was used for searching the documents. We also added a GNI model registry. It was different than the register that we use for machine learning models. This GNI model registry contains both the models that we developed or fine-tuned and also models we open source models that we downloaded from Hugging Face or other sources. This new platform also have the capability to create AI agents. Currently it supports single agents, but we are trying to evolve it to a multi-agent platform. And the last component that I want to mention is the GNI gateway. It's one of the most important components of our platform. We have different LLM or other GNI models. We have different models and also we are using some cloud-based models. And we needed a single point of entrance to these models. And we want to solve the problems like authentication, authorization, auditing kind of things in a single component. So we set up a GNI gateway to solve these problems. I mentioned about RAC, Oktay also explained how we do the retrieval of documents. And I want to give an architectural drawing of our RAC pipeline to make it more clear. So we have inference API on our platform, which we create for each use case, each RAC use case. When a prompt comes to these APIs, it orchestrates the flow first. It calls the retrieval API, which has our M running model in it. It searches the vector DB and gets the top three or five results based on the use case and returns it to the inference API. Inference API then augments both prompt with the context and the GNI gateway. And GNI gateway returns the answer to the API and then to the client. We use this GNI gateway for authentication, authorization and audit. And also it has a trotting capabilities. Since our models have limited capability, we can trot the workload on the gateway. And also we implemented guardrails that I will explain in the upcoming slides. And guardrails are called from the GNI gateway. Evaluation of the models are also important. As Opta explained, we selected a LLM model, started with it, but always new models are introduced. So we have to evaluate the performance of new coming models and catch the models that perform better than the model that we are using in production. So we set up an automated evaluation pipeline. We have evaluation data sets for each use case. Hence, when we want to evaluate a new model, we get the model and we have the evaluation set. It goes through the evaluation pipeline, which have judge LLMs and also human evaluation. Judge LLMs, there are more than one judge LLM. They are producing a score over the evaluation data set. And after that, we have a human evaluation. We have a group of people that evaluate the answer of the LLM manually. And we aggregate the results and create a success score for the model. And we have a leaderboard. We publish the performance of each model in that internal leaderboard. As I mentioned previously, we download new models, new open source models from Hugging Face or some other platforms. But before taking it to our data center and registering it to our platform's model registry, we have to scan them to understand that if they contain some malicious content or if they have some vulnerabilities. We use it to be using open source models. Usage with increasing use of open source LLM models. Attacks that are using these models have increased. So, this pipeline was also important to set up. So, first we download the models to a isolated server. We run our scans. We are using open source tools for scanning generic models. and if they are marked as secure, we register to our model registry. Another important step in security is LLM guardrails. Guardrails are used to mitigate risks of using LLM models such as bias, misinformation, security vulnerabilities and other inappropriate content. We set up both input and output guardrails. They are LLM-based guardrails but we are using smaller LLMs. And they are running for input prompt and also for the answers coming from the LLM model. So it guarantees us that coming prompt is appropriate. It doesn't have any appropriate content. Output guardrails check the answer for some misinformation, bias or some hallucinations. And we run these guardrails in every LLM code. And this was briefly the infrastructure platforms and frameworks that we use in our GNI architecture. And I want to talk about the use cases that the most important use cases that we are implemented so far. So we started our LLM journey with virtual assistants as most of the companies also did. First, we used the RAC-based assistant so that our employees, it was an internal assistant. So our employees can ask questions about our internal documents, processes and the regulations of the bank. And instead of searching thousands of documents or asking the question to their colleague, they can get the answer instantly from our virtual assistants. In fact, GNI powered virtual assistants are more than traditional chat bots. We are leveraging them as intelligent digital employees. They are increasing their capabilities every day with added functions. So we are positioning them to assist human employees in accessing information more quickly and enhancing their efficiency. With this assistant, we aim to generate value to the cost savings, enhance efficiency of our employees, improve user experience. And this will cause a sustained business growth. And the second use case that I want to talk about is increasing operational efficiency. We have virtual assistants, their assistants as I mentioned, they are continuously improving. And now we are focused on optimizing and automizing our operational processes by the use of GNI models. We integrated GNI models into banks operational processes. And it helps us deliver significant business value by optimizing the workflows and enhancing decision making in that workflows. In initial use cases, we use LLAMs for document understanding and content generation. And it also helps us to generate business value to increase efficiency, cost reduction and acceleration in processes, which will result in enhancing customer service. And finally, I want to mention about two topics that are in our agenda for future work. One of them, and one of the most important one is setting up an agentic platform. As I mentioned, currently we have a platform that can handle single agents, but with the usage of GNI models for more complex workflows or processes, we need to set up an agentic platform. So we are working on it. We are seeking for an agentic platform. Another topic is having a POC with NVIDIA's Nemo guardreels. I talked about our guardreel pipeline. We set up our own guardreel, but in order to have it more standard, scalable and extensible, we are also planning to have a POC with NVIDIA's Nemo guardreels in the upcoming months. That was all I will talk about. Thank you for listening to us. Thank you for joining us. Thank you for joining us. Thanks. Thank you. Thanks, guys. We'll see you next time. Bye. Bye. Bye. Bye. Bye. Bye. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.