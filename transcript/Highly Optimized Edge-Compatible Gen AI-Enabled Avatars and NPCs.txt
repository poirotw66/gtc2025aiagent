 Hello, we are Sam Frisch, R&D Lead at SoftServe and Igor Romanovich, Senior R&D Engineer at SoftServe. We are going to present our work for three years, even more than three years, and it's about avatars. The formal talk name is going to appear on this slide now. It's about highly optimized, edge-compatible, Gen-A-enabled avatars and NPCs for education and entertainment. NPCs stands for non-playable characters. We have our agenda. So we start with digital human and avatars vision. Then we go to digital humans and avatars market opportunity. Then we talk about NVIDIA AI Enterprise. Then we have Gen AI 3D, Generative AI 3D, 3D vision AI and variations. Going further to use cases about teachers, smart AI toys, NPCs, agents, SDRs, software, self-development representatives, humanoid robot agents and retail trial. Then we share our success stories and case studies, four of them. And we are going to have Q&A session. Let's go to the vision. The vision is pretty straightforward. Sooner or later, they are going to be like us. Digital humans, avatars, agents and so on. And two things to add here. First of all, I believe that they are going to be like us and better. And second, the elephant is created with NVIDIA DeFi to highlight that we can imagine pretty anything already. And it's going to be like us and more robust. On the following, avatar solution market, review of CBN site and more information from the sources. And we have highlights on the right. We have the most promising startups. And in the world, in Convoy AI, we are using the RTH and we are talking to in the world. In terms of numbers, we have already 139 millions of market. And it's going to grow to billions by 2033. We see that year to year growth is 30%. That's very good numbers for growing technology. Could you please go further? The market size for the full part, like avatars, is large. But there are many more opportunities there. We see that education is larger at the moment. Of 4.5 billion. And going to be more than twice to 2032. We see that NPCs surprisingly are twice as big as education. And going to be up to 22 billion in 2032. And for me, it's kind of logical. Because entertainment always competes with education or augment them. And anyways, all together they are huge. Then we come into smart AI toys. And those small toys are yet like half of an education. And growing farther, we are foreseeing 8.5 billion in 2034. Here we come to NVIDIA AI Enterprise. Our journey with those cases has started from NVIDIA's products. We got NVIDIA's audio to face and then audio to gesture. And we were amazed because at the moment it was fantastic. It's still fantastic. And then we had hands-on on the alpha preview of ACE. ACE is for Avatar Cloud Engine. And we had a deep dive inside those technologies. We are in R&D and we are curious and we have to check everything. So we can further more than just take what was provided. We combined those parts. Firstly, audio to face and audio to gesture. Then we find out the way how to get the other smart on the RTX device, but in the web browser or mobile app with RTX and backend. We started with AWS and then checked other hyperscalers. Found out that there are specialized cloud providers and having experiments with two of them. NextGAN and one Legion. It has some pluses naturally because they are like in startup mode and they are aiming to this graphical high loaded system which is closer to our R&D mindset. Then we came under the hood and experimented with more models provided with audio to face with different SR, speech recognition, TTS, text to speech and LLM, large language models. The most challenging, the most challenging was lip sync. And we will see more on the next slide. But meanwhile, you have highlighted the link where you can get much, much more details about NVIDIA Enterprise enabled solution, digital comes here. And also you can even get an Enterprise license there. Here is highlight what is Generative AI 3D and 3D with Generative AI. On the right, we have on this slide, we have Gen AI 3D and 3D with Gen AI on the right. Hi there, my name is Al Nair and I'm a digital human avatar. I'm here to invite you to experience something amazing. It's a 3D model which is supported with Generative AI created voice and had some visuals there with the printer. And on the left, we have more futuristic concepts. The cat is generated with text to 3D. So we get this character like asking, dear Generative AI, could you please create us this fancy cat for quantum experiments? And we have the cat and then we have Rick and animation. And now we are in process of making them talking. So ultimately we are envisioning that the kid can show the image or text about their favorite character and get them talking and interacting together. Here I pass on the microphone to Igor. We have identified a few use cases as the primary use cases with the largest potential impact and market. The first one is teachers. The second one is smart AI toy or smart or AI toys for kids. The third one is the third one is NPCs agents, sale development representatives or SDRs, humanoid robots or agents and retail or try-on or virtual try-on. And actually we have a few demos to show. Hello. Hello. How can I help you today? What can I buy? I'll get that ready for you. You can buy a variety of baked goods and coffee here at the bakery. We have fresh bread, pastries, cakes, and of course, a selection of hot and iced coffees. Is there something specific you're looking for? One cake and one large flat white, please. Great choice. I'll get that ready for you. Would you like a specific type of cake? Or should I surprise you with today's special? Chocolate. Perfect. A chocolate cake and one large flat white coming right up. I'll have them ready for you in just a moment. Oh, thanks. You're very welcome. I'll have it ready for you shortly. So this is the demo of, I would say, sales development representative who is selling you some pastries and some coffee. And for the second demo, we have an NPC like in a video game who will follow your voice commands. I'll have it ready for you. Move to weapon and pick up. Follow me. Follow me. Sure. I'll follow you. Stop. Follow me. All right. I'll stop following you. As for the key studies, I invite Sam to talk about the first one. Sam, please. We have on this fancy green NVIDIA's background all four of our pyramids for the latest years. They are connected by NVIDIA's GPUs. And the first is the name we already talked about this. It's the first ever touchable avatar integrated, attracted with touch. And we are going to talk about those case studies by everyone. But as an announcement, then we came from this touchable avatar to avatars enabled by NVIDIA's stack with ACE. And it came to more like scale and development implementation. Then we came back to the avatars, which are others and humans. And that's why we have elephant. And it was mostly to focus on kids and to do something unusual. And the design cats appeared instead of elephants. We had a project for the demonstration of quantum computing capabilities. And that's a cat of the Schradenberg. And here's the animation of the cat. This is the victory when he is more alive in this quantum experiments. And as Alnair presented himself, he's the Meta-Human. He's actually a divinator. He will tell you who you will be in the metaverse. And actually, it was one of our first demos that we made with the avatars. It was made with cat and edge technologies such as Ultralip. It is an emidair haptic device. And so actually, you can touch this avatar and with a lot of neural networks to perform this divination stuff. This demo, we have gradually improved it through the years. Last year, it won a Globi Silver Award. And also, we have a paper published on how we made it. You can see the link down below on the slide. This is a link to our paper on the ACM library. This is pretty much it about this demo. As for the architecture, which also could be found in paper. And the link, once again, is down below on the slide. We have a kiosk, leap motion camera, Ultralip haptic device, web camera, printer, PC application, neural network server written with Python, face swap, generator of 3D avatars, NFT minting on the Polygon blockchain, lot tables with predictions, and web page with QRs and NFT. So you can go by QR on your card with divination and grab your NFT avatar that was generated by this demo. And as for this demo, I invite Sam to talk about it. It's already more than demo. It's deployed somewhere in the Middle East in the airport. It's digital concierge, which helps you to navigate the airport in that part. And we have several others, which helps to go with finance or healthcare. So you can see the software. The software implementation is built over NVIDIA's avatar cloud engine. It makes it all together simple. He or she can, or it still, or them for AI, can completely represent knowledge for the company, engaging with documents describing. And the solution reduces customer wait times and helps people to focus on more complex creative tasks. And more details on the slide with architecture. The architecture here shows like we have, the data that we have, let's say, from 10 parts and we have generative AI parts with LLMs, with Chat Engine, with NVIDIA Riva. We have here the part with working with documents. So there are augmentations for a document process and to have the info about the customers' needs. It's supported by file storage and vector database. The part on the bottom is my favorite. It's making avatar or NPC, NPC or Engine to Engine. Here's the rendering part. Here the audio interface, which transfers audio to lip sync and facial animation with emotions. And the animation graph supporting all that. As it's targeted for enterprise, we have here the part with analytics. Once again, you can go by the link and see more details about the solution. And NVIDIA's enterprise tag there. As for the next case study, we have a teacher avatar. Once again, by the link below, you can go and see more details about our expertise with avatars. As for the next case study, we have a teacher avatar. Hello, how are you? Hello, I'm doing well, thank you. How about you? I'm doing well, thank you. Of course, let me tell you about a remarkable journey I once had in the mountains of Peru. It was a trip filled with stunning landscapes, fascinating history, and unforgettable moments with the local communities. I took countless photos, each capturing a story of its own. Would you like to hear more about the adventure, or perhaps a specific part of it? I'm doing well, thank you. You're welcome. If you ever want to dive into more stories or have any questions, feel free to ask. I love sharing my adventures. I love sharing my adventures. I love sharing my adventures. In mathematics, the order of operations dictates that you perform multiplication before addition. So, 2 plus 2 asterisk 2 equals 2 plus 4, which is 6. I love sharing my adventures. You're welcome. If you have more questions or need any help, just let me know. As for the architecture for this demo, it's pretty similar to what Sam showed us with the previous architecture. So we have RAC pipeline, including a vector database. We have custom trained large language models and the guardrails, which are protecting the answers and the questions from the model. The input to the model is coming from the Facebook automatic speech recognition model. And the output DTS text-to-speech model is the CoQ one. And also we have over Lipsync, which is optional on the server since we've managed to run it on the Edge devices to offload the server more. As for the user device side, we have an input handler here. So the voice and the text. And when you're receiving the answer from the LLM, it will be processed with Lipsync on the Edge device if it's possible, like on Android, Jetson and Windows device, for example. And as for the iOS, this part would be executed on the server. And then on both devices, everything will be converted to UI, sound, animation and text. And once again, more info on the link can be found on the link below. And Sam, I invite you to talk about these NPCs and fancy cats. Yes, it's my favorite part. NPCs, cats and quantum mechanics all together. NPCs, no playable characters for education is most promising for us. NPCs, no playable characters for us. And we made two demos with those cats. Those cats are quantum cats because the demos we created in collaboration with our quantum computing team. So it's to promote quantum computing, we took taking the sort of experiment of the cat of Shredenberg, which is half dead, half alive. If you are putting him in the box and put some radium, we don't know is it dead or alive until we open the box. Here we have the box for three cats. And we have the walls which shows are the cats entangled or not. And all the cats are created with Generative AI. Firstly, it was Mashi and Tripur. Rig and animation is animate anything inside Mashi or Mixamo. And the latest one and the greatest so far is with Tencent's HunYan2. And we'll show how it's animated. We have the mouse that she is moving. She is having 500,000 polygons when it was just created. But it already can be retopolized. And after retopology, it's like 30,000 plus polygons. And it works on the edge device. In this case, it's MagicLib2. We see that it's already pretty robust for avatars. And we see that in the closer future, they are going to be more and more robust. And we invite kids to educate. And we will make more and more of those interactive avatars, agents, digital humans, NPCs for kids and them for everybody. If you have any questions, you can connect, ask us directly. Me or Igor, Igor or me. By QR code, you can get to more details. At the booth, you can see the demos. Thank you for your attention. Thank you for your attention.