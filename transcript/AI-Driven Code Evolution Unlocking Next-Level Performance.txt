 Hello everyone, my name is Mike. I am one of the co-founders and CTO of TurinTech AI. We are a startup based in London and in this presentation I'll talk about our product, which is called Artemis, which can help you optimize the performance of your code, leveraging generative AI as well as NVIDIA hardware. We are a startup based in London, as I mentioned before, that have been working the last 10 years in the area of automatic code optimization, where we are experts in the area of genetic algorithms, reinforcement learning and other AI techniques to evolve large code bases for better performance. In particular, our team specializes in code transformation and in the last three, four years, we have worked very hard to also incorporate Gen AI and large language models into the decision making. Today, I will talk about one of the biggest problems that we see across the clients that we are working with, which is the technical depth that they are facing around their systems. And the technical depth is one of the biggest problems that are, in our opinion, and we have seen that they are slowing innovation. Technical depth, almost every company has technical depth that is a result of people focusing on code and tight deadlines without having the time to think, OK, I need to fix my existing code. Because there is a trend where everybody is trying to focus on the new cool things, new features in the product, new features that the businesses want, without necessarily realizing that behind that new features, especially if you are doing things fast, a lot of technical depth is behind that needs at some point to be fixed. Performance also is one of the things that we have seen a lot of organizations ignore or they cannot keep up because of the nature of code and hardware evolution, meaning like code that was meant to be very fast 20 years ago. It's not necessarily as fast as it can be nowadays with the latest hardware advantages. As well as we see trends where a lot of orphan code existing in a lot of institutions. And when I mean orphan code, this code that was generated by people that suddenly left a company or left an organization. And that piece of code either is running and nobody wants to touch it because they don't want to take the risk. Or people don't know and they are not aware of that piece of code and that is still running in your application. Last, there are a lot of legacy methods. There is a lot of trends into especially older code that we have seen, where if it works, this is the way we are doing things. This is how things have been working, which can unfortunately limit innovation and slow down new techniques and new approaches to solve and have better code in place. AI generated code is one of the most exciting applications of large language models and generated AI in general. And we have seen a very big uptake into the markets. All the way majority of the developers nowadays are exposed to AI assisted tools. And we see a very, very big uptake. The latest GitHub survey shows that 92% of the professional developers are using coding assistance daily. 25% of Google's internal code bases now originated from AI system. Of course, AI generated code has increased productivity. But there are also some hidden risks. And we have also seen a lot of other use cases where people are very skeptical about AI generated costs. And sometimes those use cases are justified because there is a hidden cost of AI generated technical depth. The more code you are generating and you are putting in production without controlling or optimizing it, the bigger is the chance for you to have even bigger technical depth. So in the future, we will see the AI generated technical depth code bases. Some examples, 38% they have seen increased code churn, like frequently rise and 29% decline in code reuse in AI assisted projects. We have seen use cases where there are hidden vulnerability costs, like because large language models, because of them sometimes hallucinating, they can introduce vulnerabilities in the code or bugs. So that can be very, very costly for organizations. Many times, especially in the very early years of large language models, hallucination was a very, very big problem, which many times can kill productivity. And we have a lot of developers complaining that it takes them more time to check the results of a large language model than them writing them themselves. And of course, the more code added without being checked, it adds a big overhead into maintaining this new AI generated code. So what we, what our vision and our aim is, and what we help clients and users solve with Artemis is, we leverage and we are using generative AI to increase the quality of code, either that comes from developers or AI generated code. So Artemis helps you to go from the code that you currently have with your, with your, with the inefficiencies or the bugs to the code that you really, really need. So Artemis is the first generative AI platform that continues the optimization and the optimization and the results that are generated by AI assisted tools. And it provides you with really good, fully optimized enterprise-grade code base. So how do we do that? So Artemis, first of all, analyzes your code in a secondary step. So it can take either a PR from a user or it can look all your code base and it analyzes. And first it identifies the inefficiencies and the problematic code. Then by identifying the inefficient and problematic code, it tries to fix this code or evolve this code using the power of large language models. Artemis can use one or many large language models to achieve the best results into the code refactoring tasks that we are giving. For example, if you have an algorithm that you would, a machine learning algorithm that you would like to optimize for accuracy and for speed, Artemis will leverage different techniques to achieve that goal of the user. So it will generate variations from the original code. It will assist and will review how good those generations are. And then it will self-learn to evolve it to the goal that the user has for. Once Artemis gives you the different variations or the different suggestions that it thinks they are really good, it also allows you to really execute, compile and run your code and then take the real validation from the system. This is quite automated and it's in a very, very streamlined approach. And it allows the user to intervene anytime they want in the process. Thus allowing both humans and AI to together to interact, to optimize the original code for the objectives that the user has. So what is the secret sauce, right? So Artemis is based on, as I mentioned before, on our cutting edge research in the area of automatic code optimization. The last 10 years, we have published a number of research papers where in the past we were doing code optimization based on manually written rules. Now we have extended that with the power of large language models, with the power of agents and with the power of evolutionary techniques to achieve the goal of the user. So Artemis incorporates all those techniques and all those mini agents, if you think about it, and then it provides it to the user in a very nice unified platform. Of course, internally, we have also quite a lot of good quality data through this process because every time we are generating or we are evolving or we are validating the code, Artemis can learn and knows what worked and what doesn't work. So Artemis incorporates also a set of models, both LLMs and machine learning models to help its decision making. So what you can do with Artemis, you can improve out of the box the runtime performance of your code base. You can tackle technical depth and identify piece of code that is, for example, duplicated across your application. You can optimize AI generated code for performance, for quality, for bugs or any other metrics that are important to your organization. You can modernize legacy systems, meaning like you could look old, for example, C++ code or CUDA code and say, look, I would like now to use more modern implementations of C++ or I would like to convert to C++ 23. So I can use standard libraries. Artemis allows you to do that. And in general, anything that you can measure, Artemis will assist you and help you to optimize. One of the approaches that we have is that we understand, especially the worries that big organizations have around their code, the privacy of their code and the intellectual property that they have around their code. So Artemis can be used fully offline on premise, or you can use our SAS version and it can run. Fully on your hardware. And in particular, we have integrations with NVIDIA names for model deployment. And also you can use the hardware and the libraries that you have access to fully offline in your organization. So the only thing that Artemis needs, it is access to one large language model. That's why we are working very closely with NVIDIA. And we want whenever the users are using Artemis to be able to efficiently get those large language model suggestions, both for latency, but also for throughput. So if you are a company or a financial institution that you have, let's say, 100 developers that want to use Artemis fully offline, you very easily can use that. And our suggestion is that you can deploy your own internal large language model. And then Artemis will just work out of the box. If you have thousands of developers internally, it's again the same scenario for us. Artemis can run either on the Kubernetes or on a Docker deployment that can scale. And then you will need, as an organization, to use and to have internally a large language model API layer, practically. This allows you to have fully enhanced security and privacy because none of the code base will leave your organization. Any experiment you are doing using Artemis, that will give you competitive advantage because you can leverage the data that are generated through this process. So you can even fine tune your own models or you can have your own machine learning models that you can take better decision. And also Artemis is very lightweight. So we have scenarios where, let's say, you want to give prepackaged Artemis for every developer in your organization so they can run fully offline. They can launch Artemis on AI PCs and then Artemis can run fully offline even on a single PC. I am mentioning this because we see a future where large language models are becoming more of a commodity. Organizations will be deploying internally a large language model if they really want to not worry about the cost of using large language models on a pay-as-you-go way. And also this solves a problem that you as an organization wouldn't like necessarily to provide and give your intellectual property and code, especially if that is of high sensitivity to other organizations. At the same time, of course, we understand that cloud is super important and can allow you to scale while, you know, and become more efficient. So there is also a SaaS version of Artemis. So you could use Artemis either through our Artemis.Turintech.ai. So you can just sign up and log in or you can use it on your own cloud. Artemis focuses primarily on performance and on optimizing performance. And one of the critical ways organizations can take advantage of performance is, of course, if they use a really, really good hardware and in particular NVIDIA hardware for, you know, either AI workloads, either, you know, simulations or extreme calculation where they are using CUDA. So we have a lot of clients using Artemis for boosting their code that is written in CUDA to make it faster. With Artemis, you can also convert existing C++ application, identify which places in your code base could be translated to CUDA. Or to use a library that is using CUDA. An example can be, look at my code. Where do you see a mathematical calculation that I can use one of the CUDA collections, for example? Artemis can help you with that as it will scan your code, it will identify that piece of code and then it will try to do its best to translate it and then verify it for your CUDA and hardware that you are using. And, of course, we can further tune your CUDA code without even touching the code by leveraging and optimizing the way you are using the hardware of NVIDIA. How do we do that? You could give to Artemis the same way you are giving code. You can give to Artemis your compilation flags. And it has been shown that large language models actually are very good at understanding compilation flags. They have been trained on, they have seen compilation flags, so they could suggest you and give you better variations. So we have use cases where users have put their code, they have put their compilation make files, which includes compilation flags. And in Artemis, you can evolve by saying, can you please optimize my compilation flags for better performance. Then Artemis can do that as well. Let me show you an example where we will show you an example of how we optimized a Whisper Python code. that Whisper is the OpenAI speech-to-text library. And I will show you next how you can do it yourself through Artemis. And what is the result of such optimization? So in this example, we took the OpenAI Whisper library, we passed it through Artemis, and then we found a 25% faster runtime. So speech-to-text is an important use case that is used almost everywhere nowadays. And in the left side, you have the before optimized open source library. And then on the right is the optimized library from Artemis. Hello everyone, and thank you for tuning in. I'm Suzy Perez-Guy, chief of staff at NASA. This will be much more direct. Today we'll be discussing NASA's role in understanding climate change. I'm very much looking forward to this discussion that we're going to have today. Before we get started, I'm happy to introduce our NASA administrator, Bill Nelson. So if you noticed, on the left side, we had the speech-to-text much slower, so the user couldn't see the subtitles as quick as on the right side. And this is one of the examples that you can do almost automatically to optimize code that may be important whenever you are using Artemis. Next, I'll show you the whole end-to-end process of how we manage to do this optimization. Okay, so this is Artemis. So this is our SaaS version, Artemis.unitec.ai. The first thing we will do, we will import the open source version of the Whisper library, and we will try to execute it into the, to an NVIDIA machine. So here we are cloning, we are choosing the brands that we would like to optimize, and then we create a project. You click on the project. The first thing we do is Artemis needs to understand your code base. So whenever we are giving you suggestions, Artemis uses a different set of techniques, from agents to rugs through embedding. So the first thing you can do is you can generate an embedding. So the embedding process practically will allow us to question, to have a question answering system that understands Artemis, that understands this code base. The first step is, because we care about performance, we have integration with existing profilers. So this is a Python code base. So we will use the Speedoscope profiler. We run the Speedoscope profiler, and then you can upload the output of the profiler into Artix. So we have 352 potential functions, ranked on which each one of them being the most important, the slowest. And then we have uploaded them in Artix. Next, we need to execute and run this code base. If we really want to guarantee that our results are accurate, then we can really optimize the code. So typically, in this example, we will run first, we will compile the code. We will run unit tests to verify that the changes before and after are the same. And then we will run the benchmark that comes with this application. Artemis also, because we have agents inside, it can help you in the process of generating unit tests or generating benchmarks. We see the unit test generation actually as an optimization process. So we practically take your original unit tests or non-unit tests, and then we will generate cleverly unit tests, but optimizing the test coverage in that case. So we see first that with this example, we notice that we managed to compile around the code. The code execution happened in another machine. Artemis can execute the code either on your local machine or on the server. And now we will give you an example why we see that the first Whistler model.pi function inside this code base is the slowest. So we are targeting as a start the slowest function. So here Artemis leverages as a first step Gemini, GPT-4, Claude, or whatever model like LAMA, whatever model you have access to. And then it will provide suggestions from the different models. For every suggestion, Artemis scores how good they are, but also Artemis is able to run or verify every suggestion. In this example, we see that Gemini, GPT-4, and Claude gave us variation, but they didn't manage to pass the unit tests. We see this quite often, like, especially if you don't have context around your code base, large language model may struggle. So to mitigate this problem, we have a built-in Artemis intelligent engine, that we say, which you practically say to Artemis, do everything for me. I don't need to bother to find, to search the context. I don't need to bother to find who is calling my function, what you're calling, and Artemis will practically go over your code base, go for every code snippet, and then it will do the maximum effort to generate variations of that. So, for example, in the first example, we have 11 variations generated, but now those are not generated by just an LLM. Those are generated by our optimization engine. And we can have, you can do this across all your code base. So in this case, we'll do it across all the 10 different, 10 first, for example, code snippets that are the slowest. Next, we want the user not to manually evaluate every suggestion. We want to help the user find the best suggestion. So we want to find the maximum number of suggestions, the maximum performance with the minimum code changes. So in this example, we help the system filter and choose some of the suggestions that we know they are good and let the process run end to end. The user can be involved in the process and it can choose or it can verify any of the suggestions. Then the last step is you run an optimization process, which is practically what suggestions are the best that I should apply in my code base to have the maximum performance improvement. So you let the system run. This, depending on how long your code runs, it can take from a few minutes to a few hours. And then you come back. And in this particular whisper GPU example, you get variations from the original code was running 22 seconds. Then you start getting variations and then you can download the best variation that you see. So in the previous example, we have a variation which is 22% faster with minimum code changes. You can download in this example. Now the original code on the new suggestion on the left is original code and the new suggestion. And you see that the original code run in 8.83 seconds while the new one runs in 7 seconds in this example. Now, if you take a two hour video transcription, this impact may be even bigger. So you can see it. This is one of the, this is one in our mind, simple example of how you can use Artemis end-to-end to optimize this open source, open AI whisper code. We have also optimized other open source libraries. Like we recently optimized QuantLib, one of the most popular financial libraries by a 30% reduction. We are making pull requests in other repositories like HuggingFace and also Whisper. We will make that pull request in the Whisper library. Also in the CUDA collections, we made some pull requests where Artemis was able to identify some, some inefficiencies. We will be in the Nvidia GTC. We are in booth 3001. Feel free also to sign up at Turintech.ai evolve and we will give you access and we'll give you some, some credit so you can play with our SAS version. And yeah, we are very happy. If you have some code base that you would like to try or you want to optimize or find any bugs through Artemis, please talk to us. And we're pretty sure that we will help you with that. So thank you very much. I'm also happy to be able to make time right now for this if you are new topekting today. Thank you very much.