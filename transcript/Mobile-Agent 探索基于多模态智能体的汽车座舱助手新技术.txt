大家好,我是来自阿里巴巴通义实验室的张吉今天很荣幸来分享一下我们在大模型agent领域的一个新的工作Mobile Agent探索基于多模态智能体的汽车座舱驻手新技术传统汽车座舱驻手所使用到的像多轮对话、语音识别等等的技术正在快速的发展但是我们从整体的体验上来看,离用户真正的期待还有一些距离我们先通过社交媒体上的一个视频的集合来看一下用户的真实的感受和反馈从视频中可以看到虽然这个对话很流畅很顺畅但即便是像调整空调温度这样简单的指令座舱的驻手也并不总能够帮助用户很好的完成这些任务反而使得用户的火气越来越大,显得比较尴尬那么在汽车的行驶过程中进行大量的无效的对话也会显得比较危险如果我们能够提供一个新的智能助手能够像人一样坐在副驾在用户开车不方便的时候能够听懂用户的复杂的需求帮助用户观看屏幕在屏幕上的众多的APP当中去做点击输入和一些操作直接帮助用户完成任务这样将会给汽车坐舱驻手带来全新的价值和体验为了实现这样的目标Mobile Agent就应运而生Mobile Agent不仅能够操作调整像空调温度这样的一些基本的汽车的功能还能够对汽车上的广泛的APP的生态进行操作帮助用户完成工作和生活当中的一些复杂的任务我们先来看两个例子第一个例子是工作的场景用户的指令是在微博中搜索GTC2025大会的时间然后在微信的GTC2025参会群中提醒大家当我们在执行这个指令的时候可以看到Mobile Agent它会自动的在屏幕上去点击微博的图标来打开微博然后在微博的搜索框当中输入需要去了解的GTC2025的时间然后在搜索结果当中看到这个时间之后记录下来再回到了主屏幕上去打开了微信搜索进入微信的参会群然后并且在参会群里面通过在下方的输入框当中再输入刚才所记录下来的一些参会的时间和信息再发送到工作群当中帮助用户完成了一个搜索信息并且发送信息的这么一个常见的工作的场景而第二个场景是生活场景用户所需要输入的所需要去执行的指令是在小红书中搜索西湖附近有特色的餐厅然后在高德地图中导航到这家餐厅当我们执行这个指令的时候Mobile Agent就会在主屏幕去点击打开小红书的APP然后输入西湖附近的这个餐厅并且会根据点赞数量筛选出高质量的一些帖子那么从这个帖子当中去查找到本地人推荐的高赞的餐厅然后会退到主屏幕去继续打开高德然后在高德当中的输入框去输入刚才所记录的这个餐厅的名称最后去点击导航到这家餐厅好那这样就能完成一个比较常见的既要搜索一些热门的信息和热门的场所然后要去导航到这个地方的一个比较复杂的一个生活的场景好那这些任务的都需要去操作多个APP来完成那人在开车的过程当中手握着方向盘几乎是不可能完成操作那现在只需要对mobile agent描述出我所需要做的这个事情的需求那mobile agent就能够像附加的代理一样一步一步的帮助用户完成各种复杂的任务和操作也使得开车的过程更加的高效那我也觉得汽车的场景相比手机的场景其实更适合mobile agent的应用因为在汽车的场景下算力能耗都要远远好于手机在汽车的算力环境下能够更快的去处理更复杂的任务同时汽车上的一些隐私等等的问题相对比较小一点后面我们也会提出完善的对于算力和隐私等的问题的一些解决的方案Mobile agent是一种基于大模型的智能体技术而大模型智能体在业界被普遍的认为是人工智能应用的未来OpenAI的CAPACY在他们内部交流的过程当中也提到面对不同的训练训练方法的一些论文他们往往都只会一笑置之觉得也不过如此都是完顺下的但是只要当新的AI Agents论文出现的时候整个团队都会非常认真和兴奋的展开讨论这也说明了AI Agents的技术还有非常大的想象空间然后Bill Gates也是更是对AI Agents给予了极高的评价他认为AI Agents不仅会彻底的改变我们与计算机的交互方式还将会给软件行业带来颠覆性的变革有可能是从建入命令到点击图标以来的最大的计算变革那么这也足以看见AI Agents在人工智能领域的独特的地位和巨大的吸引力智能体它并不是一个现在的全新的技术过去基于强化学习的传统的智能体也已经取得了不少研究的成果比如说在OpenAI的5以及DeepMindAlphaStar上面在一些策略游戏当中已经取得了突破但传统的智能体技术仍然存在不少的技术局限性比如说必须要在专有的环境下进行数据的采样比较低效而且只能面向特定的任务不够通用以及稀疏的奖励和长时间段的问题等等而当前基于大模型的智能体由于具备大模型当中的丰富的世界知识和通用泛化性的推理以及规划的能力能够对外部的工序进行使用并且具备上下文学习的能力这也为智能体的应用带来了新的优势和想象空间而现实世界是丰富多样的智能体在现实世界当中需要去面对多模态的环境交互这也为多模态智能体的发展和应用提供了广阔的空间多模态的智能体需要通过观察周围的环境来自主的行动并达成目标最近一两年多模态的智能体的研究也非常热门延伸出了很多酷炫的应用比如说视频理解视频编辑游戏开发自动驾驶UI操作辅助等等有很多开源的Agence框架可以使用在多模态智能体当中的GUI的智能体以移动设备PC和Web浏览器为主要的交互环境而屏幕的反馈呢其实比真实的物理世界的反馈它的确定性更高范围更小是未来的重要的技术趋势之一可以代替人类操作终端设备来大幅的提升生产的效率近两年的业界也有非常多的工作例如Cloud 3.5可以支持Computer UseApple的Intelligence支持对手机的操作以及OpenAI的Operator对浏览器进行操作等等从这个时间轴上也能看到从2023年开始业界也涌现出大量的GUI智能体开源的工作推动了这个方向的研究和进展Mobile Agent是在2024年1月份发布了第1个版本我们希望通过开源来推动这个领域的发展目前Mobile Agent也在持续快速的迭代中尤其是针对汽车座舱场景我们也进行过深入的优化这是一个在汽车座舱场景上应用的比较好的一个Agent的技术接下来对GUI智能体做一个整体的介绍在GUI智能体的执行过程中我们输入的一般是用户的指令比如说是像这个例子当中搜索惊天胡人队的比赛结果然后在笔记中去写一个战况分析第二个输入是当前的屏幕的截图比如说浏览器当中已经打开的Google的界面以及历史的操作和截图这往往是一个比较长的多个步骤的操作序列进行过大模型的感知和处理之后输出的是对当前页面的观察描述下一步操作的思考比如说在浏览器中输入胡人队今天的比赛等等并使用工具来完成思考的操作这样经过多步的迭代不断的进行任务的拆解执行和思考比如说我们也可以打开美团点一杯咖啡那么可能需要十几步的操作需要GUI的智能体一步步的自主的去执行反思和修正最终的达到操作目标后可以进行停止作为一个需要自主规划自主思考长距离多步骤执行的任务的智能体Mobile Agent在设计的过程当中也遇到不少新的技术挑战首先是UI界面理解由于UI的元素丰富多样不同的系统机型以及APP之间存在显著的差异这也使得智能体准确理解界面变得非常困难比如说不同的品牌和型号的汽车他们所使用的界面的尺寸和比例都不太一样屏幕还有可能会分为深色模式和浅色模式差异也很大不同的车型上的文字的大小图标的形式可能也不一样所以需要模型有比较好的对不同的界面的风格有比较好的泛化的理解能力其次是定位的操作智能体在执行操作的过程当中需要在复杂的界面当中精确的定位并进行精准的操作这对识别准确率的要求就非常高稍有偏差就可能会导致操作的失败然后多步操作规划反思也是非常具有挑战的一步这就要求智能体具有具备多图理解推理的能力能够对一系列的操作进行合理的规划并在执行的过程当中及时的反思和调整确保任务顺利完成最后是操作实验的问题在长距离多步骤任务的执行过程中对单步的操作实验要求就非常高因为任何一步的延迟都有可能影响整体的效率和用户体验那么这些挑战的都是我们在推进相关技术发展过程当中需要去重点重点功课的难题那Mobile Agent能够通过一句话的指令来实现操作设备首先Mobile Agent是纯视觉的方案完全通过对屏幕的理解来进行操作这样可以操作任意的App不会受到App是否提供API的接口的限制对App界面的理解也会有比较泛化的理解能力也不会因为App界面的变化而导致这个理解就发生了改变而在这个方案的方案也不会因为App界面的变化而导致这个理解就发生了改变而在这个方案的方案的方案也不会因为App界面的变化也不会因为App界面的变化而导致这个理解就发生了改变对App界面的理解对App界面的理解也会有比较泛化的理解能力也不会因为App界面的变化而导致这个理解就发生了改变所以通过感知规划反思三个Agent的结合也能够实现较高的端到端的准确率并且Mobile Agent是无需训练的在任意的设备上都可以做到既插既用是一种具有强泛化能力的GUI智能体Mobile Agent的特点是多端多App和多场景的支持举例来说可以支持手机车机和PC等不同端的设备也可以支持像高德美团淘宝等不同的App以及导航外卖购物等热门的场景Mobile Agent是一个多智能体的方案底层会使用到视觉理解大模型和文本的决策大模型的相结合的方式Mobile Agent从第1个版本发布之后经历了多个版本的快速迭代从一个学术创新研究的成果发展成为了可以大规模接入使用的产品我们最早的V1版本从单Agent开始使用的是GPT4O端道端完成率达到75%当时的文章被iClear2024收录V2的版本从单Agent开始演进到了Dore Agent我们提出了规划、决策、反思三者的协同端道端完成率进一步提升到了80%当时的文章被Nips2024收录但由于需要多个Agent协作单部的RT也有所提升从30秒提升到了上升到了60秒V3的版本从使用GPT4O转向使用我们的字眼模型千万VR的多模态模型使用的效率也大大提升虽然端道端完成率略微降到75%但是单部的RT从60秒降到了10秒在CCR2024的会议上面获得了Best Demo并且在云期大会上发布了Preview的版本我们在近期上线了V3正式版的产品在V3正式版当中我们实现了端云协同知识增强的方案相比于前一个版本平均的端道端完成率从75提升到了90单部的RT也从10秒进一步降低到了2.5秒真正实现了从Demo到可以应用的产品的跨越接下来我逐个的介绍一下我们在各个版本当中的一个眼镜的思路最早的V1版本我们依赖于GPT4O但是当时的多模态的大模型普遍缺乏输出精确坐标的grounding能力所以我们在一定程度上在GPT4O的基础上去解决了需要的grounding的能力我们通过OCR工具来检测和识别文本框来完成屏幕文本的定位以及我们用图标分割工具来检测图标和位置来完成图标定位并且我们定义了一系列用于设备操作的行为空间使得Mobile Agent能够初步的具备了点击文本点击图标打字上滑下滑返回上一页和返回桌面等等的屏幕操作能力这是我们在V1的版本当中的一些工作但我们发现要对这个任务进行更完整的理解和更好的规划往往也需要让模型能够理解过往的多部的操作历史比如说在这个例子当中可能过去有7个操作的截屏通过7个操作截屏的理解来规划下一步的操作但由于当前的动模胎大模型的一个是窗口长度的限制以及图文混合的指令一般来讲现在遵循能力都还不太足够所以现在要一次性理解冗长的并且图文交错格式的操作历史会大大的增加智能体追踪任务进度的难度所以我们在V2版本的设计当中我们开始设计多智能体的框架提出了PlanningDecision和Reflection三个智能体通过多智能体协作的方式来降低处理图文长序列历史的复杂度我们在评测任务上可以看到多Agent的准确率可以达到86.5也远远超过了单Agent的50.7也进一步验证了我们想法的有效性在V2的设计中我们把任务划分成了规划决策和反思三个阶段在不同阶段使用不同的Agent我们通过一个示例比如说查看今天的天气去记事本写下穿衣建议这个指令来展示一下三个Agent的协作过程第一步是在规划的阶段我们通过Planning Agent来分析用户指令和上一步的操作和已经完成的任务来规划出下一步要做的比如说上一步是停留在桌面那么桌面上有天气的App那么下一步就是要去打开这个App在决策的阶段通过Decision Agent来根据当前屏幕的状态进行思考形成具体的Action指令来执行比如当前的状态是已经打开了天气App那么就需要去执行记录天气和返回首页两个操作而在反思的阶段通过Reflection AgentReflection Agent去对比操作前后的两个不同的屏幕截图来判断操作是否成功如果出现错误的操作将会返回上一步重视如果没有发生错误那么就继续执行比如说前一个截图是天气App下一个截图是Home页那就说明返回Home界面的操作就执行成功了可以进行下一步这三个体之间各自其值有效的追踪任务的进度记录相关信息并进行反思操作大大的提升了Mobile Agent V2完成任务的效率和准确性为了评价Mobile Agent的效果我们设计了四个指标的动态评测成功率完成率决策准确性和反思准确性开展在五个系统内置的应用和五个第三方的应用每个App和多个App各两条基础指令和两条进阶指令上的一个完整的评测经过评测也可以看到多智能体的V2的版本会相比之前取得更好的效果尤其是在V2的版本上面加上知识注入效果将会得到进一步的提升这也验证了在Mobile Agent技术上去使用外部知识的一个有效性在V3版本中我们将Mobile Agent转化为可以在智能座舱中落地的产品应用我们和斑马英伟达联合发布了Mobile Agent V3产品其中以斑马作为汽车OS以英伟达作为端测的算力平台在真实的智能座舱应用场景中能够取得稳定的效果V3是一个端云协同的方案通过斑马的原神AI来进行端测和语音测的协调和调度在端测借助了英伟达DrivenAgex平台把对屏幕的感知和反思模块放在端测用户的屏幕图像仅在本地处理这样的设计的好处使用户的屏幕图像截图无需传到云端保护了用户的隐私同时我们把规划和决策的Agent放在了云测这些推理任务相对会比较复杂这样可以充分的利用到云上的资源来加快复杂问题的推理过程提升推理效率所以我们在V3版本中我们通过多尺寸多模态多智能体协同和断云模型架构的方式来提供座舱屏幕的感知复杂任务的规划以及座舱UI的操作和决策能力大大扩展了座舱智能助理的能力边界Mobile Agent V3在端侧部署在英伟达的DRIVE AGX平台在端侧部署的开发过程中我们和英伟达团队紧密合作做了一系列的工作来提升部署性能包括成功适配TRT-LM至英伟达的DRIVE AGX平台利用FMHA内核显著加速推理过程以及将所有的小模型优化至Tensor RT框架在端侧部署了OCRGranding Dino千万VR7B模型来进行屏幕的感知从性能数据和结果来看使用TRT-LM框架部署VR模型后7B模型的Decode速度提升到了1秒中40个TOKENS并且我们充分发挥了GPU的并行推理能力实现了30张图像的并行推理大幅缩短的推理时间在真实的场景中落地往往对效果的稳定性和可干预性有比较高的要求我们在V3版本中增强了知识增强体系有支持两种方式第一种是操作逻辑增强Tips第二种是操作步骤增强Shortcuts通过Tips可以根据用户的query来检索知识库当中的一些操作逻辑的知识保障复杂操作的完成率例如购物APP当中的搜索框的通常都在顶部而外卖APP当中需要通过点击加号来就来加入购物车等等那这些知识呢可以通过操作的Tips来输入通过操作步骤的增强呢可以将可以根据当前见面的识别来操作步骤知识比如要执行一次搜索需要点击搜索框输入文字并点搜索按钮那这一系列的确定性的连续动作我们可以通过Shortcuts的设置来干预直接执行跳过模型的感知和判断那通过这些方法呢可以显著的降低单步实验对Backcase有直接的干预的能力在真实的应用中呢表现的更加的稳定和快速我们对Mobile Agent也在众多的App和场景当中做了就是完整的测试目前呢在大部分比较常用的App和场景比如去哪儿火车票机票酒店的一些场景淘宝的搜商品茶物流等场景微博的搜资讯看热搜场景美团的点外卖买药以及高德的导航搜周边的场景上面现在的评测呢都能够取得比较高的端到端的准确率可以达到实际生产使用的要求当然Mobile Agent是一个泛化性的智能体应用并不限制具体的App我们正在进行更广泛的App的测试但我们也仍然看到一些比较有难度和有挑战的一些操作比如说在外卖买药的场景中呢用户可以同时可能会同时买感冒药和口罩等等多个商品以及不同规格的选项那么这样呢就在指令的理解上会比较有挑战在导航场景下呢可能会涉及到一些形成的推理比如说需要选择最快的七行路线需要在屏幕理解在理解屏幕的这个情况下面呢能够直接给出推理的结果那我们也在进一步的提升多摩台Agent的推理能力目前呢我们已经发布了Mobile Agent v3的API欢迎大家合作和使用这是当前Mobile Agent的一些场景的展示比如现在可以在美团当中的去点餐并送到指定的地点在去哪当中去查找指定时间的火车票并且去订票在淘宝中的搜索商品并设置商品的这个搜索区间微博当中呢我们可以去搜索一些热点事件等等Mobile Agent现在可以比较顺畅快速和稳定的完成这一类的问题Mobile Agent在汽车座舱驻手落地的过程中我们也看到了一些新的挑战近期呢我们发布了Mobile Agent e的工作对新的几个有价值的问题呢开展的研究涉及了新的分层智能体架构比如我们发现shortcuts和tips其实非常有用合理的增加知识的配置呢可以在原有的基础上实现20%到30%的提升是执行任务的稳定性的保障但是shortcuts和tips的配置呢也需要大量的依赖人工无法快速提升知识的覆盖度所以需要探索自主发现知识的技术那为了解决这个问题呢Mobile Agent e 首次在移动场景引入了自我进化模块赋予他类似人类操作设备的时候呢可以逐渐变得更加的熟练的这个能力我们把tips和shortcuts存储在一个长期记忆的模块并随着不同任务的完成不断更新每完成一个任务呢就通过两个经验反思者来根据当前任务的操作记录和错误日志的信息对tips和shortcuts进行自动的优化更新按这样的方式呢在大量的任务的执行之后模型会自动总结出对完成任务有帮助的tips和shortcuts实现自主的进化而另一个挑战呢使用户需要执行的任务也越来越复杂经常会跨越多个app比如在购物的场景呢需要在不同的多个不同的购物app上对比同一个商品的价格并找到最便宜的商品那为了解决这个现实场景中多步骤多app的复杂问题呢Mobile Agent E构建了一个多层级多智能体协同框架核心的想法呢是显示的将高层的任务规划和低层的动作执行区别开来那这种多层级的架构呢有效的提升了模型长远规划和错误恢复的能力比如在三个不同的购物平台上可以分别的去搜索用户想要购买的商品记录对比并最终找到最便宜的选项那具体来说呢Mobile Agent E有一个上级经理和四个下级助手来组成上级的经理呢负责高层次任务的拆解指导规划那四个助手呢分别是一个是感知者用于负责检测截图中细利度的信息一个是操作员用于根据高层计划来决定下一步的动作一个是动作反思者用于对比任务执行前后的截图来反思任务是否成功以及还有一种呢是记录员用于记录在多个任务步骤之间进行记录和汇总重要的信息那这样的多个不同层次的Agent相互协作完成复杂任务也欢迎大家关注Mobile Agent E的论文来了解更多的细节Mobile Agent经过几个版本的迭代目前已经能够稳定的操作大部分的汽车座舱中的APP的能力那目前呢我们也已经和一些汽车厂商在开展合作相关的座舱驻手的能力呢也即将正式的上车那未来呢我们将会在三个方向做持续的探索首先是多模态GUI-OE的推理解决更复杂的任务拆解和在座舱中的一些推理的场景第二个呢是探索个性化的交互来满足用户个性化操作的需求提升操作的效率那第三呢是知识自主进化通过在线强化学习来提升模型的自主的反思能力最后呢是我们的一个愿景我们希望未来通过Mobile Agent能够实现100次的车内交互Touch不超过一次好谢谢大家希望和大家有更多的交流和合作还有关于旨 Equity pilgrim