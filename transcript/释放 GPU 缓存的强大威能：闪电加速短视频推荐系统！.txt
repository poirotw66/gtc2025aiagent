大家好,欢迎来到GTC我们今天要讨论的话题是通过GPU Embedding开始技术来加速短视频推荐系统今天共有两位演讲者一位来自快手推荐系统的工程师李新华我是来自英伟达的解决方案架构师Eric Chen下面让我们进入正题随着近些年短视频平台的迅猛发展我们每天都会接触到海量的视频内容无论是娱乐、学习还是社交短视频已经成为了我们获取信息的重要方式面对着如此庞大的内容库那如何为用户推荐最感兴趣的视频也就成为了平台的核心挑战之一这也就是推荐系统的价值所在推荐系统通过分析用户的行为数据比如观看历史、点赞、评论等等利用复杂的算法模型精准地预测用户的兴趣从而为用户提供个性化的内容推荐随着用户数量的增长和数据规模的扩大传统的推荐算法在计算效率和实时性上面临着巨大的压力那这个时候GPU加速技术就成为了解决问题的关键无论是深度学习模型的训练还是实时推荐的计算GPU都能够在短时间内处理海量数据来确保用户能够及时获取到高质量的推荐内容在快手的推荐系统当中照回和排序阶段已经应用了大量的GPU来进行加速GPU凭借着其强大的并行计算能力已经成为了推荐系统训练和推理的核心硬件尤其是新一代GPU架构如IDA架构的引入其计算密度相对于前代安培架构有了显著的提升为推荐系统的性能优化带来了新的可能性同时这也带来了新的问题GPU算力的快速增长与CPU算力的相对滞后导致了算力增长的不平衡在实际的使用当中推荐系统不仅依赖于GPU的高效计算还需要CPU进行数据预处理任务调度和系统协调当GPU的计算能力大幅度提升时CPU往往会成为系统的瓶颈无法及时的为GPU提供足够的数据支持导致GPU的算力无法被充分的利用这种现象在高密度计算任务当中尤为明显例如实时的推理场景为了解决这一问题我们需要从多个角度入手其中一个可能的解决方案是通过减少对CPU的依赖让GPU的计算能力得到更充分的释放在快手推荐系统的在线推理架构当中模型计算部分已经使用了GPU来进行加速在CPU的负载方面Mbedding查询阶段占比达到了80%我们如果能够将这个阶段从CPU卸载到GPU上那么问题就能够迎刃而解在Mbedding查询阶段主要包括Mbedding的远程获取和本地查询以及本地存储同时得益于AIDA架构GPU更强大的算力和更大的显存让上述的方案具备了可以实施的基本条件Mbedding查询操作具有两个显著的特点高度并行性和内存密集性首先每一个MbeddingID的查询是独立的这使得它非常适合GPU的并行架构尤其是在批量查询的时候GPU可以充分发挥它的优势其次Mbedding查询操作主要是内存密集性的任务计算量相对较小而GPU的内存带宽又远高于CPU和远程网络因此GPU在这一场景下具有天然的优势一般由于Mbedding表通常非常庞大无法完全加载到GPU内存当中但幸运的是数据的访问往往具有时间的局部性遵循密率分布也就是超过90%的内存访问集中在了不到10%的热的Mbedding上基于这样的一种特性我们可以采用一种分层存储的策略将高频访问的热的Mbedding保留在GPU内存中而将低频访问的冷的Mbedding存储在CPU内存或者是参数服务器当中通过这种方式我们不仅可以减少GPU内存的压力还能够显著地提升Mbedding查询整体的性能下面我们将具体介绍如何设计高效的GPUMbedding cache以及我们实验的结果Hello 大家好我是来自快手的李新华下面由我来给大家介绍一下GPUMbedding cache的设计方案首先我们说几个定义一个就是featurefeature就是用于描述用户或者item它们的属性和特征feature经过编码之后形成数字slot一种抽取规则一个slot它可能会抽取出一个或者多个sign来我们可以举个例子我们有一种抽取规则获取用户他喜欢的电影的类型那么feature呢就是有个用户喜欢喜剧片或者动作片sign就是喜剧片或动作片经过编码之后形成数字embedding就是sign的线量化表示paint memory这里我们问了情况下是指的是CPU的paint memory我们最终要用于run graph时会有一个按照特定形式组织好的数据用于run graph这个paint memory这个paint memory呢就是用于组织这部分数据用了dtypetensorflow的dtype例如fp16ip32dim的话就是dimension的一个缩写es就是emending server的一个缩写这里的话es主要就是用于存储emending的一个服务器case name一个es的一个地址我们可以理解成一个运名吧groupdtypedem和es的一个组合后面的时候我们会介绍一下好我们先说一下传统的CPU的emending开始的设计有因为我们在处理的时候可能首先是会把一些数据进行准备好比说我们会用用获取用户的画像文章的画像这些准备工作然后把这些东西按照一定的规则抽取出来抽取成特征就是好比一个slot抽取出一些sign来这就是形成了一些特征然后呢我们会把这些数据这些特征对应的emending给找出来排在pain memory上pain memory我们刚才说过了就是按照特定的形式组织的数据我们把这部分数据传到dpu上gpu上传到gpu上之后用云乱graph那么这部分数据那么这部分数据slot抽取出来的sign它怎么查找它对应的emending呢我们一个sign首先会去读取开始读取开始了之后如果是说miss呢它会去fetchemending server把fetch回来的东西更新到开始里面返回这个emending如果是说开始里面直接命中了它就直接把这个emending返回这就是sign获取emending的方式因为我们的抽取规则不一样抽取规则它使用的demission也不一样所以呢不同的抽取规则它就有不同的配置我们这地方就产生了好多种配置不同的配置那么我们想要处理这些东西的时候这些东西数据是跟业务比较相关的一些slot它可能又会有不同的一些sign它会对应不同的emending server有不同的dtype也会有不同的demission这样的话我们对于GPU来说跟业务相关的东西是不太适合GPU去处理的那么我们想要做的一件事情就是说把这做成一个统一的一个结构GPU可能就是处理起来会比较简单一些就是把这一部分去业务化刚才我们说到一个group的概念我们把dem dtype case name相同的数据放在一起形成一个group之前的时候slot它数据是随便排放的现在呢我们把它有一种排序规则把相同的这些数据相同我们这叫group相同group的放在一起因此呢我们就把一些sign给摆放到这些地方那么做了这一步之后我们的数据就跟业务无关了可能不同的group有不同的处理规则但是我们首先去去除了slot的这一层概念跟业务脱离了我们现在呢数据也是这里有一片sign然后我们对应的paint memory只要是说把这部分sign找出来的数据直接的按顺序填充到paint memory上这样我们就能够进行后面的一些工作了我们就说这一步是把去业务化了好我们之前的处理流程呢是一个request来了之后我们会收集这些sign收集完了之后我们会把这些sign进行去重去重完了之后去查找相对应的emending如果是说miss呢我们还会去远端获取emending最后的时候我们把它排放在paint memory上这是我们一般的处理过程但是这一般的处理过程呢它会比较的跟业务比较挂钩会比较的就是说处理的时候比较简单但是呢它有一些浪费好比说我们一个request来了之后现在的处理是说首先我们还是会收集sign收集sign完了之后我们会做一些batch为什么会做batch因为我们对于gpu来说不可能一个request一个request去处理这样的话会比较浪费gpu我们会把好几个request的放在一起一起去软graph一起软graph我们request来了之后其实不同的request它产生的sign也会有重复好比说request1产生了100个signrequest2产生100个sign其实request1和request2去重之后呢可能只会有180个sign它可能有20个sign是重复的这样我们一起把它放在一起batch有这么一个这是其中一个好处再有一个好处呢我们对于把它放在一起这样的话形成的数据就比较大对于gpu来说也会比较好我们少了每一个request都去掉gpu减少了launch的次数我们这个地方首先改造呢就是说把request这一步收集完了散做batch不是说一个request一个request比较简单的去处理了做batch呢我们做完batch之后我们会进行一步就说去重去重之后这里还是会查询一般的server不是查询一般的server就是查询开始这里是gpu开始了查询完了之后我们还会做一些其他的操作好比说因为我们的es是对应着好多个shut的一台es上面存储不下所有的一般零我们会把这个分成好几个shut分成好几个shut呢我们让gpu来做这份feshut的工作这样的话就进一步的减少了cpu要做的事情我们做了这一些操作呢就是说把cpu的工作挪到gpu上来做让cpu要做的事情更加轻量级这里后面会举个例子然后呢我们把收集好我们把cpu这里去重后的s我们经经过查找查找完了之后我们会把这些miss的s再给cpu让cpu去收集起来然后呢让cpu去获取这些miss的s的eventing最后的话把这些eventing给返回给cpu让cpu做进一步的其他的处理cpu拿到miss的s它会把这些miss的s更新到cpu开始里面并且也会恢复出那整个的paint memory这是一个过程好我们细一步的会说后面我们刚才说到我们会有会把一些那个相同属性的放在一个group里面我们把这一个group我们假设我们就叫一个table这个table的处理呢我们怎么着处理呢拿过来之后我们首先会进行unicunic完了之后我们会去查询查询我们查询完了之后这里会多一步salt的工作主要就是为了刚才说的把不同shad的数据把相同shad的数据放在一起让cpu去处理的时候更加简单一些我们举个例子我们首先收集收集s收集好了sand之后我们要进行unic这是收集好的sand收集好的sand我们进行unic这里面有些重复好比说1212我们去重完了之后形成这么一个去重后的结果去重后的结果我们还会形成一个post data这post data的话好比说这个12这个sand在去重后的里面的位置这是第0号位置13在里面第1号位置22在里面第3号位置就是说这么一个跟原来的sand相同大小的一个post data的速度用于最后的恢复工作我们unic完了之后会进行一个query操作这里假设22这个sand是在gpu开始里面命中了这里去query完了之后22号命中了我们就会产生这几个sand最后的时候我们会再有一个post data2这post data2会记录着这些unicsand这个miss的sand在unic里面是哪个位置好比说这个12在里面的02位置13是1号位置88是2号位置9至第4号位置这也是为了后面的时候恢复使用我们查询完了之后我们会进行一个排序这个地方的话就是主要是给cpu减少工作量来做的好比说举个例子这里是假设cpu有两个假设emended server有两个sand两个sand我们按照数字的膜来取兰飞备着sand但那还可以有其他的方式那么12和88就会分到就会在一个sand里面13和9就会在一个sand里面这样的话我们最后排完了之后就是88和1213和988呢这post data2它会改写848在第二个位置就这么一个概念12在0号位置13是在1号90在4号这么一个处理过程处理过程这个处理完了之后我们形成的这个sand by bucket size就是miss的sand并且这个miss的sand它已经是按照sand排好序了cpu拿出来可以直接的发到invent server上去获取结果刚才我们说到这个miss的sand呢它是cpu要把它收集起来发到远端它也有一个collect的过程因为我们刚才说到group划分的时候有几部分一个就是dtypedmission和一个imvent server因此不同的group里面我们也会有相同imvent server的我们为了让这些发送的次数就是说这些数据聚合在一起一起发送我们做了这么一个操作就说好比说这两个ingroupa table和b table他们两个是一个imvent server的我们会把它进行一次聚合s0和s1s0是shads0是shad呢把a的和b的都拿过来发往shad0的我们都聚在一起这样一起发出去如果是说这一次request没有各数限制的时候但是往往我们是怎么着我们会有一些限制一个request的它不会发送特别多的s因为发送特别多的它特别大的话会影响速度因此呢这里我们会有一个划分好比说500个s是一个一次限制我们会把500个化成一组这里的时候就会体现出分shad的一个好处来就是说我们会把举个例子这里a有对于shad0来说a抽取出来了一下250个sb有850个我们首先会从a里面取500个然后发送出去这是构成一个request的还能取500个取完之后呢它剩下250个b呢它有850个我们去取的时候直接能取出500个来就直接发送出去b还剩下250个不还剩下350个我们首先拿出250个来跟a会在一起发出去剩下100个呢它在最后的时候再单独发一次这样的话就完成了cpu向eventing server发送数据的过程这里的话cpu要做的事情就比较的轻量级了那么我们把这个数据发出去cpu发出去之后它当然了用同样的方式进行接收因为它最后的时候同一个shad里面的数据是连续的因此从eventing server返回回来的数据呢它也是填充到一块连续的memory里面去这样的话就比较的快再有一步我们第三步我们把收集回来的数据eventing server我们把eventing server返回来的数据就是查询完的miss的sine的对应的100同步到gpu上同步到gpu上之后我们怎么着恢复出那个paint memory来呢我们刚才说到我们有一个posed data2posed data2我们去查找就是miss的miss的eventingmiss的sine对应的eventing我们会去拿着这个eventing填充到指定的位置就能够把这个unic sineventing恢复出来有了unic sineventing呢我们就会再用同样的方式把这个gpu paint memory给恢复出来就这么一个过程就是一个逆向的一个过程好我们这里说完了主要的过程我们来看一看整体的架构上的改造是哪块呢我们之前的时候一个request来了会直接的自己独立的去处理这都是在cpu上进行的现在呢我们会把request多个来了之后首先会做一个batchbatch完了之后我们dance的部分还是直接填充然后spass的部分我们会集中的去处理刚才也说了我们这里用batch完的去跟gpubatch完的去进行gpu cache的query这个操作的话会把多个request在一起去处理减少了launch的次数这一部分就是刚才cpu那一部分这一部分就是恢复的那种过程这就是说之前和之后一个改变的过程我们还做了一些其他的处理好比说因为刚才刚才我们也说到我们会尽量的减少gpulaunch次数后面的时候我们后来还增加了cuda graph的一个处理还是为了减少launch次数再有一个呢我们对于清理数据还做了一些处理好比说之前的时候query是query或者说update它会支持LU的处理但是呢实际的工程里面它更多的使用的是说FI FO或者说定期的超时好比说这个开始存储的数据我会在五分钟之内全部叫五分钟之前的全部不要了全部之为过期再有来的话你再去查询Eventive Server会有这么一些机制会增加了几种数据过期的处理方案再有一个会做一些table fusion的工作好比说我们有些table这个table上面Dimition是16维的我们只有一百条数据另外一个table呢它下一个table是32维的它有一万条数据其实就16维的这个数据它对应的3比较少这时候呢我们会把16维的数据也给扩充到32维叫它到32维里面去查对象说后面浪费了一点浪费不多因为它毕竟是一个比例比较小吧但是我们做以外定开始的时候那个开始大小32维大小我们就可以设置比较大16维的相当于一个填缝就不用说单单的在设一个16维的呃再有一个我们会我们刚才说到呃说到过程的话可能一些很细的东西没有体现出来就是说我们还会做一些以外定以外定开始的更新的时候会做一些操作我们更新并不是说每一次来了之后查询完了之后都需要更新然后开始我们会有一个更新的一个现成如果是说他跟他请求比较多比较快我们会做一些呃控制他的呃次数的这么一个操作就有一个单独的现成去更新以外定开始呃后面的话我们还会考虑就是说一个就是呃在quary和update的时候现在是有锁的有锁操作的我们在想后面的时候怎么把这个锁给呃去除掉或者说换另外一种方式去替换一下让GPU里面少一些锁操作呃再有一个我们现在有一个比较困难的地方啊就是说我们在去处理的时候因为我们不知道不能够预先知道这个散他有多少个他对于里面有多少个呃对不散有多少个因此呢我们给他分配空间的时候我们就不能够预先的通过程序跑起来之后再给他分配呃这个不能够提前分配好因此呢我们就是说首先我们会把程序跑一下然后得出数据的分布空分布情况来然后会给以外的开始的大小设置一个值这个地方是比较麻烦的后面的时候我们在想怎么着能够动态的去分配这一部分呃现在我们测试的时候因为呃这一部分是在整个的服务里面是占有比较多的CPU的因此呢我们现在CPU降低的话会降低到原来的呃四分之一到二分之一之间呃对于KPU平子来提升的话如果是说不是GPU BUND呃一般能够提升呃就因为不同的服务是不一样的我们测试的是1.6倍的提升量吧呃这就是说我们主要是为了解决CPU嗯CPU瓶颈的问题然后呢我们会我们做了一些呃做了一些操作吧把一些CPU的工作然后把挪到GPU上去让GPU来协助呃加速当然呃在做的过程中的话把一些工作挪到GPU上去其实它的呃latacy也会降低好这就是我今天要讲的内容谢谢大家谢谢大家谢谢大家