 we have a Thank you. Thank you. Let's take a look at today's agenda. So we'd like to start with the motivation vision. Why are we developing fully automated trains? And why do we need a fully automated simulation pipeline for this purpose? Afterwards, Jose will guide you through the technical details of the simulation pipeline. We will give you insights with regards to our learnings and what we changed during the development of our pipeline. This means that we also approach the automatic scene generation in a fully automated manner. Finally, we'd like to conclude with our learnings and also provide an outlook with regards to our next steps. Let's take a look at the motivation vision. Our department is focusing on the highest grade of automation, which is Goa 4. Our simulation pipeline is targeting this highest level of automation. However, our simulation pipeline is also highly beneficial for lower grades of automation. This means Goa 3 and Goa 2. What does Goa 4 mean in detail? It means that every step of the train automation is actually automated. This starts with the train preparation. It goes on with the train motion and especially the perception incident management. This means, for example, perceiving an object on the tracks and reacting to it in the best possible way is fully automated. What are the benefits of railway automation? So on the one hand, we see a significantly increased demand in railway traffic. On the other hand, we see a shortage of skilled labor due to the demographic change. This is something we'd like to compensate for. Furthermore, we have many more benefits, like, for example, the energy savings due to the significantly more efficient way of driving that we can achieve. Additionally, the operation of the vehicles is just limited by the maintenance cycles. So all in all, we're achieving many benefits due to automating our railway operation. You might ask yourself, what are the challenges of railway automation? And maybe you heard about the automated metro in Copenhagen, Denmark, which is a closed system. This means the access to the track area is restricted by, for example, fences. On the contrary, we are dealing with an open system. This means, of course, we have protective measures, like, for example, level crossings, where we have barriers. But in general, our track areas are not fenced. This means there is a lot more uncertainty and a lot more incidents that we have to react. As we've seen on the slide before, we are dealing with an open and unconstrained system. And we know that for the development and testing of automated driving functions, we need huge amounts of data. Fortunately, in practice, these non-regular situations, like the one depicted on the right-hand side, are very rare. On the other hand, we need this data for developing these functions. So our idea is to heavily rely on simulation to generate this data that we need for the specification, development, and testing of automated railway functions. At our first lighthouse project, Sensors for Rail, which we presented at the ITS World Congress 2021, we equipped the train with manifold sensors, 14 in total. So different types of LiDARs, different cameras, different radars. And this project can be regarded as the foundation for our current work with regard to train automation. In 2023, we started the successor project, Automated Train, which is building upon the foundations of the Sensors for Rail project. At this time, we have two different trains, the BR430 from Alstom and the Mereo train from Siemens. You see some similarities with regard to the sensor setup, but you also see some deviations. And in parallel to the vehicle engineering, we started to building the simulation toolchain to support the project. This means that we automated the toolchain, that we brought the test areas to the Digital Twin, that we resampled the sensor setup within the Digital Twin, so that we have a solid, I mean, foundation for supporting the project by simulated data. A common assumption from Automotive is that around 70% of the test data used for developing automated driving functions is simulated. 20% is then acquired usually on testing grounds and 10% in real-world traffic. At Railway, we have seen that the probability of non-regular situations is significantly less. Therefore, our assumption is that 80% of the test data needs to be simulated, 15% acquired on testing grounds, and 5% in real-world traffic. Let's take a random perspective. So here you are seeing the results that we presented at the Digital Summit 2023 in Jena, where we brought the sensors for rail train that we have seen before with the sensor set, and also the surroundings of Hamburg, of the S21 line to the Digital Twin, where we simulated different real-world, non-regular situations that actually happened. Let's take a look at the benefits of simulation that we want to achieve in contrast to real-world testing. So on the one hand, we want to be more economical. And if you take a look at the right-hand side, where we have a flooding of the track area, this is something that is prohibitive in reality, even on the testing bed. It would cost millions. You would ruin the infrastructure. So this is something really beneficial that you can do in simulation. Furthermore, we want to be unsurpassably safe. And we have the requirement that we need to put objects and even persons on the tracks, and we would endanger people by this. So this is a huge benefit of the simulation. Furthermore, if you did testing on the testing ground, that you know that it requires a lot of time. You need to prepare the scenarios. You need to record them, sometimes multiple times. So this is also a huge benefit of the simulation. Furthermore, we have a requirement that is imposed by the Sendelec. It requires that every step is traceable. So if you have a testing scenario where you are recording it, you are afterwards using it for tests, then you need to trace this path back. This is also something that we incorporate in our simulation architecture by definition. Furthermore, if you have an extensive ODD, and we have an extensive ODD, then you will have to cover thousands to millions of scenarios. And this is where scalability becomes really crucial. Now we would like to guide you through our novel architecture that we developed to achieve the aforementioned goals of automatic large-scale synthetic data generation for automating railway operation. And we would like to explain it step by step to you on the following slides. Let's take a look from a high-level perspective of our novel simulation architecture. It can be separated into two main steps. Step A, the simulation step. Step B, the consumption step where we have different consumer applications, for example, for testing purposes or for training machine learning networks. Step B, the consumption step is a big step. If we take a look at step A, we can subdivide it into three main sub-steps. Starting with the definition of non-regular, respectively regular situations, according to our operational design domain in our application lifecycle management tool. Second, we bring together digital map data, elevation information, 3D assets, FMUs, for example, for resembling advanced train functions and physics. And afterwards, after the data is simulated, we persist it so that it can be consumed by our consumer applications. Simulation architecture is built upon strong ecosystem around NVIDIA Omniverse. So we are using plenty of state-of-the-art tools. For example, Polarion as our application lifecycle management tool or Math Robotics for our data management and data storage. Additionally, we are using, for example, GitLab for our CI-CD task, Docker for containerization, ROS2 as our robotics framework and FMI, FMU for bringing together different expert simulations. For example, for our in-house developed train physics or advanced automation functions. In our two-day presentation, we would like to focus on the left-hand side of the simulation architecture. This means the actual data generation. We would like to omit the right-hand part because we're intending to prepare a dedicated talk with regard to this. How we're consuming the data, how we're utilizing it for testing and machine learning purposes. And now, Jose will guide you through every step of the synthetic data generation and give you extensive technical details. Thank you, Sebastian. In the following slides, I will give you a deep dive into our simulation pipeline. The first step in our simulation pipeline is to define a logical scenario in Polarion, our application lifecycle management tool of choice. This task typically falls to a subsystem tester. In this process, logical work items are defined using Scenic, an industry-standard programmatic programming language. Scenic provides us the ability to outline various options and ranges, and to create a new scenario for the concrete scenarios that we will simulate. In this example, we observed that from this single logical scenario, 300 concrete scenarios will be generated. The created concrete scenarios will leverage either the sensors for rail sensor set or the automated train sensor set. And they will incorporate either the VR430 train model or the MIREO train model. Additionally, each scenario will feature a vehicle positioned as an obstacle, placed within a range of two meters from our train. In the next slide, you will see how this all looks like in Polarion. Here you can see how the previous work item looks like in the Polarion Web UI. In the top section, we can select which ROS2 topics to record during the simulation. For this example, all published topics are being recorded. Additionally, we have the option to specify the NVIDIA extensions we want to use. In this case, all the in-house IPM NVIDIA extensions will be loaded. You'll notice that the logical work item is currently marked as New, which is the default status assigned to any newly created work item. Finally, in the bottom section, you can view the Scenic code that you saw in the previous slide. Once a logical work item has been defined, we can start the sampling process. The main goal of the sampling pipeline is to consume each logical work item that has the status New, and generate a series of concrete work items from it. These concrete work items define specific scenes that will be simulated later. The pipeline operates within GitLab and can be triggered either manually by a simulation engineer, or automatically on a predefined schedule. All newly created concrete work items derived from a logical work item are also stored in Polarion for traceability and management purposes, and they receive the default status Simulation Requested. Now, let's take a look at a concrete work item. Unlike what you saw in the previous example, concrete work items do not consist of Scenic code. They are defined in a standard JSON format. The description no longer uses sets of optional values or ranges, but defines more specific information. We can see that the JSON file defines the exact position of the train in 3D space. Also, a single sensor set and single train model have been selected. The file also defines the position of a vehicle that will be spawned as an obstacle in the scene. This file represents only one permutation of potentially thousands, each one with different train and obstacle placement, as well as sensor sets and train models. On the right side of the slide, there's a 2D representation of the concrete work item. The red dot indicates the position of the train, while the blue dot marks the location of the obstacle. Now that we have our concrete work item defined, we can start the simulation pipeline. The simulation pipeline runs once for each concrete work item. The pipeline operates within GitLab and can also be triggered either manually by a simulation engineer or automatically on a predefined schedule. The pipeline consists of several jobs. First, the first scenario job retrieves a concrete work item from Polarion that is marked with the status simulation requested. Then, a 3D scene is generated based on the details defined in the work item. The train and obstacle are positioned in 3D space at the specified locations, and the correct train model and sensor set are loaded. Next, the run simulation job starts, which is executed in an NVIDIA OVX using iSEC SIM as the simulation engine. During the simulation, virtual sensors record synthetic data while the train moves. Once the simulation completes, the output is saved for further testing for automatic train functions or for machine learning purposes. To understand the simulation step better, let's take a closer look at the core of our pipeline, the run simulation job. The run simulation job consists of two containers running in parallel in an NVIDIA OVX machine. The main container runs a customized iSEC SIM image in headless mode. This simulation features virtual sensors carefully calibrated by our perception team to closely match their real-world counterparts. Cameras, radar, and LiDAR sensors are positioned on their real-life mounting locations based on the selected train to be simulated. The iSEC SIM container also includes custom ROS2 message definitions for publishing simulation ground truth data. While the simulation is running, the synthetic data generated by the virtual sensors is published by our custom ROS2 publisher extension. LiDAR, radar, camera, and ground truth ROS2 messages are transmitted outside of the iSEC SIM container via a fast DDS network communication. Simultaneously, another custom extension generates ground truth data in ASAM open label format, a widely recognized standard in the industry. The second container running alongside iSEC SIM is a simple ROS2 recorder, which consumes all messages from our custom extensions and stores them in a ROSback. Once the simulation concludes, both the generated ROSback and the ASAM open label file are stored in MARV, our data management platform of choice. In this slide, we can see how this looks like in the MARV UI. Its row in this table represents a dataset generated by a simulation pipeline run. We can see in the tag section what train model and sensor set were used for each simulation. All datasets are stored here for further processing. MARV also offers built-in visualization tools directly within its web UI. In this example, we see a simulation run in HVLE, our test track area in Berlin. On the left, you can see visualization of the ROS2 camera messages published by the simulation, and on the right, the LIDAR messages. We simulated a flooding scenario and a broken tree on the tracks, which would have been very costly and logistically challenging to replicate in a real-world test. Once the simulation pipeline is complete, it updates the status of the corresponding work item in Polarium to simulation finished. Also, for traceability purposes, it attaches to it a reference to its corresponding GitLab pipeline and to the generated ROSback in MARV. This slide also presents an example of synthetic data generated in the simulation pipeline. On the left, the S21 line in Hamburg is shown. Here you can see the recorded LIDAR data and RGB messages from three cameras mounted on different positions on the train. On the right, we showcase our current results of radar simulation at the HVLE area in Berlin, incorporating functionality that came with ISACSIM version 4. Now that you have an understanding of how our simulation pipeline works, I'd like to share some key lessons we've learned during this ongoing, iterative and challenging process of creating a digital twin of the rail infrastructure. As mentioned by Sebastian before, our goals for creating a simulation were that, compared to real-world testing, simulations offer significant advantages. They are much more economical, safer, faster, traceable, and much more scalable than real-world tests. For our first simulation of the S21 track in Hamburg, we manually modeled the 3D scene, which led to highly photorealistic results. With it, we achieved two key points in our goals. Our simulation is unsurpassively safe and fully traceable. However, manually creating 3D scenes proved to be a very labor-intensive process that was not only costly and time-consuming, but also prohibitive to scale to hundreds or thousands of railway kilometers that we need to bring to our digital twin in the future. This went against our other 3D goals. The good news was that all three challenges stem from a common root – the manual process of scene generation. So, after our first digital twin of the S21 tracking Hamburg was finished, we set our focus to automating the scene generation process as much as possible. In the next slides, I will show you how we achieved this when working on generating the HBLE testing ground in Berlin, the second area that we brought to our railway digital twin. The goal of this process is to automatically collect and integrate information from various sources to create a high-quality digital twin. Let's have a look at how this works step by step. First, we incorporate elevation data available from public sources. Next, we pull weather information based on the dates specified in the concrete work item, which allows us to adjust the sky and the position of the sun accordingly. Then, we load rail infrastructure data and building models provided by our DSD colleagues from Digital Register. This includes objects like tracks, poles, and signals. Each object is annotated by labels and corresponding RTX sensor materials provided with Omniverse. This enables us to generate high-quality ground truth and synthetic data when these objects are perceived by the same way of virtual sensors. Afterwards, we add the selected train model to the scene with its corresponding virtual sensor set. Then, the vegetation layer is added. To conclude, we put as final layer the desired non-regular situation upon the scene, in this case, the floating. This process is fully automated within our scene generation pipeline. As a result, we've developed a method for producing rail digital twins that is not only safe and traceable, but also cost-effective, fast, and highly scalable, achieving all of our simulation goals. To give you a better impression how accurately our digital twin aligns with reality, I would like to hand over back to Sebastian, who is now in the HVLE test area. Welcome to the HVLE testing ground in Berlin-Spandau, this time in reality. This site, which we have recreated within our digital twin has been instrumental in real-world testing, including past trials for the SENSORS4Rail project and our ongoing automated train project. Here, we can safely place obstacles on the tracks to evaluate our systems under controlled conditions. Additionally, you can see our dynamic mockup unit, which we use to capture real-world sensor data and to assess the obstacle detection system currently under development. The exec sensor setup you see here has been replicated in our simulation, enabling us to generate highly comparable synthetic sensor data. This significantly extends our real-world data set for data replay tests, allowing us to cover the operational design domain more comprehensively. Now we come to the conclusions and outlook. So to sum up, we have achieved our goal and we built an automatic synthetic data generation pipeline for automating railway operation, which is highly economical, which is unsurpassably safe, lightning fast, fully traceable and massively scalable to support our specification, testing and development of these functions. This simulation pipeline is built around Omniverse and accompanied by a bunch of state-of-the-art tools. To name them, Docker, GitLab, Math, Simulink, FMI, OpenShift, ROS2, Scenic and Polarion. Now, after achieving our goals, what is next? So on the one hand, we would like to work on the test automation. This means to automatically inject the simulated data in our soft-in-the-loop and hard-in-the-loop setups of, for example, the automated train project. Second, we would like to work on the qualification of our simulation tool chain according to Senelec. At first, we would like to target T2 qualification. Subsequently, we would like to target T3 qualification. This means code generation for supporting our machine learning demands. Third, we would like to even stronger interconnect our simulation with further expert simulations. We gained very good results, like aforementioned, with the train physics or train automation modules. And we would like to build up on this foundation to make the simulation even more extensive. This concludes our presentation today. We would like to thank you for your attendance in our GTC talk and also for all your questions. And if you're interested in further information, please check our DSD website or get directly in touch with us. Furthermore, we would like to send out a big thank you to the whole IPM Perception and Data Factory team at DSD, without whom this achievement presented today would have not been possible. Thank you very much. Thank you. Thank you. New temperatures etrightαι Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.