 Good morning, everybody. I'm very happy to enjoy GTC. I'm Chiara della Casa. I'm a project manager in the Supercomputing Department in Cineca, located in Bologna, Italy. Today, I would like to present you a project that is related to preserve an ancient medieval tower in Italy using a digital twin built with NVIDIA Omniverse. Bologna is located in the northern part of Italy and is a medieval city. It is called also the city of towers due to the numerous medieval tower that once dominated its skyline. During the 12th and 13th centuries, Bologna had more than 100 towers built by the wealthy family as a symbol of power and for defense purpose also. The Garisenda tower is one of the iconic towers in the city along with the Asinelli one. It was built in the early 12th centuries by the Garisenda family. And originally it stood about 60 meters. That is about around 200 feet. But it had to be shortened to 48 meters in the 14th centuries due to the severe tilting that was caused by the unstable foundation. The tower leans at 4 degrees similarly to the most famous tower of Pisa and it requires constant monitoring. Like it like the tower of Pisa is leaned for centuries because the ground on which it was built gave some way after construction. It has undergone several restoration efforts over the century to address its pronounced lean structural integrity. In 2023, unusual movements were detected leading the tower being classified as high risk for collapse. Soon, the municipality cordoned off the surroundings area after sensor detected this tilt. And then we initiated to prepare extensive restoration plans. In December 2023, the Bologna measure announced a comprehensive plan for the restoration that is estimated to cost 20 million of euro and will span at least 10 years. This has been needed to secure and restore the Garisenda tower. To improve the monitoring in this complex scenario and also to assist the restoration and conservation of the tower, we decided to build a digital three dimensional twin of the monument using Vidian Omniverse in collaboration with the municipality of Bologna, the University of Bologna and as Cineca. The integration of the digital twin into the restoration process exemplifies how advanced technologies can help the preservation of historical and cultural heritage, ensuring not only the stability but also the longevity for the future generations. What is a digital twin? A digital twin is a high fidelity replica of a real world object. It is an accurate simulation of the object, the asset process that exchange with the digital copy ground through data that are useful to build this high fidelity replica, but also to be predictive and help to maintain and optimize the functionality of the object. In the case of the Garisenda tower, this is very useful because it helps us to understand the behavior of the tower, the object tower, but also to simulate its behavior when something is happening, especially in the case of problems and augmented tilting like in this case. So in the reality, the digital twin is a high fidelity replica that should be physically accurate and true to reality. The physical world should constantly send data to the digital twin in order to make it able to be predictive. Digital twin requires a new technology foundation that is based on four key pillars. The first one is open data standard. Digital twin requires a huge number of data. Hence interoperability between different systems is needed. And of course, seamless data exchange across platform is needed. This can be done also by using scalable accelerated precision time system like a performance computing infrastructure and data center that are able to handle massive data set and run it in real time. Real time physical accurate simulation can help us to make a real world physics digital twin and also to enable it for predictive analysis and fidelity simulation. And of course, artificial intelligence can help us to make the system more reliable and more capable to help us in taking decision. So digital twins are not only technology models, but they are a mix of data, technology, AI and capability to simulate. So why we decide to do it to do this digital twin in Bologna? Because we have one of the most important supercomputing center in Italy and in Europe. And we are hosting Leonardo. That is the ninth most powerful supercomputer in Europe, in the world. Sorry. So the first thing is that we have to do this digital twin in Bologna. And it is a keypad with 14,000 GPUs that allow to have parallel computing. It has 500 computational nodes that can perform highly complex calculation across multiple task, supporting AI, physics simulation and data analysis. These nodes are all connected via optical fiber. So ensuring that high speed data transfer can be done. And the Leonardo capability is very high in order to, for example, train chat GPT 3.5 in one day. It is equipped with different type of capability in order not only to perform AI, but also scientific computation that are useful for physics simulation, climate modeling and engineering calculation. So the Garrisenda Tower Digital Twin Project, CIE consortia that has been held by the Comune di Bologna, that is the project coordinator and of course the data owner. The partners are Alma Mater Studiorum University of Bologna, that is the Garrisenda Tower expert team. They are in charge of the structural modeling of the tower. We as CINEC are the infrastructure expert and we are providing the computational simulation of the tower. And of course all the resources needed to build this digital twin. And NVIDIA, that is the technology provider by offering its NVIDIA Omniverse technology. And they are also in charge of building the data ingestion pipeline. The Garrisenda Tower is equipped with more than 100 sensors that are installed in different parts of the tower. And that continuously measures temperatures, inclination, deformation. This is very useful for the structural health monitoring. This has been done also in the past. But this is also considered a big data that we can use to develop our digital twin. Towards this data, we can have a numerical simulation that can provide us with information about what is the data. What can happen in the future to the tower. For example, we can simulate the effect of a failure. Also, we can check the anomalies. For example, the regular sensor measurement as has been done in October 2023. And so we can understand in real time what is happening to the tower and act immediately. For example, by giving an alert to the population that something critical is possible to happen. What are the challenges of the project? Of course, one of the big challenges is related to data. How do we maintain a single source of truth? How can we consolidate all the different data sources in one place? This can be done by using our resources, our data center, and try to make all this data interoperable as much as possible. And then how can we simulate new action to take on the tower? What is the impact of the construction site on the town? We can reply to this question by using the capability of the university and their acknowledgement to be part of the project is very important to answer to this question. Then other points are related to the different stakeholders that can make an access to all the information that we can provide. The expert from the different fields that can jointly collaborate. And all of these questions can be, of course, speed up the workflow and reduce the communication cost and uncertainty by using a platform that can deal with different stakeholders coming from different parts of the world. So this is the potential of a digital twin. Of course, we can build this virtual geometrical 3D replica of the tower and then consolidates all the available data that we have. In this way, we can enable comprehensive situational understanding by visualizing contextualized information. And of course, we can also enable what if experiments with predictive impact by coupling existing data with models at various degree of fidelity. And if I fidelity measure available, we can also provide automatic segmentation of of scans. This is very important because in cultural heritage stage is very needed to improve the accuracy and analysis of the monuments. So the automatic segmentation led us to isolate different architectural elements, for example, walls, columns, and sculptures in order to make it easier to study the monuments, analyze the specific features of the historical artifacts and buildings. But also, high fidelity measures help us to track changes over the time to identify cracks, deformation or erosion in monuments. And last but not least, they help to build virtual reconstruction and create interactive digital twin, allowing all the stakeholders, historians, architects, but also to public to explore historical sites virtually. So what has been done until now? We put all the data that we have currently. So point cloud, laser scanner, autumn image, reconstructed surface that are useful to reconstruct the geometry of the tower, the sensor data, the pendulum, the stretch wire, and the model, the finite elements, model mesh and elevation model, that help us to understand the behavior of the tower and to try to simulate what can happen. What did we want to demonstrate by using all this data? We can, of course, make a visualization of the point cloud data that is interactive, that can be seen by different point of view. We can integrate our point cloud data with the cesium Google map style in order to make more realistic. But also we can insert the sensor position for the visualization and then couple it with the sensor data in order to have a dashboard that can show us what the sensors are measuring. So, Omniverse is a powerful tool that helps us to put those data all together. It's made by different components that help to import all the data. And the powerful tool that is up to this level, that is USD Composer, is a tool to enable photorealistic rendering from one side and from the other, help us to create, editing and visualize the digital twin of the Garry Zenda. USD Composer is a tool to enable us to create a digital twin of historical monuments, both for conservation and analysis. To simulate environmental environmental effects, like weathering light earthquakes on the heritage site. To enable immersive VR, AR experience for educational purposes for museums and to facilitate the collaboration for the restore efforts by allowing team coming from different countries to work on a shared 3D environment. And of course, AI driven tools are provided to build machine learning application. This is the high resolution point cloud that we have. You can see that is both outside and inside and is very useful to identify structural components and also to compare this point cloud, the status of the tower over the time and to detect where erosion and cracks. One important point that is this high resolution point cloud include the basement of the tower. This is one of the point that there are some problems and so it should be analyzed very well. The sensor data, as already said, about 100 sensors all over the tower. The sensor data measures local data like temperature, deformation, inclination that are measured at different levels of the tower and are used, of course, to track the changes to the tower shape. The deformation is related to the environmental and structural stresses. The inclination, of course, to monitor tilting or leaning of the tower over the time. And then we have some global data that are coming from the pendulum and from the accelerators at high frequencies that are measuring natural frequencies. The pendulum is like an historical or modern pendulum system, used to track long term oscillation and tilting. While natural frequencies are the frequencies that the tower is moving and the natural frequencies detect vibration. These accelerometers detect vibration helping to assess the structural integrity of the tower. Of course, change in natural frequencies can indicate damage, weakening or material degradation. So this is very important. And that's why these are acquired at very high frequencies like 100 Hz. This is what I have been done until now. So you can see the tower with this high resolution point cloud. So you can view the tower from different point of view. And then, of course, you can interact with it. So you can view the sensor list. As we said, we have different sensors, different type of sensors. And you can see how they are in the tower with different colors, with different representations in order to see what you want to look at. And then it is possible also to see the dashboard that are showing the data that are captured by the sensors. And so in real time, you can have all the situation of the sensors and see, of course, how they are going. So what's next? We want to go on with the, especially with the predictive part of the of the work. So what we want to do is data fusion with predictive models. And for example, in this slide, you can see on the left statistical forecast model that is SARIMA, Seasonal Autoregressive Integrated Moving Average. It is used for the tech, detect seasonal trends in time series data. For example, like in this case, a tilt variation over the time. You can see that over the time, the peak are more higher. Instead, in the right part of the slide, you can see the ARIMA model that is an autoregressive integrated moving average. It is used to predict structural deformation and identify possible risks based on the past data trends. So you can see that there is expected value and then the measurements that are quite spreading related to the expected value. And also there are confidence intervals that are useful to understand where this variation can be problematic. And so you can see in this in this graph, there are historical data, there are predictions. And so this is useful to assess possible future shifts. And of course, this is very useful for proactive maintenance, but also for the restoration effort. Another thing that we want to do is to try to predict the global behaviour of the tower using a machine learning approach. As you can see in the left graph, we started to see if there are correlations between different measures. In this case, it is presented the correlation between temperature and the normalised frequency. The red dashed line indicates a linear trend and it suggests that the temperature changes influence the tower dynamic behaviour. Also the correlation coefficient confirms that there is a strong link between thermal variation and structural response. And this is very important because, of course, it's to put together different measures to see how they are coupled. It's useful to understand the correlation. Sometimes, like in this case, it was not expected that the temperature can be a parameter that has this important value. And of course, this can be done only if we can use machine learning, if we can use supercomputing power to make this heavy simulation. In the right part of the slide, you can see some data about the pendulum. There are the measure data and the predictive values obtained by using a machine learning approach. machine learning approach. And you can, as you can see, the machine learning approach seems to be quite confident. And so we can use this approach to try to forecast structural changes. So this is the final video that shows the tower in the city. As you can see, there is a building surrounding it. So it's very important to preserve its stability also for security purposes. And of course, you can see that the omniverse can show us in this environment how the data can be coupled and how the simulation can be done. Real time data are very important for this work. And of course, supercomputing power, very accurate technologies like NVIDIA Omniverse are very useful for the cultural heritage. And I think that in the future, this application should be spread overall to help us to understand how to preserve and how to restore this important cultural heritage environment. Thank you for your attention. If you want to contact me, it is my email address and then I will reply to you with very pleasure. Thank you.