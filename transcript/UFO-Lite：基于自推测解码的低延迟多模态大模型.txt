大家好,我是西藤,来自百度今天给大家分享一下UFO Lite首先呢,我会介绍一下UFO Lite相关的技术背景尽可能让这个Talk self-contained接下来呢,将介绍一下UFO Lite的技术原理以及一些相同实验最后呢,给大家介绍一下UFO Lite的一个相关应用可以看到,当前的多巴泰大模型的竞争其实是非常的激烈从这个科技巨头啊,到出众公司以及学术机构都在做着相关的工作然后训练成本的动辄这个千万美金也是这个千摩大战是吧然后堪称是AI时代最昂贵的这个顶配的这个内卷这个多巴泰大模型呢,不仅是训练成本非常高然后同时呢,它的这个部署的成本也是非常昂贵的对于这种投过的这种应用呢就动辄呢,是有着这种千万甚至是一级的这个DU因此呢,多巴泰大模型的这个成本呢它是支持着这个一级DU产品持续增长的一个技术基持然后除了这个成本之外呢其实很多应用就是它面临的一个问题是说我这个模型的速度决定这个产品的一些适用的一些场景所以就当说一些这种思考模型调度模型包括一些agent以及包括我们一些那个像一些排体的模型特定的模型要求这个模型必须有就相当于模型必须有足够快的速度它才能够落地所以本次套呢主要是针对这个多巴泰大模型的这个推理成本以及这个这个推理性能的这么一个优化然后作为技术延伸呢也会给大家介绍一下与本次套相关的这个多巴泰大模型的一个训练成本的优化然后我们首先看一下多巴泰大模型的这个LM部分它为什么慢它的瓶颈究竟在哪里我们以这个输入Token就是可以看到以输入这一个文本序列为例然后模型呢它是处于这种layer by layer的进行一个前向转播然后直到预测出这个Next Token的一个logic也就是说下一个Token的一个概率然后呢比方说它可以通过这种类似贪心解码的策略从这个Token当中呢就是从选择概率当中选择一个Next Token然后选定了这个Token之后呢它会把这个模型在和这个就是Input Token在Concad起来在就是Recursive的进行这个模型的一个前向的一个传播然后可以看出呢对于这种模式的这个延时瓶颈它其实是非常明显的首先呢这个大模型本身的做单次前向它的速度就非常慢然后同时呢对于特定的这个任务有可能比方说像这种创作类的任务然后用户可以让你生成这种几千字的一个文案是吧或者对于这种就是长就是有让你来对于解题类的或者是对于这种Coding类的这个任务它有可能会执行这种数百次甚至上千次的这个迭代然后刚刚讲到这个瓶颈嘛所以说就是如图所示左边呢就是刚刚说到的这种Token by Token的方式的一个传统模式右边呢就是这种SPD的采用了一种生成式验证的这么一个一个范式然后呢在推理过程中就是我们需要选取这种快速的就是轻量级的模型它首先呢就是属于我们这里面叫Draft Model它可以通过N次的这个迭代呢就是它快速的就可以生成这种预测序列的一个一个Draft序列然后大模型呢它做这个它只需要做一次这种并行的Verification这种范式呢就看起来它似乎可以就实现这种无损或者说近似无损的一个加速但是呢这种SPD的方法它其实是有很多潜质的依赖条件的比方说呢就是这个Draft Model它生成的这个分布它必须尽可能的与原始的分布就要高度一致否则呢它这个接受拒绝率它会下降从而呢会导致这个模型整个的推理延时甚至会造成推理延时的一个增加然后另外呢它这种对于Draft Model它的生成速度它必须也是要显著的快于这种原始模型否则呢它也会就是增加到的这个额外的开销它可能会抵消掉这种并行加速的一个优势同时呢它对这个这里面我也可以Draft Model其实我们可以把它叫小模型这样更好理解就是我们叫大小模型这种小模型的它这个Tokenizer它必须原的这个大模型的Tokenizer它需要是相同的Tokenizer或者说至少说它需要有一个相近的一个Tokenizer才能实现这种加速然后这里面也稍微延伸一下就是这个比较火的这个Deep-Sec的它这个MTP就是Multi-Token Prediction顾名思义呢就是它通过这种多头一次预测多个Token它预测的FirstSecondThirdToken然后从而呢实现这种加速但是MTP的方法呢它其实依赖一种安特兰的训练当然呢它这个方法其实其实后面我们讲到可以详细来展开一下就是它跟UFO的Lite它其实是可以证交的它至少是跟跟我们这个DraftModel跟小模型它可以直接证交就它可以直接让这个小模型的这个预测的Draft的速度来翻倍当然呢它这个跟跟我们的这个大模型的这个这个叠加呢它需要就是刚当然需要更加深度的一个叠加它这对于小模型直接可以拿来就我们可以Office Chef直接拿来用的然后根据前面讲的这个背景呢这里面我整体的简单介绍一下就是就是UFO的Lite这个topicUFO的Lite呢它是业界首个就是多模态的这个投机解码的一个工作它呢主要解决了这个多模态投机采样因为它为多模态的这个投机解码它引入了很多新的一个挑战比方说多模态任务它通常涉及到大量的这种短token的一个回答就比方说让你回答yes or no或者来做这种选择体当这个序列长度小于这个窗口大小的时候呢就传统的这个SPE方法对于这种生成任务它的效果就是不是特别好这个后面在消融实验的时候会有具体的一个展开这里面不做展开然后另外呢就是说多模态模型它其实引入了这个视觉特征就是Vision Encoder然后与这个LM不同呢就是在这个多模态大模型当中有超过就是一般情况下在很多任务它是超过90%的token它都是这种视觉的token从而呢它会导致这个这个这个这个feature的空间呢它其实非常大从而呢就是使得这个接受的概率它会远低于这种LM的一个情况然后除此之外呢它包含了这种视觉的这种分支然后如果用标准的这SPD的话其实会带来额外的这种系统开销然后除了多模态大模型引入的这种独特的一些挑战之外呢就传统的LM的这种SPD的问题它其实也是存在的比方说现实当中就比较很难找到合适的这种Draft Model它不光是对于这种Tokenizer的一致性的要求同时呢还对这个输出Token的分布的一个要求同时呢它要求这个Draft Model它有足够快的这个推理速度然后UFO呢它与很多这个其他工作就是我们会看到很多这种加速的工作它是基于这种Python推理来做的就是它其实偏research然后UFO Lite呢它其实我们是直接基于这个Tokenizer的LM来进行研发的就是它这个是直接并且是跟这个缓存机制做了深度的一个优化它是可以在L220上面包括它基于这个IP8的一个加速它可以达到一个极致的一个推理速度它是直接可以就是面向这种产业级的一个应用然后同时呢这个UFO Lite这个方法呢它其实是适用于任意的一个多梦态大模型它其实都可以就是进行这种通过自协同来进行加速因为自协同的话它就没有这个Tokenizer的一个限制我们可以把这种就是通过Int4的一个TRT的模型本身把它作为一个小模型来做一个自协同所以它对模型选行上就没有任何的要求然后同时呢就是对于任意大小的这个Tokenizer的就相同的Tokenizer的模型比方说像这个72B的模型跟7B模型亦或者说这个7B的模型跟0.5B的模型亦或者说这个8B的模型和这个1B的一个模型它其实都可以来进行这个加速接下来呢我将介绍一下这个UFO Lite它的一个原理可以看到这是多梦态模型它的一个一个Speculate Dequealing的一个示意图刚刚也提到由于引入了这个Vision Encoder呢因此它会带来一系列的这个挑战然后我们逐一来看这些问题是如何解决的这里面其实这就是一个UFO Lite的一个就是SPD的一个属于一个速度扣的一个图然后这里面呢我们稍微展开一下就是可以看到如图和这个算法所示呢就是给定这个小模型就是跟这个就是P是这个里面的小模型然后那个Q呢就是这里面的一个大模型然后可以看到这里面的这个小模型和大模型它在交替的进行这个推理直到最终的铲出结果当然这里面就涉及到里面有一个Windows的概念这里面不做展开这个Windows后面其实有消耗实验到底说我这个小模型就是它到底是通过这个自律规的方式生成多少次然后大模型来验证一次这是后面我们其实消耗实验的一个参数这里面可以看到就是小模型它可以通过这个自律规的方式来进行K次的一个迭代然后大模型呢它只需要进行这个就是一次的并行处理所以说这里面就是可以非常快的来进行这个大小模型的一个协同然后基于前面所讲的呢可以看到这是就是这是UF Lite的一个整体的一个架构图如前所述呢就是UF Lite它其实这里面的Draft Model其实适用于任何的这个模型以这个图当中为例就它是一种最最最常用的一个方式就是因为这个我的大模型和我的这个量化的比方说4比特量化或者2比特量化的一个小模型就是它的本身的一个自协同它可以适用于任意的模型就是对模型没有任何的限制所有的这个模型都可以套到这个架构里面然后同时呢就是说如果我把这个里面的Draft Model就是小模型替换替换掉其实这里面也是可以适用于刚刚讲到适用于任意同Tokenizer并且它的那个就是满足一定约束条件的这种大小模型然后就是刚刚说到就是所有的off the shelf的模型你直接拿过来就可以用然后但是这个架构当中呢还兼容了一个就是一个training staff这里面为什么加上一个一个training的过程呢其实这里面完全是可以不做训练直接是就可以部署但是我们其实并不是要求我们这个模型有多么快的能够部署然后拿达到一般的一个加速比这里面training staff主要有两方面的一个考虑就是尤其对于这个4比特量化的这个模型呢它在在线量化它可以比离线量化有更高的一个加速比然后另外呢就是很多SATA的那个模型呢它其实加了很多就是大量比例的这个in-house的这个data就是它其实我们很多论就是它虽然说可能训练框架是开源了但是它的这个私有的尤其高质量的这个私有化的数据呢其实它并没有开源所以说呢很多实验我们没办法复现所以也没办法达到一个极致的一个加速比然后另外呢比方说对于我们现在很多像4考模型啊或者是对于一些其他的模型我们需要构建很多的这个数据所以说这里面其实相当说我们是一个可以灵活的选择就是说就是是否来训练除此之外呢就是对于在刚刚也提到在架构上对于任意的两个不同的这个tokenizer的模型来说就是它其实我们要通过这个训练可以解决它的这个模型分布的这个一致性的一个问题然后除了在在线量化之外呢其实这个UFO Lite它也支持通过就是蒸馏来进一步来减少大小模型的这个输出的一个地府然后这里面的这个蒸馏呢主要是针对这个模型的蒸馏就是和数据蒸馏相比呢就是模型的蒸馏可以直接作用到这个小模型的输出分布上从而呢就是说因为它直接是用大小模型的这个输出分布来作为我们的监督这个刚好它的蒸馏的Loss跟我们这个优化目标协同的目标是高度一致的所以它可以直接来提升这个接收率也就是说可以获得更高的一个加速比然后和在线量化一样呢这个蒸馏其实也是可选的就即便是我不做了任何蒸馏我们也可以直接用这里面其实在后面销容实验呢也会介绍一个蒸馏的一个收益当然呢这里面做蒸馏和我们做训练其实都是为了有达到一个极致的一个一个加速比然后这里面其实就介绍一下这个UFO Lite的一个实验就是为了相对的公平其实不同的模型它的训练数据不同它其实也并没有办法完全去达到一个公平就是我们这里面训练的数据呢就是97%它其实都是来自开源的数据集只有3%的数据是In-house Data然后这里面数据呢也是为了后面的一个消耗实验如果是说就是将训练的数据增加一个数量级然后并且可以involve更多的这个In-house Data的话然后并且我们加入类似这种RE的数据的话其实即便是0.5B的模型它其实也可以对于像思考模型啊对于推理模型啊对于解题模型它其实也可以达到一个非常不错的一个一个结果然后最近我也看到很多工作它声称吧就是0.5B模型就是通过这种RE的这种数据蒸流它其实在这种很多的就是推理能力上就是它其实已经超过了7B的一个效果其实这里面是同样的道理就是说它不是一个消融实验就是在消融情况实验情况下0.5B的模型它是不可能超过7B模型的然后并且呢我们这个其实这里面其实刚提到UFO light它其实并不是说跟其他的模型去比一个benchmark的一个指标而是说对于任意的这个模型其实我们都可以来写最后面讲一下然后这里面实际就是我们在实际场景当中呢遇到的问题其实比公开的benchmark它要复杂的多比如说比方说我们可以涉及到这种翻译类的包括涉及到博物馆文物啊涉及到数学能力啊涉及到建筑的一些识别啊其实可以到达几百个甚至上千个子的分类我们要在每一个分类上面都让多门碳模型做到比较好的一个结果所以这个公开benchmark它其实只是作为一个参考然后可以看到在训练数据及可比的这么一个情况下呢UFO light它可以达到和就是2B模型左右可比的一个速度然后并且它可以达到跟一个7、8B模型可比的一个精度然后这里注明一下就是因为我们这个这个SATA的模型每时每刻都在变化嘛所以其实UFO light它不是说跟某一个模型去比就是说我们是可以基于任意的一个当前的一个SATA的模型然后可以产出呢就跟它精度接近然后并且速度比它更快的这么一个模型它是一套通用的一个方法然后刚刚也提到嘛就是传统的这个投机采用算法它其实不能够很好的兼容这个短Token的一个任务然后UFO light它其实我们提出了一个叫confidence based的一个自适应的一个切换的一个策略就是对于短Token任务呢它其实也是有比较显著的一个收益的然后UFO light它其实我们提出了一个叫confidence based的一个自适应的一个切换的一个策略就是对于短Token任务呢就是对于这个inter 4的模型呢就是4bit的模型它其实这个量化的量化的量因为FPS像W4A8嘛就是说就是就是FPS它其实加速其实非常明显的就是4bit相比于8bit的话它其实或者是W4A8跟这个W8A16相比的话它有一个30%到40%多的一个一个加速就是4bit相比8bit的话它其实或者是W4A8跟这个W8A16相比的话它有一个30%到40%多的一个一个加速它有一个30%到40%多的一个一个加速然后但是呢它的弊端就是说4bit量化它的那个精度掉点是比较严重的其实从这个表中也可以看出来但是这里面结合这个量化训练再加蒸馏的话它就可以把这个精度补过来然后这里面就是为了跟其他的就是公平的一个对比嘛这里面Table3其实release是一个Python推理的一个速度其实如果说基于TensRT LRM的话这里面其实会有一个更高倍数的一个一个加速比然后随着这个就是Windows因为SPD里面它其实有一个就是Windows大小包括我们可以看到就是不同的这个Windows下它的这个接受拒绝率它是整体是变化的然后整体的一个结论可以看到就是对于这种就是我们在对任意的场景其实就是包括不同的那个长短任务啊对于这种比方说我们对针对解题类任务对于风控类的任务包括对于这种创作类任务其实不同的场景下我们不知道这个模型到底说需要选怎么样的一个Windows来做可以看到如果不合适的一个Windows的话它反而会使这个模型的这个就是它产生负面的一个影响然后所以呢就为了让这个方法可以更加鲁棒的应用在这个实际当中呢我们可以做了一个比较算是相对完整的一个消耗实验然后可以看到就是我们就是评估了不同的这个Windows大小下面它对于这个不同长度的这个就像不同长度可以就是理解为不同的这种多么大的一个任务然后可以看到对于传统的这个方法呢它的这个模型的这个整体的性能它对于这个这个窗口的长度它其实是非常敏感的然后可以看到就是说我们提出这个Dynamic window呢它可以有效的解决了就是它既可以达到一个模型相对较优的一个演示同时呢就是相当于说我们不需要在这个场景当中去精细化的一个调整这个任务因为实际当中比方说我们把这个模型给到下游的业务是吧就是我们比方说我们以API的形式去交付给客户但是我们是不知道客户场景当中到底是怎么样的一个任务的所以Dynamic window刚好就解决了这个模型的一个自适应的一个问题实验关系呢这个最后我给大家介绍一下这个UFO在实际当中的一个应用然后这里面的这个就橙色的这个这个这个这个颜色呢就代表这个这个小模型原始的输出的一个结果然后蓝色呢就代表它被大模型拒绝然后修正之后的一个一个结果然后可以看到就是我们通过这个大小模型的协同呢就可以把原本就是做不对的这种这种case就给它就是做对了然后这个这个其实就刚刚提到的就是对于这种短短query的回答的一些就是就是任务就只回答一扫弄可以看到就是第一个跟第三个case呢他他代表的就是说这个问题原本小模型他就是做错了然后他他他他就是大模型帮他修正过来了然后呢第二个case呢就是说小模型他的confidence不够然后他这个结果需要大模型来判断但是大模型判断之后呢他是一致的所以说他相当于经过但是他并没有拒绝然后第四个呢就是说小模型本身他的执行度就非常高所以说他本身就是不需要走大模型所以对于这种case他就会有一个显著的一个一个加速然后这个是也是一个对于这种quire长文本的一个一个大小模型的一个一个协同然后后面我给大家举一个呃这这也是这这相当于是类似这种推理类的一个任务也是就是本身原本小模型他其实可以看到这里面他把这个体就做错了然后经过大模型的修正他其实可以把它就做对了然后并且可以达到一个比较高的一个优素然后这里面其实对于一些复杂的一些尤其是比较密集的这种场景可以看到这个小模型原本是有幻觉的然后通过这个大模型协同就把这个小模型的幻觉给修正过来然后同样这里面这个这个问题其实也是这里面也是有幻觉的然后可以看到就通过这个大模型协同也是大小模型大模型把小模型的幻觉给修正过来行那整体的talk就给大家讲这些然后如果大家有问题的话可以通过这个email联系我因为我们 Go On