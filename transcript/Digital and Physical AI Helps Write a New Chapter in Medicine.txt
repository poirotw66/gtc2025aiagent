 Good afternoon, everyone. Welcome to GTC 2025. Nearly 20 years ago, NVIDIA made a big bet that accelerating computing was a way forward. And its most impactful application, advancing human health. Here's a glimpse of our journey into healthcare so far. For almost two decades, NVIDIA's accelerated computing platform has helped the healthcare industry use the most advanced technologies to improve human health. In medical imaging, breakthroughs in accelerated computing may be a part of the technology that has been developed. Accelerated computing made advanced diagnostics possible. New devices gave doctors better tools. Scan times shortened. Futures were changed. As computing advanced, so did the vision. CT scans now capture 4.5 billion times more information, revealing every detail. Accelerated computing made AI possible, and AI enables recognizing problems faster and catching diseases earlier. Now we are at a new frontier where every hospital, patient room, and medical device will embody AI. AI brings precision care to those who need it most. Genomics has advanced from decoding the human genome in days to minutes, turning complex signals into insights. Today, genomics sequences are being designed by generative AI and paving the way for groundbreaking treatments. With accelerated computing, molecules are modeled and simulated at atomic precision. New digital agents help scientists design experiments, develop therapies, and push the boundaries of discovery. Every breakthrough isn't just about technology. Hi, Maroka. Hi, Anna. It's good to see you again. It's about people giving patients the best chance to heal and recover. That's why, together, we can do what's never been done. Accelerated computing, AI, and robotics. Together with you, our partners, we're pushing the boundaries of what's possible. And this is just the beginning. Please welcome to the stage, NVIDIA Vice President of Healthcare and Life Sciences, Kimberly Pell. Wow. A theater with a balcony. Welcome to GTC 2025. On behalf of Kathy, the entire healthcare team, as well as everybody behind me, the amazing, incredible partners and ecosystem of folks who are on this mission statement to improving health together. This is just absolutely extraordinary times. At this GTC, a conference that has, in my mind, and I think many of you who are here joining me, has become the AI and Healthcare Conference. We're joined by over 700 healthcare companies from over 40 countries. We have something like 40 hours of content. That's an entire work week of incredible content across the critical domains of healthcare. We have the who's who from the industry leading companies who have been in this business, dedicated to this business on this mission for centuries, some of them. And some of the companies that are joining us here are actually just a couple of days old. It is absolutely incredible. Many of you have chosen to release some of your most important breakthroughs here at GTC with several dozen new product announcements and critical breakthroughs. And I hope to celebrate them with you today. And we really, really understand that this is a profound honor to be able to celebrate them with you today. So let's talk about accelerated computing. NVIDIA pioneered accelerated computing because we wanted to build computers and computing platforms that solve problems no other computers could. Accelerated computing, some of the earliest applications were in our field. In our field, they were in imaging, medical imaging, being able to reduce the dosage of radiation by something like 80%, but it increased the scan times and improved the image quality. Or genomics platforms that are still being pioneered today that without accelerated computing would not be tractable. All the way into things like molecular dynamics simulation, being able to see biology interacting at scales that allowed scientists to really see how medicines, proteins interact and design things at light speed. Accelerated computing also gave way to the most important technology breakthrough of our time that we're living through, and that is artificial intelligence. However, artificial intelligence isn't generalized in the case of what we need to do in healthcare. It actually is very domain specific. There's a reason why physicians, clinicians, nurses, they go to specialized schools. They practice for years on end, sometimes 30 years to become a domain expert in any given field. It's very domain specific. And so we dedicated ourselves to understanding enough about the domain, but translating it to the computer science that allows for these breakthroughs to transcend. And so not only do we create acceleration libraries, but we try to codify what's necessary from a domain specific perspective into platforms, domain specific platforms, whether that be for imaging, genomics, drug discovery. And now you're going to hear a lot about the future of healthcare robotics. One of the areas that is definitely an unsolved problem is biology. We still don't have a deep understanding of biology, not to where we need it. And if you look at the many, many decades of past, the extreme breakthroughs of platforms that gave us new sight into the resolution of biology. Everything from all the different sequencing platforms, proteomics. We need to look at biology at every single scale because it gives us much, much deeper understanding. We have ways of not only reading biology at incredible resolution, but the incredible breakthroughs. I just met somebody from Jennifer Doudna's lab of CRISPR allows us to edit these things, design biology. But we need an accelerator. And we have one now. We have the virtue of the incredible breakthroughs of what generative AI has done and large language models as a tool in our tool belt to apply that to the field of biology. And you can see that we are starting to experience some exponential levels of biology intelligence by being able to represent biology in a computer. Many of you in this room are pioneering the absolute breakthrough model architectures and downstream applications of being able to take a sequence of biology and train in an unsupervised way models that have some deeper understanding of biology to not only read it, but to write it, to generate it, to design it. And this has been a breakthrough that we continue to see almost every single week, new models that are coming out. We recently announced with the ARC Institute just a couple of weeks ago the world's largest biology foundation model, EVO2. This genomic foundation model was trained on something like 9 trillion nucleotides. One of its breakthroughs though, and you're still seeing how we have a lot of catch up to do, if you will, to what we've been able to achieve in large language models, in that we're still learning about this data. And how to get this data to be able to be represented and encoded into these computer science approaches of transformers. One of the things we have to do is, biology has much, much longer context length than spoken word and spoken language. Right? The human genome is 3 billion base pairs long, 3 billion sequence long. So working with the ARC Institute, one of the major breakthroughs of EVO is the context length. It is absolutely proven that context length has a correlation to the model's ability to understand. And so EVO1 to EVO2 increased context length from 100,000 to 1 million. This context length has extraordinary resources that it needs from a computational perspective. And so working with the ARC Institute, Berkeley, Stanford, UCSF, we worked on creating the ability to train these models in a massively shorter period of time. When you increase those context lengths and you're going towards these tens of billions of parameters of models, the computational resource is immense. So we need to optimize that both for the pre-training but also the post-training. And also what you're seeing here with a model like EVO2 that has more general capabilities is we're seeing biology models now also follow the scaling laws of the other artificial intelligence domain of large language models. Where we have pre-training, where we're taking gigantic 9 trillion nucleotides of information from hundreds of thousands of genomes and pre-training a model. But then post-training these models, you also want to be highly efficient. So we've got to make sure that the training is we're working on the efficiency there. So they're post-trained to do some amazing things. What's maybe a little difficult to see but is in the paper and I encourage you to read it is almost like an app store of potential applications of EVO2 because it generalizes across biology in some really profound ways. Whether that be being able to predict pathogenicity from variants or whether that is to actually at test time compute generate a plausible whole genome for the first time. So the applications of true foundation models like EVO2 which just came out just a few weeks ago is just starting to come into view here. What I'm super excited to announce is all of the work that we're doing in biology and I'm going to talk a lot about it in our NVIDIA BioNemo platform. Is we are going to be able to enable this level of scientific understanding in the computer, this digital lab if you will, right at the bench side on what Jensen hopefully you saw it, the NVIDIA DGX Spark. This is a super computer, I wish I had it here, they just logistically couldn't get it to me from the keynote hall. But imagine it's right here. You can hold this AI super computer of one petaflop of FP4 performance in the palm of your hands. You literally can place this right next to every single lab on top of every researcher and scientist desk. And it's going to come with this out of box day zero efficiency. All of the software that we've been building, all of the NIM architectures for biology foundation models, imaging models, large language models, post training, agent development is all now in the palm of your hand. I think what Jensen said is you can start reserving yours today and they'll be shipping in July. They're going to be offered by OEM partners from Asus, Lenovo, Dell and HPI. So you're going to be able to get these cute little super computers that are just so powerful to be essentially that dry lab next to you and do some really, really rapid research. We see the future as every wet lab needs a dry lab. I think we are starting to really understand why this is becoming more and more necessary. It used to be that experiments would try to answer a singular question, move to move on to the next decision point and do the next, right? Very sequential process. You recognize that what we can do in the physical world can be hugely augmented by being able to do things in the digital world. And so we've been on this quest to building a complete platform to be the complement to that wet lab. These are extremely intelligent scientists who have dedicated their life to the craft of doing the science. These are these different tools, the scientists, the data. So the data is set in the world. Why wouldn't we want to connect a data flywheel that is from that scientific question, from that ground truth generation, into what I would call biological intelligence, the IP? And now we have in this very right here, right now, today, not the future, the ability to connect these IPs into a workflow, also known as agents, and help them be along the ride. with your scientists, having conversations, automating the research, criticizing the hypothesis, kicking off an experiment, perhaps. And so the future is also going to have agents that will work alongside every single scientist and researcher. And you could do it on the palm of your hand in that little DGX. But you could scale that all the way to very, very large research questions in the cloud. So we've been working on this platform in VideoBioNemo for some time. And it's really starting to come clear how important this is for every single drug discovery, biology lab to complement their wet lab with this dry lab and set up this data flywheel. So we focus like we did on Evo2 to make the training of these models super efficient because we're nowhere near where we need to be in terms of their capabilities. So training is a super important part of what we do. Everything you saw in Evo2, we've released all of the optimizations, the training recipes back into the BioNemo framework and open source. We take really fantastic models, many of them that are built by you, and we put them into our NIMM infrastructure, our NIMM architecture. This is essentially take an amazing model, and we put it into a box of very, very high efficient compute with a simple API wrapper. Okay, NIMMs. And we have many of them, now dozens of them for BioNemo, Evo2 being one of the latest. We also have OpenFold and MSA Search that is just coming out too for a lot of the structure prediction and keeping that, continuing to implement very, very efficiently. And then we take these NIMMs and we put them into very efficient workflows. For example, we have a virtual screening, we call it a blueprint. This is an assembly of these NIMMs that can perform functions. They can do these, you know, otherwise very large calculations. And we do all this because we know that we have a approximately $300 billion R&D industry. Every year we're spending $300 billion on R&D and drug discovery. And we want to make every dollar of that count, right? Because there's still new medicines to be discovered. And so this idea of AI drug discovery factories to improve R&D productivity is becoming very apparent. In silico medicine has set the pace for the industry. Let's just say it. Alex and the team at In silico are now going from target to candidate within a 13 month timeframe. They've got 10 assets green lighted to go to the clinic. And they just announced how they're bringing humanoid robotics into the lab to, again, assist and automate some of the otherwise work to accelerate our ability to validate and get ground truth data. Many of these companies are screening billions and billions of compounds, but synthesizing many, many less because they're using the predictive power of these models, the generative capability of these models, the the optimization, the diffusion aspects of multimodal to optimize these molecules such that when they do go to synthesize, they can synthesize many, much fewer of them. Again, optimizing for the R&D productivity. And we're seeing this happen across the board. So complementing the drug discovery process now with the AI factory, where you're connecting that data flywheel that takes all of your science and experience. And codifies that IP in a computer so you can drive these incredible productivity gains. Not only that, but amazing computer aided drug discovery companies are also integrating AI. Here you have Biovia's generative therapeutic design that's built with NVIDIA Bionemo. We want to be able to deliver very high quality synthesizable compounds. And so the Biovia team in just a couple of weeks was able to put NIMS in for generative chemistry right into their generative chemistry workflow. They want to make sure that you have high quality synthesizable compounds, accelerating, you know, candidate developments. You can choose your own oracles, right? And so you have a customizable workflow here, but it's really injected with the power of generative AI. So you have a lot of data that you can choose to enable, you know, to enable, you know, very dynamic workflow and take a very data-driven approach to drug discovery. And also announcing Cadence and the Orion Molecular Design Platform. Cadence, this platform is cloud-based and it incorporates AI machine learning for these workflows. It helps to identify new medicines requiring billions of molecules. And it enables researchers to really efficiently explore the potential 1.2 synthetically accessible compounds. You know, when we're talking about trillions, what we do at NVIDIA to accelerate it, make it more efficient, and then make it easy. If this is too hard, the scientific research community just can't get a hold of it. And so it's been a wonderful experience working with the computer aided drug discovery platform companies to integrate BioNemo right into the platform to give the whole industry access. This ecosystem is really on fire and it's amazing to work with them. So the biopharma industry has definitely woken up. The idea of drug discovery AI factories is here. We're building the end-to-end model development lifecycle so that they can, the IP that they generate from their data is still the IP. And then putting it into workflows that really drive a modern drug discovery process. These AI native companies, the AI native companies, the tech bios that we call them, right, who are very data driven first, codifying all of that data into platforms that they can interrogate to accelerate the development of assets. It's just an incredible way of flipping on its head, the biotech industry into tech bio. And what we just showed you is the computer aided drug discovery partners are able to now take this otherwise very difficult to use generative AI and inject it right into user interfaces and easy to access software platforms that can empower the scientists and researchers across the ecosystem. So like I described, one thing is how can we build these foundation models, true foundation models? We still have a lot of work to do, which is why we focus on the pre-training and post-training. We want to assemble these models into workflows, but we also talked about maybe we want to complement them with things like agents. So NVIDIA has been really hard at work on building our AI agent platform. We have introduced, you've probably heard it at Jensen's Office, we have introduced new reasoning models. I think everybody in the room has probably heard of DeepSeq R1. Reasoning is an incredibly new frontier of large language models and what we're able to do. It helps us, without reasoning, we can't create what we need to, which is very advanced agents. We can help bring reasoning in with these new three models that we've created. The NVIDIA LAMATRON, excuse me, the NVIDIA LAMATRON reasoning models. I knew I was going to trip up on that. There's a lot of words. Based off of LAMA, that's why we keep the name there for you. But these reasoning models are incredible. And we came at them with three sizes. We wanted these agents to be able to be deployed onto edge devices or have deeper capabilities or in the future, you know, really ultra for that deep research and thinking that you're going to want to do. If you're going to arm your scientists with agents, these agents better be pretty smart. Right. And so we've really brought in a new set of models. And, you know, the the ecosystem, many of you I'm looking at in the crowd are creating agents of all kinds that are really going to transform not only the experience of health care, but also our ability to unlock the information that we have been digitizing for the last couple of decades, whether that be our electronic health records, whether that be digital wearables, digital wearable data, real world evidence. Our friends, a bridge are here. They've built reasoning writing right into their their platform for bringing in just incredible context when you can bring in the context of your patient. You know, if you go to the doctor every time they say, you know, what is your date of birth? Ask you all these things. What if you don't even have to do that? Because your reasoning agent in the background can go bring that into the context of the conversation that you're having with your physician. A bridge is already, you know, deployed with over 100 health systems and it's just getting better enriching the context behind these these clinician and patient interactions. Concert AI is really the idea of virtualizing, if you will, clinical trials. How can you take what is very, very complicated data across oncology and bring it into the realm of understanding so that you can stratus you can build a much better strategy according to your clinical trials? What sites should I choose? And be able to predict what patients your populations you're going to need and what is going to actually be the outcome of these clinical trials? And reasoning is happening throughout all of these, putting on chat interfaces so that you have many, many different domains across what is a complicated clinical trial process to be able to help everyone along the chain of clinical trials be able to benefit and accelerate the decision making process. Because every day you don't start that clinical trial is a massive opportunity lost for that drug to make its way into market. Hippocratic AI recently just announced that they have essentially a marketplace now of agents where clinicians and nurses and otherwise can get involved in the design of agents. Over 400 use cases they have identified. They have over 25 different specialties that they can target. And they've got dozens of installations going on with their platform as it is today. And now they've just empowered everyone in the clinical ecosystem to get involved in and build their own. And how they can take a holistic survey to talk to people in charge. So is helps we use the curious process at the scale. We Очень Eyp Min 68 These are all I see. My students are here какие- Häfen The current team is more advanced that can lead to us. this to be able to get you into the healthcare services that you need. And then a huge growing ecosystem of solution development partners and global system integrators. We're working with Accenture, Deloitte, with McKinsey and their Quantum Black team, Quantify, Wipro, across so many use cases. To name a few, Deloitte we've worked on with their frontline AI, which is built on all of our ACE and our avatar systems to, again, be able to build really lifelike, very personalized agents for different hospital and healthcare applications, whether that's preoperative appointments or otherwise. And then Wipro is working with some of the largest institutions helping to automate customer service. In the areas of payer systems, you're receiving a lot of phone calls. And so are there ways that we can help to streamline that so, A, we can get care to patients faster and otherwise build a lot more efficiency. So we're just getting started and there's so much creativity. And now we have a new notch in our belt, which is reasoning. So here I want to give you an example of a biomedical research agent and a blueprint that is going to walk us through. So now you have an agent and what do agents do when they can reason? What does reasoning actually mean? Reasoning means that you have an ability to use chain of thought and come up with a plan. And when you come up with that plan, you can then execute on that plan. You can act by calling into tools and data that can allow you to make the next decision. You can use reasoning models that even after you've done that, you can ask it to validate. Did I come up with something that is viable? And so this whole idea of using reasoning at the core of what agents can do is really amazing. And I'm going to show you some examples. So, for example, you want to, here we have an example of, you're asking, I really, I want to look at cystic fibrosis. I want to know a little bit about some of the recent small molecules that have been discovered. And I want to look at potential alternatives. Can you help me with this process? And so what is the first thing you're going to do? Okay, the first thing you're going to do, the reasoning agent is going to say, I need to go do some literature search. Let me go look what are some of the recent compounds that are identified and actually the therapeutic targets. And it essentially does that. It goes through, here's the literature I found. Here's the source. And this is the snippet in that source that I'm going to use. And then you're going to say, okay, well, let's build a hypothesis. What should we do next? And so it walks through the hypothesis where this is the core reasoning agent. Again, I identify the target protein for cystic fibrosis. In this case, it's CFTR. And then it says, okay, I need to get the person who queried for me, my scientist, a seed molecule to go into the next stage. What is the next thing we want to do? Well, we want to virtually screen. We want to virtually screen a whole bunch of molecules against this target. And so we're calling into BioNemo in the blueprint, which is a set of tools that can essentially do virtual screening. And you're seeing that here. And then finally, you want to create a research report. You want to summarize everything that you just discovered. And so here you have the summarization of the results. You can provide a structured report. You could have it read it if you want. And you can have the agent look through it and give you some criticism. And so the power, I go through this example because NVIDIA also announced today a toolkit called NVIDIA Agent IQ. And this is the ability to use reasoning models so that we can connect agents to agents. We can connect agents to data. We can connect agents to tools. And so I kind of just walked through how you might see a future scientist work by being able to ask a question and go through all of these four processes. And this is what is now in front of us. It's incredibly exciting. I'm excited to announce that this week, today, we are announcing that Epic is accelerating AI with NVIDIA NIMS on Microsoft Azure. So as you know, Epic manages health records for about 325 million patients worldwide, about 80% of the United States. My chart, which I have, is used by about 200 million of us. And we're partnering with Microsoft to make these NIMS available via Azure AI Foundry so that Epic can incorporate them into their electronic health record software. They have use cases. I think they've announced some, call it 200, that are going to be building agents that can improve patient care, improve provider experience, a whole bunch of operational efficiency that we can work on. And if you think about the ability that these NIMS are now available in Azure AI Foundry, there's just zero configuration for this deployment. You can scale as fast as you need to. And so we're really delighted to work with both Epic and Azure on these incredible NIMS that we put so much blood, sweat, and tears in making them extremely efficient, giving some recipes about how you can have these agents work collectively together to empower some incredible workflows. This revolution is just beginning, but it's rapidly accelerating. We went quickly past the perception phase into generative AI where we could really take the world's knowledge and then generate new things into agentic AI that, you know, allows us to do things like act, produce work, do tasks, intent-based computing. And we're at this next frontier where not only do we have digital agents, but we're going to have physical agents where we're going to embody AI into physical things. But in order to do that, we need to have AI that understands the physical world. And so you're hearing Jensen talk a lot about physical AI. Let me try to break it down for you. Physical AI is really about three computers that you need to do physical AI with. We've been working on these three computers for several decades. The first computer is to develop AI itself. All those perception algorithms, all of these large language models, and all of the agents, and what you have recently heard of is our world foundation models where you're taking, you know, potentially sensor information and turning that into action information. And so once you develop data AI, you need to have a place where you can test and train this AI and validate its AI in a place that isn't the physical world. Because in the case of healthcare, in the case of surgical robotics, for example, it's too dangerous. In the case of self-driving cars, in fact, it's too dangerous. We don't want to have to only deploy this in the physical world to find all those corner cases to teach a car what to do. And so simulation and being able to do synthetic data generation is a very important part of the three-computer process that has now just become incredibly powerful with the ability to do accelerated computing, true physics-based simulation, and now generative AI. And then finally, we need a real-time, run-time computer. You need a computer in the robot that embodies the AI and can do things and make decisions at very low latency. So essentially, you need a run-time computer. So three computers, a training computer, a sim computer, and a run-time computer. We've been working on the AI computer for a long time, and we make it domain-specific. One of the platforms that we've worked on with a large ecosystem, many of you in the room, is Moni. This has become the standard for medical AI development. You know, over 4 million downloads. We're talking several thousand papers now written using Moni. So many contributors. We're still winning competitions with Moni. And so we're so excited about the traction here. We've been working on it for five years. But it's now taking its next leap, and that is Moni Multimodal. And we've created the ability to, one, be able to work with all of the healthcare-specific data. It's not easy. DICOM, text, surgical video, electronic health records, different omics data. So we create all of the data I.O. transformation such that we can build these models in the first place. We then create models, multimodal, vision language models that understand 3D imaging and text. It's different than when you're just doing 2D imaging and text. It can understand surgical video and text. We can incorporate now reasoning. And so we also complemented these models with an agent framework, the ability to put something like a radiology agent together that can essentially, what's not so easy to see here, is describe its chain of thought reasoning, providing the healthcare industry with a level of transparency that's, you know, frankly required. We want to understand the chain of thought. We want to see the transparency. But we want to be able to use these very powerful tools. And so Moni Multimodal, we've contributed ourselves a couple of reference agents. So a radiology agent that has reasoning and can do report generation. Or a surgical agent that can do information retrieval. Or it can be a note taker while you're in surgery, you know, an AI watching the video and can take notes. And we're also working with, of course, great contributors and collaborators from Mount Sinai and RadImageNet on essentially a 3D vision language model and a chat CT model from Zurich. And so we're going to continue to expand this. But this is starting to put the power of multimodal into the hands of every developer. So that's computer number one, the AI training computer. It's now multimodal. Multimodal it must be. And it can be agentic. Computer number two, we've also been working on for some time, is NVIDIA Holoscan. This is our multimodal real-time AI sensing platform. So think of this as the brain that runs on the device. On the edge, can also run in the data center. But it's a real-time sensor computing platform. Today we're announcing Holoscan 3.0. This is an amazing launch of this platform. It has fantastic new capabilities to allow for essentially dynamic flow control. So you're going to want to, at runtime, depending on what's coming in from your data, that sensor data, you might want to make different decisions on how you're flowing through these image processing pipelines. And so now at runtime with dynamic flow control, you can do that with the new Holoscan 3.0 without all the crazy complicated coding. So we're just making this platform super capable for the age of agentic AI and physical AI and robotic AI by introducing some of these incredible features. And we have just so many amazing use cases for Holoscan. Today we also, in the booth, we have Barco and the soft security team that are introducing their suite of essentially intelligent OR. Their Nexus platform, which you could describe as the central nervous system of the operating room. You can essentially connect any medical device or streaming, you know, information and be able to add artificial intelligence onto it. And then you have a new soft acuity monitor called the Brilliant Assistant. And it has the ability to use voice. And instead of having to type into a computer or have the surgeon take any hands off control, be able to recall information about the patient. Could you tell me what prescriptions they're on? Can you tell me any comorbidities? Can you show me my preoperative plan? Can you show me their previous MRI? So incredible workflow efficiencies because we're bringing AI, agentic AI, and real-time AI throughout the entire operating room. So we have an incredible ecosystem developing on Holoscan. And I have a really special one to share with you today. We're working with a company called Synchron, which is working on brain-computer interfaces. They've created essentially a transcathode implant that allows for the signals of essentially motor actuation to be captured and translated so that somebody who has suffered from areas of paralysis can start to regain the ability to work with the physical world. So I want, on behalf of Synchron, I want to introduce you guys to somebody super, super special. His name is Rodney. So let's have a look. So let's have a look.データ Did Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Pretty amazing. Rodney and Synchron team. What a special honor. And Rodney, we can't thank you enough for your generosity, sharing your story, the Synchron team as well. This is what we can do when we're working together, taking advantage of what is otherwise technology and tools to solve incredibly important human problems. So we have the next wave of AI, which is coming in this physical AI. We have to increase the ability for us to create these opportunities, and that's going to be at every single scale. Every hospital is going to embody AI. Every operating room is going to embody AI. Patient rooms, you can see this happening already with our partners, Artisite, are going to embody AI. We're going to have 8 billion devices and 8 billion patients being served by AI. And so we have to give you the domain-specific tools that you need. And so the third computer that's needed is we're going to bring here is Isaac for Healthcare. Okay, and I want to show you quickly how Isaac for Healthcare works. Healthcare is one of the most complex physical industries in the world. Hospitals, medical devices, and operating rooms save lives for millions. But the world needs more. With our population growing and lifespans extending, we need to bridge the healthcare gap. Physical AI and robotics can help. That's why we built Isaac for Healthcare, a three-computer platform to develop, simulate, and train AI to understand the physical world. Isaac for Healthcare can fast-track the development of physical AI for robots and autonomous systems by developing in a digital world before deploying in the physical world. First, developers can bring their own robots, sensors, and patient models into a digital twin environment. Next, Isaac provides powerful tools for physics simulation, sensor simulation, and environment creation, mimicking the real world with incredible accuracy. Then, to get the robot ready for the real world, you can use imitation learning to teach the robot by showing it how to perform tasks and use synthetic data to help the robot learn different scenarios with reinforcement learning so the robot can adapt and learn complex tasks too hard for traditional programming. Teaming up with device and robot makers, we're speeding up AI and robotics in healthcare to expand the lifesaving capabilities of medical imaging, surgery, and the entire healthcare industry. Okay. Isaac for Healthcare is here. We couldn't do it alone. We did this with some amazing partnerships and contributions, whether it's the Auto Lab and UC Berkeley, the Pair Lab in Toronto, Georgia Tech. Many of you have contributed to this. And we're making this platform available in early access. It's going to come out with two reference workflows to start. One is how can we start to bring autonomous capabilities to medical imaging devices? Imagine being able to extend the ability for the world's population to get access, as well as into surgical task automation. So those two reference workflows will be part of the early access program. The healthcare robotics ecosystem is incredibly rich and growing. We have a fantastic relationship with Infusion, who is amazing at advanced medical imaging technologies, offering research platforms, consultants, ability to help accelerate your journey into this physical AI realm, as well as advanced image processing. We've had the pleasure of working with Moon Surgical for some time. I want to congratulate the team on getting the first FDA cleared ability for AI-driven motion of the robot, so the endoscope can track tools and move autonomously. This is an industry first and literally was just announced today. And then there's going to be humanoid robots that are going to be assisting all over the healthcare industry, whether that be elder care or in the hospitals assisting the staff. And we're super excited today to announce that we're expanding our partnership with one of the longest collaborators that we've had, GE Healthcare. GE Healthcare is a pioneer in diagnostic imaging. We've been working together on accelerating the ability to process data. We've worked recently together taking advantage of the generative AI era, creating some very, very powerful foundation models. One is a Sonosam track for ultrasound to be able to do real-time segmentation, and a generalist generalizing a lot of applications in ultrasound, and bringing AI capabilities to their over 500,000 medical devices. And here we are about to embark on the next journey. We are going to work on creating physical AI capabilities so that ultrasounds and x-ray can become more autonomous. Imagine the world's most important tools for both early detection and diagnosis. We can extend their reach if they can introduce more autonomy and the ability to reach outside the walls of hospitals or be more accessible so that screening can happen on a much more regular basis. And so I see our friends here. Thank you, Roland. Thank you, team. This is an amazing extension of our partnership, and we have so much great things to come to extend into a much larger industry when we can take these powerful sensor technologies and give them a physical AI, embodied AI capabilities to extend the access to healthcare. And so quickly to wrap up, I know I'm gone a little long. This is the most exciting time in healthcare in at least my 20 years and I think for several of you. Drug discovery is on the precipice for understanding biology in ways we never have. Drug discovery AI factories are being built across the industry. You're seeing the computer aided drug discovery platforms integrate AI, making it so much easier for researchers and scientists to benefit. And you also see that every scientist is going to have a digital scientist working alongside them so that they can move their hypothesis thinking. They can move their research even quicker. Digital health is a gigantic industry. It's something like a trillion dollars short of humans that we need to address with digital health. We have a new powerful tool called reasoning that's going to give us the ability for agents to work with other agents. Agents can run anywhere. They can run on a device. Moon Surgical in our booth will show you how they have agents running, seven of them, right on the robot itself in the operating room. They can run in the cloud and they can scale to hundreds of health systems like a bridge is doing. And then medical devices is moving into its next phase to be able to embody AI and become more autonomous with physical AI. We have worked on the real time sensor processing platform, Moni with AI, and now we're introducing the ability for you to build these robots where they're born. They want to be born in the computer with Isaac for healthcare to train, test, and develop them and then deploy them into the real world. So with that, thank you all for coming and I hope you have a fantastic GPC.