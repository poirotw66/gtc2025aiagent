 Hello. Welcome to my presentation. Today I'll be talking about productionizing LLMs for quantitative analysis of market risk. And this is an exploratory analysis that me and my team is carrying out in Barclays. First of all, let me introduce myself and my team. I'm leaving a technical hands on engineering team here in Barclays for Barclays Chief Technology Office. We benchmark all new technologies related to HPC and AI, both software and hardware frameworks. For production users, our team standardizes specific technologies based on business requirements. With respect to software frameworks, we benchmark, we test accelerated frameworks around AI models, training, fine tuning, deployment, inference, monitoring, explainability, guardrails, controls around all these models and how this can be implemented from a technology point of view. I will also benchmark distribution frameworks, distribution frameworks for data workloads, for HPC workloads. Then my team is responsible also for benchmarking and bringing data science platforms, on-prem data science platforms, cloud platforms, and also SaaS platforms. We benchmark software design, storage, databases, a big chunk of work is around databases, particularly now with Gen.ai and all the new retrieval methods. So vector databases, graph databases, even GPU accelerated databases. And also we are in the world of traditional data operations. So how do you prepare the data in order to train a machine learning model? How do you do feature engineering, feature selection, MLOps, and also of course retrieval methods around semantic retrieval, RAD graph methods. So all these software's, all these benchmark performing by my team. And at the same time, we have a series of benchmarks around hardware frameworks, around GPU solutions, alternative AI, silicon chips for model training and inference, storage solution that come in the form of appliance software and hardware, and next generation hardware components. So next generation NVMes, persistent memory solution, and so on. Finally, we do perform occasionally research. I have been working quite a lot around quantum machine learning where I have a couple of publications. And also, I were working on numerical optimization algorithms, particular convex nonlinear optimizers with constraints. So that's an overview of my team. And for this presentation only, I'll be showcasing how we benchmarking Nvidia technologies, also both the hardware and the software frameworks. And in this presentation, I'll actually be focusing mainly on the software frameworks, because this is where that, yeah, I see, and I find all the current advantages in the HPC world for AI workloads. This presentation should not and is not considered as a technology endorsement. It is showcasing how we experiment, benchmark and test the Nvidia technologies, the same process applied to any other software and hardware vendor. So let me start with an old story. The story actually for benchmark Nvidia has not started now, but it is a very old, I mean, seven year old, as you see here, story that started back in 2018, when we brought in our lab data center at DGX one. So for those of you that don't know what the DGX one is the very first Nvidia high performance computing, specifically focusing on AI, that time actually machine learning workloads, with Kudai, CAF, CNDK, MXNet, TensorFlow, and so on. Look here, we have torts, we don't have Python. So it was all days the time, but probably we're one of the first UK bank leveraging back in the days Nvidia GPUs for HPC workloads. This is an old post that you can find that time in the world. So I think that's a lot of people who didn't have a lot of attention because people didn't know what is a DGX one, but now I think everybody knows. So why market risk analysis needs AI? First question. Market risk is inherently dynamic and is influenced by several microeconomic factors, geopolitical events, and so on. So several parameters affect market risk. Analysis must process vast amounts of unstructured data in real time to assess risk exposure accurately. Traditional approaches, manual reviews, rule-based models fail to scale with increasing volume of financial disclosures. Also, one thing that I have not put here is that we do have also multiple sources of information that need to be actually collected at the same time simultaneously. And as time goes on, the amount of information and the channels of information is growing month by month. The challenges you have a data overload. All this, we have lack of real time analysis, and we have ambiguity subjectivity. So that means that different information can and most of the times is interpreted differently across different analysts. So LLMs and in general, Gen AI approaches offer context-aware natural language at the studying and enabling efficient information extraction from my financial documents. RUG, particular retrieval augmented generation, so with RUG, retrieval augmented generation, allows models to retrieve real-time market data instead of relying solely of pre-trained knowledge. And also RUG is a very cheap alternative of training, fine-tuning or continued pre-training and LLM model. Also, AI reduces time to insight, improves consistency and it causes quality risk modeling. Of course, all this should be with respect to the correct risk assessments before deploying anything into production. Correct controls and guardrails on the top and all the analysis that I'll be talking here is experimental. So it's not yet in production. So we are experimenting. So let me go to the requirements. First of all, this solution and this experiment that we carry out a fully on-premise GPU ecosystem. And the reason why it's on-premise for regulatory or strategic reasons, because quite a lot of business areas do not plan and cannot plan into cloud solution. So we need always, which is a vital, I mean, at least for me, to maximize GPU consumption at any step of the process. GPUs come with a price, therefore we have to maximize the consumption. GPUs should not be there only for the inference. GPUs should not be there only for the actual model training. We can nowadays leverage GPUs even for data preparation, even for databases. There are quite scalable technologies out there. So maximizing GPU consumption at any step of the process. With maximizing GPU consumption, we mean that a workload needs to actually scale across GPUs through GPU interconnectivity on a node level, if we are having a single node or on a cluster level. So independent of the workload, we need to have auto scaling in the most in the most simple way in order not to go visit back our code and make changes. That's the very first thing. The second thing is GPU segregation. Not all AI use cases are gen AI use cases. Still, the majority of the models that are in production are traditional ML models, Random Forest, XGBoosh, Light GBM, and these require a fraction of the GPUs. Therefore, having multiple teams working on a single GPU by maximizing its consumption is a very efficient way to make the most out of all your GPUs in production. Orchestrating completely different types of workloads, real-time workloads, batch workloads, dealing with structure and unstructured data simultaneously. This is a very basic requirement as well. All the solutions need to be fully containerized and Kubernetes compatible. All the environments currently that run HPC workloads run on Kubernetes. Therefore, we need to standardize and we have standardized Kubernetes. Scalable with respect to compute, storage, and distribution. The end user does not know and should not know how many documents has as an upper limit to upload on your application. Should not know how many concurrent users your application is able to post. Therefore, your application needs to be agnostic to compute storage and how the distribution of workloads is happening under the bonnet. Usual workloads that we're dealing with is around a million documents, average 100 pages, 500 users in total with simultaneous concurrent users, 10 up to 100. So this is the scenario that I'm testing. And here is a proposed system architecture for the on-prem hardware setup. So starting from the bottom, we see the four GPU servers, each one hosting four GPU cards, this point, H100. So in total, we have 16 GPUs in total. This is what I was talking before, the four GPUs in total. So we have four GPUs in total. This is the Kubernetes management nodes that the actual users will be actually logging in. So usually, the user interact with the clusters through the control plane. And typically, we use the cube CTL command line to create and retrieve information about, yeah, to create and retrieve information for the actual compute nodes. So these were the actual users log in. And this is the storage. So storage scalable. So, yeah, which is agnostic for the system. So based on requirements, we can add more, yeah, we can add more nodes here. So this system currently has a capacity of on-prem 80 terabytes of data. We are starting as experimental, it can expand up to 500 TVs of data on this four node storage solution. So, yeah, the clients are actually, as I said, everything is internal, no internet connectivity at all. So yeah, actually, proxy is in case that we need to connect externally, but currently, we don't need even the proxy. So we do need to connect the user to approach all activities, you know, on, ease in case the NAFROessment programyz user is operating in caseç†Ÿ10 then here needs the Portland system. We also have an ability to connect internally, check inside the network First of all, we try to standardize as much as we can on open source. Open source solution because they have wide adoption, support by the community, and tested many, many times across many, many different industries. Performance. Software selection with respect to performance should be done depending on the exact use case requirements. Fastest, biggest, largest is not always the best option. Good enough, 70%, 80% good is usually what we go for the majority of use cases, not for all the use cases. Interoperability is a key component for selecting software. Transferable framework for a variety of different systems and different use cases is a key because we want to ship and lift one framework for a different use case without having to do a lot of engineering work. And of course, security, full control and ability to integrate all company policies and systems, legal compliance, cybersecurity, and so on. So these are usually best practices for software selection. An example for a query and retrieval, I'm going to show here using the NVIDIA NEMO. So the NVIDIA NEMO, we use it to curate, customize, and retrieve the relevant information. Here are the retrieving information that can be found publicly. And U.S. Securities and Exchange Commission. These are publicly available data. And I'll try to retrieve information about Tesla. I'm just using Tesla as a well-known company. So the interesting thing with the NEMO framework is that it is fully configurable to the policies of your organization or to the policy of your business area within a big organization. NEMO is using under the bonnet, the proprietary language, sorry, not the proprietary language, an open source language, which is called Colang. Do not confuse it with Golang. It's written Colang. And it is fully configurable in a very easy way. Let's see how the retrieval looks like. And from all these documents, we ask, and this is the analyst query. It's an actual questions around, yeah, Tesla's increased exposure to lithium pricing and changing manufacturing. The NEMO retriever depicts the correct document. And does the proper rug under the bonnet. So the rug, I'm going to show you what does under the bonnet. And it starts retrieving the information for the Chinese manufacturer and the lithium exposure price point. And the actual document that this information is retrieved in is this one, which is again one of the documents that I showed you before. So for the architecture that I'm going to show you, it uses a series of NEMS, which NEMS are NVIDIA microservices, fully, fully repackaged that run on the top, that run natively on Kubernetes. They are built on the top of CUDA. And they can scale up independently on the workload and the concurrency automatically. And I'm going to show you a scenario how we can actually scale within a Kubernetes ecosystem these type of frameworks. So the question here is how we actually benchmark and experiment with NEMS. So I'm going to show you an example of actual benchmarking. I'll be using the Metalama 370B model. I'll be using 2048 tokens input output tokens 1024 for FP16. I'll be using open source and NEMS. I'll be using the open source solutions and the Tess RRT from LLM from NVIDIA. And here are the solution, the results. So you see concurrency for 100 users and you see tokens per second, actually, for 100 users. So you see VLLM performs okay. NEMS outperforms everything. Olamet, TGA, this use case that I'm using, they are okay. They are performing okay. So for 10 users, we of course expect these numbers to be higher. So for this use case, the NEMS framework, so tokens per second, is outperforming everything. And tokens per second is quite important because if we increase the actual output tokens, what we are interested in is about the overall end-to-end from the first token till the end. So for long context window, tokens per second do actually count quite a lot. Now we are going to see the same thing, but we are going to see time to first token. And you see here for time to first token in milliseconds, let me add logarithmic scale. So you see pretty much the NEMS and the VLMs are pretty much the same. And of course, both of them outperform on LAMA and TGA. If we go now for 100 users, we see the results. We see still that the VLM and the NEMS outperform the Olamet TGA. VLM and NEMS are pretty much the same, but the important thing here is that we are talking about FP16 here. Next slide, I'll be showing you the results for a smaller model, which is quite good actually for any RAD implementation. Input tokens, output tokens are the same, but now for FP8, for this use case, the NEMS outperforms everything. It performs VLM, the Olamet, the DGI with a big gap. And the reason is that it is optimized for FP8. And most of the applications, at least for RUG, I mean, for a GNA, FP8 is good enough. Now, this is, I showed you a way how you actually select the NEM. So how the entire workflow looks like. We have a document. Then we have a PDF parser. And we have a NEM here, which is a YOLOX NEM, which does object detection, chart as images. Then we have, yeah, VLM, element detector, OCR. And therefore we have table extraction. And everything then is filtered and ingested. And actually through the NEMO retriever, they populate the vector DB. And now the NEMO retriever does the re-ranking and sent to the LLM. By the response, the user has query. And this is the workflow based on the documents and the vector DB here that we have. We have a query. The NEMO retriever goes to the vector DB. The NEMO retriever does the re-ranking. The LLM sends a response. And here we have the user feedback. So everything is self-contained. The important thing here in this workflow, which, I mean, for me, it's one of the most important stuff, is that we are not bound to use at this point an LLM, a given LLM. We can use any LLM. We can use any retriever. We can use any OCR solution because it is containerized. Using NEMS within an NVIDIA framework is super optimized. So I mean, I have found out that, yes, you can build your own containerized solution. Sometimes it's preferable. And for some use cases, we do build our own fully containerized solutions. But if you want to skip quite a lot of engineering effort for auto scalability and so on, then NEMS are a good solution. Cluster monitoring metrics. That's critical. We need to have, and this is an example that we use one of my GPU clusters, we need to have full monitoring because based on the monitoring, we can actually start scheduling jobs and auto scaling the cluster. So monitoring can be done fully with open source technologies, Prometheus, Grafana, and you can have full visibility of our Kubernetes cluster and GPU utilization. So you can see scheduling. You can see scheduling based on limits from your Kubernetes cluster and so on. So it is quite useful and pretty much very compatible with all data centers, because these are the standard solutions. Auto scaling based on the metrics. Having all the metrics from Prometheus and Grafana from the various pods, we can actually send this to Prometheus operator and have a metrics aggregator. This metrics aggregator will actually collect all the metrics from the GPUs as they come, but also from the Kubernetes. This will be coming from HPA, the horizontal pod autoscaler. And based on the criteria that I have set, for example, in case that you have GPU utilization, more than 80% allocate dynamically to more GPUs of this pod, then we are able to auto scale the actual workloads based on concurrency, based on, for example, if the one pod here is a rack based on document uploads, based on requests. So these pods correspond to independent workloads within our workflow. Conclusions. I think that the best key takeaways from this presentation should be that maximization of the GPU utilization at all times is a key. GPUs are expensive. Therefore, we need to actually maximize their usage, if possible, throughout all the steps of a given project. And this actually does not apply only to GPUs. It applies to any component of your, any hardware component of your workflow. For example, another example would be NVMe. NVMe are also expensive. Therefore, leveraging NVMe as an additional memory, slower memory, but as a memory buffer is another good utilization of high-performance storage. All the solutions should always be containerized and agnostic on the actual workloads. That can be achieved by engineering the containers in a way that can auto-scale automatically without having to do anything under the bonnet in a real-time production environment. All this requires very robust monitoring. Monitoring for your GPUs, monitor for your Kubernetes, monitoring for your storage. And the monitoring should actively be proactive. That means to actually, based on past experience, to be able to locate a problem before this achieved. And currently, what we are actually working on is based on the metrics that we gather to have an ML model on the top in order to say that if this happens based on past experience, be cautious, because this might occur with this probability in the future. Thank you very much. Please let me know if you have any questions. Thank you.