 Hi everyone, thanks for coming. So today I'm going to be talking a little bit about the big data challenge of radio astronomy and how we're applying NVIDIA technology to try to manage this data challenge. But first I figured I'd try to define what exactly we mean by when we say radio astronomy. So as you may know, the visible part of the spectrum that we see with our eyes is actually just a tiny part of the electromagnetic spectrum. When we talk about radio waves, we're talking about the low energy, low frequency, long wavelength part of the EM spectrum. We're looking at centimeter to meter scale wavelengths. Why this is interesting to us, of course, is because our own atmosphere and our planet is actually not transparent across the entire range of the electromagnetic spectrum. Our atmosphere blocks the energetic end of the EM spectrum, which is great because otherwise we'd be radiated by all sorts of stuff coming from the sun. There's a small window for visible light, of course, which is what we used to see. And then a very, very large, what we call radio window. This window is, of course, used for satellite communications because the atmosphere is transparent in this part. But we can also use this to try to look for signals actually originating from space. So the most famous example, perhaps, in radio astronomy is the search for extraterrestrial intelligence as immortalized in the film Contact. You know, we actually can do a lot more with radio astronomy. All sorts of interesting objects out there in space are very, very bright in radio wavelengths, from neutron stars to, for example, the Event Horizon Telescope black hole image. It can be very interesting. We can look for interstellar molecules, which tend to resonate on radio frequencies. We can use radio to probe sources of galactic and cosmic magnetism. And then also with radio, try to look back to the era before any of the first stars or galaxies formed to try to understand the physics of the dark universe. But radio astronomy is actually quite difficult compared to optical astronomy because of optics, because we're trying to observe things in wavelengths that are much larger than visible light. So just as an example, you know, we see things with our eyes. The aperture is about four millimeters. And that sort of defines the resolution that we see things at. To see at the equivalent resolution in radio, we would need to have an aperture of 40 kilometers. So that's 25 miles for people on the imperial system. And this just really isn't practical as an infrastructure project. So what we do instead is interferometry. And interferometry is essentially trying to simulate or emulate this larger telescope using multiple small telescopes. And just as an example of how far interferometry has come over the past few decades, so this is actually the center of our own Milky Way telescope taken by the VLA, or Milky Way galaxy taken by the VLA telescope located here in the States. We can see that SGR-A is actually the supermassive black hole at the center of our own galaxy. So we see some of the bright magnetic fields. These things labeled SGR are the supernova remnants. And then this sort of, this object labeled radio arc was a big mystery for a long time in physics. People thought maybe it was a gigantic cosmic string or something. But recently, actually just in 2022, we have an updated much higher resolution, much more sensitive, so we can go a lot deeper image of our own Milky Way galaxy taken with the Meerkat telescope, which is located in South Africa with 64 dishes. So many more dishes, although they're much smaller than the VLA telescope, where now we can see much more detail about our own Milky Way galaxy. So we have way more detail with the supernova remnants. So these are sort of the bubbles. They're basically explosions from stars in our own galaxy. We can see there's not just one, what we call radio filament, but many, many of these filaments that sort of seem to point in the direction of our own black hole, Sagittarius A star. So this is a great discovery in astrophysics, and it's actually an active point of investigation to sort of understand what's creating all of these structures in our own Milky Way galaxy. And we're actually going to try to take this further with this currently under construction project called the Square Kilometer Array Observatory, so named because it should have one square kilometer of collecting energy area when it's fully complete. This instrument is going to have two sites, one mid-frequency, so targeting centimeter scale wavelengths, located in South Africa, extensively extending the meerkat array. So we'll be going from a diameter of about eight kilometers to a diameter of hundreds of kilometers. So in principle, we should finally have our human eye resolution equivalent camera, which can take pictures of the radio sky. And we're also working on a lower frequency, so sort of meter scale instrument, in Western Australia, sort of shown in the photo on the side here, uses slightly different technology, not dish antennas, but instead sort of looks more like TV antennas. And this is an NGO project, so many different countries across the world have come together, forming a treaty sort of similar to CERN, for example, in terms of particle physics. We have a construction basically has been taking place over the past 10 years, with an operational budget of two billion euros. Construction has started about three years ago, and is well underway, and we're expecting to begin early operations in 2029. And we hope to be able to run this instrument for an operational lifetime of 50 years, so it'll be going for quite a while. And when we say that this is a next generation instrument, it's going to really improve the field of astronomy, this is what we mean, is that as a function of frequency will have much, much better sensitivity compared to current instruments that are currently in operation. And what this means is, you know, compared to optical telescopes, which are a little bit easier than radio, this is just an example, for example, taken by the Hubble telescope, this deep field, where just in this tiny patch of sky, they see 3,000 galaxies. But because radio astronomy is limited by interferometry, limited by the longer wavelengths, the corresponding pictures of this field are very small, you may be see 15 radio sources over the same patch of sky. But with the SKA, we will be able to see radio emission basically for all of these equivalent optical sources. So we'll have thousands of radio sources, where this is actually three-dimensional data, of course, because we also have the associated frequency information, so the response of this intensity is a function of the frequency that we're observing in. And what this means in terms of getting the radio in addition to the optical, I just thought this was a nice example. This is the Hercules A galaxy. These sort of bright stars are the stars in our own Milky Way that are between us and this little sort of blobby elliptical Hercules A galaxy. All the sort of blobs behind that are actually other galaxies in our own universe. In optical, Hercules A isn't necessarily that interesting. You can see sort of this sort of blurry mass of all the different stars. In radio, however, it's a bit more infamous and a completely different story. This is the corresponding picture of this galaxy in radio. You see these massive structures that extend far beyond the extent of that galaxy. These are actually many times larger than our own Milky Way galaxy. And we believe, in fact, that this is all cyclotron radiation coming from what is essentially a cosmic particle accelerator. And the engine driving this is a supermassive black hole at the top of the center of this galaxy. So being able to see these things in radio basically gives us completely complementary information about these objects, gives us a great probe of really extreme physics happening in space, and lets us learn a lot about the evolution of our own universe, in addition to studying supermassive black holes or this jet process. However, we don't get this for free. I mean, the interferometer is ultimately a computer telescope, and it becomes very computationally expensive and very computationally challenging. I personally love this paper by Ye et al, published in 2023. They did a very high resolution image of this field in the northern hemisphere. This is just a tiny cutout that they were showing here. And just making this picture took them 52,000 core hours. So on the computing system that they had in the Netherlands, that took them six days to create a picture that was only one day, so eight hours of observational time. And then to really, this is only using a subset of the full array, in fact. When they used the full array to do the image, it took them a quarter of a million core hours, so multiple weeks to create this image of just one day of observations. Of course, they learn a lot by doing this more expensive imaging. You can see that they go from having this sort of blurry, smeary effect to actually being able to make a structure that looks very, very similar to what we saw in Hercules A, so a similar cosmic particle accelerator from this galaxy. And the square kilometer array, of course, is going to be an even bigger and more computationally expensive instrument. We have a very significant data flow. We're going to have terabytes per second basically coming out of the actual instrument. We're going to need to be basically doing the calibration imaging of that fairly in real time. We're not going to be able to afford to be doing this processing in scales of weeks to process images that only took hours of observations. And then once these images are calibrated, the plan is to distribute them to a network of what we call regional centers, so regional data centers located around the world, where then scientists will be able to access. And in fact, actually anyone in the public, so in 2029, if you guys want to look at any of these nice resolution pictures, in principle, anyone should be able to log on to one of these regional centers and access any of the data products created by the square kilometer array. This network, again, is going to be located around the globe. We'll be sort of piggybacking off of some of the existing links. And in Switzerland, our current plan is to host our own SKA regional center as a versatile cluster in the Alps supercomputing infrastructure that is located in Lugano in Switzerland, sort of running alongside some of the other scientific platforms that we have running. So just to explain the computational challenge a little bit more so we get some numbers and scalings, the problem coming here is that we may not have that many antennas, we don't have thousands and thousands of antennas, but each pair of antennas creates what we call a baseline. We basically take the correlation of the voltages measured by every single dish here. So the numbers of those baselines increases as the square of the number of antennas. And then, of course, we want to basically measure that every eight seconds. That's sort of the time that we tend to average over. Because as the sky rotates, we kind of sweep through Fourier space. And you want to do that over eight hours, give or take. So very quickly, even if you just have 64 antennas, you end up having hundreds of millions of points, which you need to use to eventually reconstruct your image that you try to analyze to sort of figure out what on Earth is going on in the center of our own Milky Way galaxy. And the current estimation for the square kilometer array is that we will need a 200 or 300 petaflop system in order to run the imaging and calibration workflow. This is really high-throughput computing rather than high-performance computing because the actual number of operations we have to do isn't super high. It's more that we have just a huge data volume running through the system. And most of this computational load, which I've kind of highlighted in red here, is really coming from Fourier transforms, and specifically non-uniform Fourier transforms, that we need to do in order to do this kind of interferometric imaging. And so we're exploring a lot of different solutions now at the university. So the big one is high-performance computing, which will be the focus of this talk that we'll get into in the next couple of slides. But I also just wanted to mention a few of the other things that we're exploring as sort of more research-oriented topics. We've looked into quantum computing, and actually the quantum Fourier transform is very, very useful for radio interferometry. Just need to get the gate error down by another order of magnitude, and then it starts to be feasible for the image sizes we're looking at. So hopefully we'll get there over the next decade. And then just because we're here at GTC, I also wanted to mention a few of the projects that we do on artificial intelligence, we do image reconstruction on the sphere using deep neural networks, we use diffusion-supported classifiers for data discovery, so anomaly detection in some of the data sets that we've reconstructed from precursor telescopes, so telescopes that are currently running. And then we also have a research project looking at developing physics-informed neural networks, neural networks that can actually obey the dynamics of dark matter, for example, for cosmological simulations. So just in terms of high-performance computing, we've done some algorithmic development just trying to rethink how we do this interferometric imaging step. Where I just wanted to show this as an example, our original speedup is here on the Y-axis. Original Python implementation, of course, is always at 1, so you're not speeding that up. Refactoring this entire code into C++ made things faster by about a factor of 2, which is pretty good. But we got a much better speedup from completely rethinking the algorithm, so improved about that by an order of magnitude, basically. But then we really got the advantage, and in particular the scaling advantage, by refactoring things into CUDA, so the top plot and stars at the very, very top. So GPU computing has been very, very useful for us for refactoring this code and trying to basically make things as fast as possible. But the full workflow in radio astronomy is very, very large and extensive. And this sort of picture that I've showed you on the previous slide is just a teeny tiny piece of one of these boxes down below. And we basically need to get everything kind of ready to scale in order to support the commissioning of the telescope, which is happening now as we're building the dishes and they're coming into operation. So we're working both on benchmarking. So this is an example of benchmarking results from one of the state-of-the-art pieces of software in radio astronomy, where we have the speedup factor on the y-axis and just increasing the number of threads on the x-axis. And you can see that only one component here is kind of scaling. This is strong scaling, so it's really not doing very well. A lot of the components don't scale at all, which is not good. And so one thing that we've been looking into is the NVIDIA Rapids and PyData ecosystem. Sort of all of this sort of easy drop-in GPU computing that are easy for astrophysicists to kind of use and develop. They don't need to learn CUDA. They can already all know Python. And so we've been investigating if this can actually help us manage these gigantic radio astronomy workflows. And now I'll turn it over to Etienne. Okay, is it working now? Okay, thank you. So now I'm going to give a few slides on some ongoing work that is looking at using a new format for radio interferometry data. But first I'm going to just quickly present the actual default format that we have for this kind of data, which is the CASACO measurement set version 2 format. It basically can be seen as some sort of relational database where you have main table. Here it's the red square box there that contains the data. And around it you have a set of auxiliary data that gives some further information about the data containing the main table. Each table there maps on the disk as a directory and in each directory you get a bunch of files which are either data files or metadata files. In the listing below, so this is just part of the listing of the main table for synthetic data set that I will introduce just after. You see the files that don't, well the name of the file don't give you information about what is contained in these files. But the biggest one here, the 212 gigabytes correspond to the data column from the main table. And this is the column that basically contains all the visibility data that Emma was speaking about before. One thing that you notice in this listing is this table.log file at the end. This is the way this MSV2 format manages the access to the table. Meaning that if you're in a read-only mode, then you have a shared lock case where multiple processes can read from the file. But in case a process want to write to the table, the process needs to acquire a unique log file so that it prevents any other processes to access the table at the same time. So this is really not great for parallel IO that we need for handling the huge amount of data that we want to process once they will be generated by the SK. And to that end, there is an open source project called XRADIO that is working on the development of a new format, which is called the MS Measurement Set version 4. And this one is based on X-Array data sets, so X-Array being lab-alented dimensional arrays. It will use the ZAR storage as a backend, which gives you two, several things, but the main point is that you have chunks which maps to single files on the disk. You can get compression out of it. And of course, I mean, this is designed for efficient parallel IO operations. And another feature is that once all these data sets, there will be Dask ready for processing. So I'm not sure if you're familiar with Dask, but Dask is a Python library that kind of brings you for very little effort parallel processing in the Python ecosystem. So really behind this project, really scalability is the master world that we can scale to the volume of data that the SKA will produce. Very briefly, this is just a small comparison. Just ingesting this 200 gigabytes of data using MSV2, MSV4, using in each case 10 workers or 10 processes, single core in parallel. And you see that with MSV4, you get easily a time to speed up with a default chunking that is obviously not optimal. But with a bigger chunk of the data, then you can again get much better performances. So here, a point is that when you do chunking, it's obviously that you have to conduct a bit of a survey to see what would be the optimal chunking that you need for the kind of application that you want to run on this data. The data set I've generated for this small experiment that I will show here. This basis on the full model of the Scalo telescope using the 512 antennas. You get the picture on the left hand side on the top that indicates you where the stations are. Each by the station or the station clusters are, each dot being a six station cluster. And in each of these cluster, you get 256 antennas. The output for 100 time steps and 512 channels, we end up with like 26 billions visibilities to process. What we want to do with this data set is to produce 8K by 8K pixel images using the CUDA Finu FFT library, this QFINu FFT, to run a non-uniform FFT on CUDA. So with CUDA on the GPUs. We have two frameworks, MPI for reference and Dask because we investigate Dask within the project. I'm not sure how familiar you are with these two approaches, but with MPI, you basically define yourself, the task and the communications of the task. And to reach a solution, but with Dask, you kind of formulate what you want to get. And basically Dask, given the computing resources that you give to him, will define tasks and will handle fully automatically all the parallelism of the computation behind. The difference there in the approach that with Dask, you have to trust what Dask is doing. In MPI, at least you know exactly what you're doing, assuming that you do the correct thing. We tried both approaches, so we've compressed and non-compressed data using the NVComp library, which is also a library from the RAPID ecosystem that can do compression decompression on the GPU. So the big red cube here would be what you get out of the simulator. So corresponding to the monolithic file that I've shown before, the 212 gigabytes of data. If you run the MSV2 to the MSV4 converter, then you end up with a chunk, a series of smaller ND arrays, the chunks. In this case, I've chunked every time step and every channel. So ending up with like 51,200 chunks in the dataset. On each of these chunks, we run the new FFT that will produce each a single image. And at the end, we basically average out all these individual images to get the final image. And that's what you see here. So the point source that were simulated are basically where you see the yellow colors, the yellow points, into the final image. So nine point sources. Simple words about the hardware that we're using. So at EPFL, we have a GPU cluster running H100 GPUs on that. And for the results that I'm showing here, I'm using up to 32 full nodes, which is 128 H100 GPU cards. So for the Dask, the Dask environment, I'm using the Dask CUDA, also from the Rapids, which is Dask but running on GPU workers. And I assume that schedulers and the workers are always up and running before I start the solution for the timing. And for MPI, a chunks are equally partitioned over the ranks for the solution. So all that works. Okay. So a nice thing with Dask is that it provides you with a live dashboard of what's going on with all the workers in your system. Which is pretty nice when you develop and because you get very quickly an intuition about how the processing is going. What you want to see is really packed information. You don't want to see any white there or trailing things that takes ages. By the time this small video is running, we have 16 workers, with a re-chunk of the dataset of 42 channels, we have produced almost 16,000 individual images out of these chunks. So again, to come back on the choice of the chunk size, this plots, in a few words, it indicates you that given the chunk size, which is on the x-axis, the time to solution varies a lot. And the point is that really the chunking should be thought well advanced before going to production, because this has a major impact on the runtime. In this case, we see that we've re-chunking to 64 channels gets you to be kind of the fastest solution for all worker distribution. For the results, where we stand now, in green and blue, you get the MPI solutions, which is scaling pretty well, and which are the fastest solution that we have at this stage. In this case, you don't see much difference between the non-compressed data and the compressed data that we loaded into the GPU decompress before processing, just because the compressor that I've used, this LZ4 compressor that I've used, does not treat the kind of data that visibilities are because they look like random data. And this compressor doesn't do a great job, but we have another compressor that we will inject at the future stage there, and redo the experiment with that. With Dask, we came close, but not at the level of the MPI yet. The last two solutions, they should look better because of the dimension of the dataset. There was a bit of load imbalance that was injected there. But up to 32, this is what you get. Yeah, one thing that... One point, last point. So, with Dask, you rely on Dask to distribute and the processing. But as it was also shown in Perkins et al. in papers in 2024, there is things that you can do to impact positively the behavior or the choices that Dask is doing to get great speed up and scale, nice scaling when you increase the system. In our case, so basically what they did is to improve the data locality in the Dask processing. So, it avoids to Dask to transfer data when this is not needed. You want data to be processed together to set on the same place in your processing framework. In our case, what's helped a lot to get better Dask timing was to inject a worker plugin on the plugin to accumulate locally the individual images. And in the performance report, you see that basically the transfer between the workers was greatly reduced. Thanks to that. But one of the next things that we have to do is to see if the work from Perkins et al. can be applied also in our case. Okay, so with that, I give it back to Emma. Thanks, Satoum. So, just to sort of wrap that or sum up a little bit, it does look pretty promising so far, but there is a lot much further to go because we have to sort of extend these like little toy examples that we've been looking at to the full pipeline, which is something that we're going to be working on over the next few years. But of course, the clock is ticking because the telescope is being built. We need to have something that works by 2029. And one thing that we're really interested in and continuing to explore is this GPU decompression. So, what we saw with NVComp wasn't that effective because radio astronomy data is very noisy. Unfortunately, our receivers are just sitting out there in the desert. And so the LNAs mean that the voltage signal we measure is basically constantly jittering. And unfortunately, this doesn't work super well with compression algorithms like LZ4. However, this can be something of a blessing in disguise because that means you're quite robust to lossy data compression as long as our compression errors are incoherent. Meaning that if I average the compressed data over a long time period, those different errors should sort of cancel out. And in fact, this kind of approach is used for currently operating radio telescopes. For example, the largest radio telescope in Europe called LOFAR has their entire 22 petabyte archive compressed using this DISCO algorithm, which basically achieves truncation or basically does bit truncation, 3-bit quantization or 4-bit quantization, to get a much, much smaller data size. And so this plots on the side. We're just showing the example signal from one baseline. You can see that it's quite noisy. The compressed signal when you go down to 3-bit quantization is still quite noisy. But the important thing is that because we're often averaging these signals together, not over this kind of instantaneous seconds, but over many, many, many hours, this is okay to add this amount of error. And we can get massive, massive compression. Unfortunately, there's no GPU implementation of this algorithm, but this is something that we're very interested in exploring and seeing how it works with NVComp. We're also very interested in exploring AI-based compression, because this has worked very, very well for climate data, which in some ways looks similar to radio data. So using GANs, VAEs, and especially to combine this with data reduction, leveraging Fourier wavelet and shapelet decompositions for sparsity, which have been applied very, very successfully for AI image reconstruction in the field of radio astronomy. So just to sum up, I wanted to just give some thanks for, of course, the funders of this research that we've presented, thanks to the EU and the SNSF, and also some special thanks to our colleagues in South Africa, who are also trying to deal with this big high-throughput computing problem. And I just wanted to advertise for PASC. Generally, if you're interested in scientific computing, we do this big scientific computing conference in Switzerland, where we talk a lot about these issues of high-throughput computing, high-performance computing, and especially artificial intelligence. So if you're interested, you're very welcome to come attend PASC in Switzerland and sort of learn about what's going on in the Swiss community. So thanks a lot, everyone, and we can take questions now. All right. Thank you so much. Big applause. Do we have any questions? We have one. So where are you doing your compute for the radio telescope, right? Because you don't have a giant pipe that goes to Switzerland. No. So are you doing it, you know, on-site with the radio telescopes? And if so, how are you shielding them? Because you have these fans and everything else that's creating RF signals? So it won't be directly on-site. Meerkat, for example, they have their computing center kind of behind a mountain compared to where it is on the radio telescope site. Probably we'll have a dedicated, for the 300 petaflop system, we'll have a dedicated supercomputing center in Cape Town. They're currently trying to figure out, doing the procurement essentially for that, to figure out where that site will be. For the one in Western Australia, there's this POSI supercomputing center located in Perth. So that will host all of the calibration imaging workflows for the low-frequency instrument in Western Australia. Awesome. So what does that look like? Does the, you have signal that's, you have light that's converted to signal, electrical signal voltages or whatever. It's sent on a pipe that goes directly to that facility in South Africa? So we'll have on-site, we'll have on-site correlators, which are usually put in sort of like shielded, smaller shielded containers. But yeah, so we'll have the actual signal then gets put into like fiber optic cable in the ground. That then goes through some correlation and sometimes some like transient detection that's on-site, so stuff that's like very, very low latency. So like if there's a big supernova happening, then you really want to be able to see that that's happening. So you can tell observatory management and control, let's point the telescope there and see what's going on with this very interesting astrophysical explosion. But the more complicated stuff, so especially calibration, which is like this very expensive iterative process because you have to solve for your sort of unknown antenna parameters at the same time as trying to reconstruct the unknown sky. Those signals then get sent on a dedicated link to the supercomputing centers, which will be either at Cape Town for the South African telescope or in Western Australia. Did you say it was like 300 terabits a second is what you said was coming off of it? What did I have on the slide? No, I don't think it's I think it's like seven terabytes per second. Seven terabytes per second. So you have seven terabytes per second going into your data center in Cape Town. Where do you guys write that like on your file system? Like how are you doing with it like there? Right? Okay, so I understand how you're getting the data from there to Cape Town. Great. So now what's happening there? Are you caching on local disk or are you do you have some super like where where's that data being written to the moment it enters that data center? Yeah, that's a good question. I don't work on the ingest unfortunately. Okay. But maybe that information is on the SK website. Yeah. Thank you. I really appreciate it. Thank you. Maybe one point on that is that the SK in the end should be some kind of a real-time pipeline. So I think they don't expect to store all the data for the full pipeline. They will need to reduce it before they can come to some storage. Thank you for the talk. It was very interesting. I was wondering when you compute those FFTs, how much data do you need to have for an individual's computation that must run sequentially? For an individual. So we can actually partition the problem because the FFT is a linear operation. We can kind of partition it into like whatever smallest operation fits in memory. I don't know if I have. Yeah, that's the number I'm interested in. Like what is the smallest amount of memory you need to work with? Do you remember what we've done for BIP? On top of my head, not like this, but I mean you can count first of all for the number of visibilities that you have. At least in the pipeline blue build that we've been working on, you kind of build a square matrix, which is the number of antenna times the number of antennas. In that case, the case of SKLO, this would be 512 by 512 or a bit less. Half of that because you need only a lower part of the matrix. And it depends also if you do single precision or double precision computing in the end. But after that, of course, you need some space also for the output image that you will produce. And there, this can be a bit more problematic if you want to go to very large images with like, in that case, this was 8K by 8K. But in the end, we would like to produce images which are 80K by 80K. Yeah. So, as Emma said, we can do that sequentially somehow and accumulate the results. So, you get a bit of freedom there to partition the problem size. Yeah. So, just to have some order of magnitude, is it like kilobytes, megabytes, gigabytes? No, it's gigabytes or tens of gigabytes. Yeah. Tens of gigabytes that need to be done in like atomic sequence. Exactly. Yeah. Okay. And... I mean, in the processing, usually the amount of RAM that you have memory available will be certainly the limiting factor of what you can put and process at the same time. So, this is typically something you can tailor. You can query whatever requests, whatever free memory you have on your GPU. You can adjust the size based on that, what you can compute at the time and go like this. Just trying to fool the GPUs. Right. So, 10 gigabytes might be a little too much. But, have you guys looked into FPGAs for this? Yes. There is actually another research group at EPFL looking at CGRAs and they're prototyping on FPGAs for this kind of operation. Yeah. I think that's a very interesting way of approaching it. Very good. Looking at the time, maybe just very briefly, can you comment, like, understand you're going to be generating big images about visualization systems you use and how you want to do data analytics, visual analytics and the outcomes? That's a good question. So, traditionally in radio astronomy, everything is kind of done by eye. So, this, like, the analysis of this is done by eye. With the new, much larger 80K by 80K images, we're sort of now seeing this very interesting paradigm shift in the field of people finally developing more automated techniques for the analysis of these images. So, like, some very interesting analyses of, like, galaxy cluster mergers now using AI to try to identify some of the shock fronts in these objects and AI trying to look for sort of new galaxies or, like, new objects out there. Yeah. Beautiful. Thank you so much. Big applause to our speakers. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.