大家好,我是张泽华我是李健我们来自京东广告研发团队今天分享的主题是基于Tensor RT-LLM在广告场景的生成式推理加速方案这次分享会介绍我们在生成式推荐领域具体做了哪些工作其中,重点会针对于工业化落地场景尤其是在具有高囤土、低延时这种要求的生产环境我们所做的生产级推理加速方案本次分享会分为四个章节首先会介绍一下在生成式技术蓬勃发展的时期在电商广告场景我们会有哪些新的发展期间而行业、产品、技术的升级又可以帮助我们解决哪些广告业务的痛点第二,将介绍生成式算法具体在广告场景如何应用落地第三,为了解决生成式模型推理延迟的吞吐压力我们在工程实践方面做了软硬件协同设计以及诸多优化来达到算力的极致释放最后是总结与展望其中,前半部分将由我来向大家介绍后半部分的工程实践将主要由我的同事李健为大家来讲述具体的技术细节方案那么我们进入第一部分广告领域的发展契机真人士大模型蓬勃发展时期其实给广告的整个生态带来了许多的契机它重塑了非常多的产品流程、交互形式我们第一代的生成式技术在广告系统中的应用主要是面向于像广告主等弊端的用户上线了例如广告促销文案的生成广告图片创意生成来提升投放效率与投放的效果而伴随着大模型技术蓬勃发展之后的逐渐收敛像训练框架、推理模式都有大踏步的发展像TP、PP、EP等训练模式都逐渐伴随着进行成熟在推理方面首次Token的延迟也从最最开始的十几秒到现在毫秒级就能产出而这些技术的发展都给我们在广告场景的应用生成式算法提供了非常坚实的底层基础最后伴随着不断的探索生成式的大模型在广告领域工业级的应用范式也逐渐被探明然后我们看看生成式大模型的引入可以帮助我们解决什么样的广告业务痛点第一个知识融合我们认为不论是广告也好还是电商场景下的广告也好本质上就是一个知识理解的过程用户的知识比如说用户在什么样的背景下访问了这个应用又因为什么样的需求而产生了购物的欲望环境的知识比如最近的热点新闻节日商品的知识就是商品的信息而传统的电商场景建模其实是具有非常多的局限性的没有办法很好的去融合这多种知识而大语言模型则是一个相对理想的知识融合器也是一种知识提炼的工具当它提炼的知识不够及时容量剩下不是足够大的时候我们也会通常尝试通过RAG的方式去注入更多的知识和信息第二Scaling Law广告推荐场景的主流的CTR模型都是一种稀疏部分偏大但是稠密部分偏小的模型放大它们的参数量可以达到的边际收益锐减其实是非常严重的而今的大语言模型时至今日所表现出来的Scaling的曲线都是远高于传统模型的所以我们认为在这个方面大模型的增长模式是我们追求的一种增长模式即如果能够给它更多的数据和算力资源它可以获得较传统模型更多的收益至此引出我们对于应用生成式大模型的几个核心目标第一参考大语言模型的建模思路提升模型的效果提升我们的匹配效率这里人或匹配的效率大幅上涨也是我们一个重点追求项基于大语言模型的自然语言理解能力来实现知识的融合与注入第三基于多模态的理解能力提升我们对像产品图片促销等各种各样多模态物料的理解而为了在广告系统这种耗时极其苛刻的场景下实现大模型应用目标其实是非常具有挑战性的而我们通常是一种比较贪婪的我们既要又要还要那么我们既要找到一种应用级别中可以持续提升收益的算法演进路线想要设计出能够随着数据的规模和计算资源的变化而能够有高效拓展的算法找到这样合适的算法而只是第一步我们还需要确保这些算法能够快速的落地这就要求我们在开发的过程中特别注重工程实现的效率和灵活性最后低成本高性能其实是我们追求的最终极目标而在工程的实践中我们需要不断变化和优化我们的计算效率从而获得商业化场景上的优势那么进入第二部分我们来看一下生成式的算法体系首先简单介绍一下整个广告系统中的传统的算法链路主要包含图上的几个关键环节当然很多广告系统其实是图中的架构的这种变种或者是它的升级版本如图中锁左侧锁式当一个流量请求到来的时候会首先帮助这个用户找到有可能感兴趣的商品这通常都是几万几十万个然后通过一个粗筛也就是到达千级别这里就到达了粗排的环节然后经过一个精筛筛出到百级别这里叫做精排再给每一个广告的商品加上它所相关的利益点标题等创意信息最后再经过一个重新的排序过程后面就基本是要展示给用户的信息了而我们把这个过程系统的来看整个广告系统其实就是在解决三个核心问题第一个核心问题就是检索的问题检索就是从一个库中尽可能多的找到用户感兴趣的商品第二就是对于整个感兴趣的商品的集合进行筛选这里其实就是一个信息过滤类的问题最后我们把过滤和选出来的商品再补充上广告场景上所特有的数据比如说广告文案促销信息等等这里其实是一个信息补足的问题而这三个问题中一个是关乎于理解即如何去理解用户的兴趣需求另外一个是匹配其实就是能够帮助用户找到他最感兴趣的内容最后其实是知识融合的过程也就意味着说外部的知识是如何更加有效地注入到整个算法的链路中来那么我们首先来看一下传统的数据是如何进行组织的首先我们来看用户的行为我们认为用户的行为其实是可以通过人或厂来进行描述的人比如说用户的年龄性别这些都是用来描述人的或我们就在电商场景上认为是商品而厂我们通常是指人的这些行为所发生的场景比如说访问应用的手评比如在搜索框里搜索了什么样的商品而有了大语言模型的加持我们可以组织更加丰富的方式来表达和描述这样一个行为比如用户的画像是什么在什么样的场景下产生了点击或是购买或是分享或是等等等等的行为第二类数据我们认为叫电商知识电商知识其中最主要的还是人和商品以及还有大量未被良好结构化的数据比如说我们经常能够看到评论区里的各种各样的观点有的是好评有的是差评或者是还有大量没有办法去分辨其好坏的评论并且向用户随手拍摄的图片并不是一个指带非常明确含义的内容以及当时所在的活动场景是手机购物节还是加装购物节传统的LP任务往往没有办法很好地去理解这些问题而在大模型在通识理解的基础上我们再将这些电商知识就可以做到很好的融入刚刚讲的数据那么我们来看一下这些数据背后我们如何将电商场景的数据和模型有机的融合传统的CTR模型CVR模型这一类稀疏的模型我们往往是拿数字化的ID进行表征比如在电商场景里的一个商品的ID叫做0003但是这个0003背后的含义却是缺失的比如0003本身是一个白色的铸铁材质的某一个品牌什么样的型号的一个哑铃而这些具体的描述信息是没有的虽然这个0003这个ID具有非常强的区分能力但是背后的那些含义的表达能力却是偏弱的假如我们把它切换成文本类的数据它可以通过自然语言的方式来描述这个哑铃它具有很强的语异性但是它的区分度反而不高至于它是A品牌的哑铃还是B品牌的哑铃它至于是一个哑铃还是杠铃此时区分度的确是有些模糊的地域因此我们介于两者之间选择了一个叫语异ID的东西我们既希望它有良好的区分性具有ID化的这种天然优势另外我们也希望这些ID化的背后具有更加丰富的语异存在这里是整个语异ID的建模思路我们将整个商品进行的量化表示比如如图上的阶段一所表示我们将完整的商品表述通过encoder decoder的这种模型结构生成对应商品的item embedding再通过RQVAE等方法量化产生出相对应的商品语异ID接下来我们来看下面这个语异的ID如何来构造生成式模型的提示词通过自然语言来描述环境信息用户的历史行为用商品的语异ID来进行表证生成式的模型接收到这样的提示词信息之后来生成下一组的语异ID而通过语异ID去进一步去匹配用户所感兴趣的商品这里其实是我们一个编码表证的方案示意图我们假设一个商品的语异ID由四组数字构成通过encoder模块来计算item embedding然后在每一层的语异ID中寻找其最近铃然后通过计算其残差这一层的残差去下一层的语异ID中进行重复的操作最终我们形成了四层的语异ID在训练这个网络的时候再通过最近铃的embedding通过decoder去还原它原始商品的embedding有了刚刚所表达的编码和表征以后我们就面临如何将此类的信息与大语言模型更加有效的连接起来这里就涉及到高性能模型的选型以及其背后复杂的工程实现下面有请我的同事李健为大家来讲述工程实现的技术方案那么由我来为大家继续讲述我们如何在工程落地中具体的应用大语言模型的工程实现方案我们在上门完成了生成式算法的建模后就到了工业化落地的环节而在具体的落地实现中我们遇到的主要挑战其实是延时和吞吐无论是广告也好推荐也好但凡它是一个实时的场景它的推理耗时空间其实就只有百毫秒级别而整个生成式召回的推理过程包括用户序列特征的计算与提起与EID的计算生成式模型的推理以及最后生成的与EID再转回具体商品的这一系列的全过程都要在100毫秒的耗时的限制内完成的而第二个是吞吐有的时候呢其实我们可以通过横向扩展计算资源来压低整体的推理耗时比如像现在比较成熟的像Tensor并行序列并行等一花算法其实是可以直接来降低推理耗时的但在具体的商业化的落地场景我们就不得不面临的一个成本问题我们上的这个模型所需要的资源消耗是多少而这个资源就可以带来给我们带来多少实际的收益都是需要我们考虑的而吞吐其实就直接影响了这一指标我在具体的性能跟吞吐的优化方向上我们主要在两个维度上进行了一个优化一个是单节点的算力释放这一环节重点是我们来挖掘单卡单节点的性能上限而另一个维度则是分布式的软硬协同通过分布式的易钩硬件使模型推理过程中的不同环节可以最大化的分配合适的计算资源以此来提升整体的模型的吞吐我们目前整个的推理过程其实是基于Tensor RTLLM进行优化建设的那么我们就先介绍一下我们在Tensor RTLLM上的一些最佳的实践过程首先在吞吐方面这里先介绍一下目前影响吞吐的一个最大的因素我们通常可以这么认为在未达到GPU的计算资源上限的条件下我们使每次单次推理运行中所计算的token总数越高让它实际的得到的吞吐就可以越大而在在线推理的一些传统的batch方案中这个batch策略其实主要的目的就是尽可能的通过聚合多个请求的方式把每次模型推理的计算量来提升以此来获得更大的吞吐而在生成式模型中单次推理所涉及的token总数其实是由两部分组成的一个是context token这里一般其实就是指输入提示词的序列token长度这个值其实一般是比较大的因此一个batch的推理过程如果包含了新的请求这一般会得到一个较大的运行时token数也就直接可以获得更大的吞吐而第二部分则是生成中的token数通常呢一般是指batch内已经处在生成阶段处理的请求它的每一个请求的处理长度一般是1因此我们想要获得更大的吞吐我们需要打更大的batch来得到更大的一个运行时token数而这两部分它会直接受TENZRT LLM的两个推理参数影响一个是maxbatchsize另一个是maxnumbertokensmaxbatchsizesize这个参数是指单次推理所能处理的最大batch数而是如果这个数值是过小每次调度的batch数就小一旦生成中的请求数量达到最大的batch数就无法再次调度新的请求导致运行中的token提升不上去最终影响我们的整体吞吐而当batch数过大时有可能产生一直在调度新的请求这么一个现象而新请求较长的提示词长度也会很快把险存容量消耗完间接的影响了我们整体的吞吐因此这里一般建议我们可以根据实际的kvcatch承载的能力的最大值来匹配最佳的batchsize而同时为了最大化矩阵乘法的运算效率我们尽量在这里来匹配成32的倍数另外一个参数是maxnumbertokens这个是指每次推理所能处理的最大运行时token数而这个其实是更直接影响了我们的吞吐与batchsize一样这个数字如果过小其实是直接限制了吞吐的而过大也会额外的占用显存从而降低整体的一个吞吐量而这里一般可以直接通过而这个maxnumbertokens这个值其实就没有一个很可以直观的计算公式这个值一般只要合理的配置就能达到一个很高的性能这里有一份数据给出了maxbatchsize和maxnumbertokens不同配置下所获得的一个吞吐量可以看到我们根据kvcatch的容量公式可以直接计算出它的一个最佳的batchsize是133而在对32进行向上取整得到最终的batchsize是160那么我们可以看到表中的batchsize在160的情况下一个合适的maxnumbertokens是可以得到一个最优的一个吞吐数值了那么上面我们既然提到了我们需要根据kvcatch容量来设置最佳的batchsize而一个模型所能利用的kvcatch容量其实是我们刨就了模型运行时必要的显存占用以外所剩余的显存容量那么我们看一下模型运行中的一个显存构成情况除了一般的很直观的像模型参数激活值kvcatch这些我们经常关注的显存占用以外还有一个容易被忽略的显存占用情况就是token采样解码decoder过程中所需要的显存token采样解码就是我们通过生成式模型推理出token的磁表概率空间后基于某些采样策略从中选出最合适的nexttoken过程当我们的场景需要生成式模型一次性的推理出多组候选时就需要beam search这种采样策略比如在我们生成式召回场景为了给用户生成尽可能多的候选集通常需要比向对话场景返回更多的候选集用户供用户选择因此推理参数中的beam宽度是我们需要设置一个很大的值的而采样阶段的显存需求则直接受到了生成的序列长度Batch Size 磁表空间大小以及beam宽度的影响这里拿了一组参数作为对比当我们的Batch Size是32的时候beam宽度是64所需要的解码显存占用是接近4个GB的比我们某些模型经过量化后的容量还要大因此在我们设置一些推理参数的时候也需要结合我们具体的业务场景在吞吐效果延时中取到一个折中的取舍效果找到一个折中的取舍的数值另外一个影响吞吐性能的环节则是超长序列的输入请求一般的用户请求输入长度不一其实是我们业务的一个普遍的情况有的用户行为非常丰富它的输入序列的长度就比较长而有的用户平时操作的比较少它的序列就会比较少叠加这些影响那么长序列的单条Token较长就消耗了Max Number Token的资源导致了Batch Size最终不能打得特别高影响了我们的吞吐此外长序列的请求与短序列的请求一起Batch到一起在进行推理时呢也会影响其他请求的延时这个上面这张图呢就举了一个例子在新求新的请求来临的时候我们经过Batch的调度策略将请求A B C D E打成一个Batch进行模型推理其中AB是新请求而C D E呢是已经处于生成阶段的请求在实际的推理过程中请求A与请求B需要先经过Contest阶段或者一般叫Prefill阶段来计算请求AB的KVCatch之后呢对请求C D E进行生成阶段的计算这步推理后请求C达到了序列生成的终点生成完毕需要进行响应返回而科目端C得到响应结果然后模型再进行下一步的推理迭代我们这个时候可以看到请求C的响应呢因为请求AB的加入导致它需要等待AB处理完之后才能进行响应延长了它的请求处理耗时而如果请求A它是一个非常长的超长序列则更会大大延长请求C的实验这在广告系统这种TP99延后时延苛要求的业务场景是不可接受的而TenCent RTLLM它提供了一种推理能力可以缓解这种现象这个功能叫Trunked Context开启之后它会将请求Context进行切片每次生成式推理中只推理一部分分片这里如厦门的这个右图所致它会提前将请求A进行切片切成了两片第一次推理来计算请求A的第一个分片的KVC这样子的话它使得每次推理的Context Token就比较少请求C呢也可以尽快地完成计算并响应客户吨通过这样的功能呢我们既减少了每次请求每次推理所占入的Token数从而可以聚合更高的Bitch来获得更大的吞吐此外呢每个请求呢还可以获得更平滑的这么一个响应时间在微调方面Tensor RT LLLM提供了多种微调模型推理方案这里就介绍一种常用的LARAL微调推理方案在Tensor RT LLLM中它可以在一个基座模型上面应用多个微调参数并且提供了微调参数并且提供了微调参数的一个缓存管理机制相同的LARAL任务它不需要额外的占用以及推理时拷贝同一份LARAL参数如这里的右图所示在我们构造推理请求出我们同时构造LARAL的TaskID以及具体的微调参数在推理引擎的内部会通过一个LLU缓存来匹配具体的LARALTaskID来优先使用缓存中的微调权重进行推理既提升了一个推理性能也降低了显存占用最后再提一点在我们通过引擎来构建微调推理模型时我们其实是建议手动配置MaxLARALRANK这个参数的因为这个值它在引擎中的默认配置是64而如果直接用64进行配置会额外占用很多的显存因为我们一般的微调参数的RANK值一般在4816这些小的值直接配置64会更大的占用显存所以这里是建议直接根据业务的实际情况来进行配置介绍完Tentart的最佳实践那么下面介绍一下我们基于广告场景的特性又做了哪些特性的优化这里先重新对广告场景模型推理特征分布的背景做一次简单的回顾第一是时效性强这里的时效性除了指延时以外还有指带模型的时效性这主要是为了让模型可以更快的捕获用户兴趣模型参数的更新周期通常是分钟级的第二呢为了获得更多的照回结果生成式的采用参数并宽度需求更高实际业务场景并宽度设置会是一个很大的数值第三用户的行为序列重复度高这里主要是指相同用户的多次请求这个用户的历史行为基本上是一致的每次仅仅增量只多了一部分前置的一些行为序列可以被大量的重复服用第四为了保证生成的商品属于广告集合我们的生成过程其实是需要一种受限生成的这么一种模式基于上桌的特点我们在搜索采样阶段就是闭幕色彻的过程做了较多的优化我们首先对采样搜索的范围进行了优化上文的算法背景章节我们其实提到了模型的提示词输入除了有商品的与ID外还有大量的自然语言来描述环境性计因此我们模型的token词表空间是包含自然语言的token的这个数量级一般在15万左右而模型生成的token的表征空间呢而模型生成的token的表征空间呢则需要约束在广告的与ID集合的范围内这个广告的与ID集合空间通常在千级别因此我们的优化思路就是将搜索空间从15万直接压缩到千级为此我们设计了两种压缩方案第一种方案是静态阶段我们在模型引擎构件导出时直接将模型的最后一层LMH层的表征空间加速到千级使得模型推理以及在采样阶段都在这个千级别的空间内进行搜索第二个方案则是动态阶段在模型推理时我们结合受限生成给出动态的表征集合来约束生成范围从而降低搜索空间通过这个方案呢我们可以动态的感知有效的广告语域范围并做出僵硬的推理性能优化此外由于生成的是语域ID它的长度是固定的而在整个的token减码过程中如果达到了生成的最大长度会提前进行路径导出我们将原路径导出的单线程计算模式优化成了多线程计算模式来提升导出的核函数计算速度而在模型时效性方面模型更新呢则是广告模型服务能力必备的一个能力而传统的模型更新方案它为了保证模型更新时的正常服务能力通常采用双buffer的模式进行更新需要在同一时间在显存中保留两份模型占用双倍显存因此呢我们设计了两种更新模式首先是针对大模型的更新较大的模型通常一个模型仅参数全中就占了显存的三分之一以上外加kvcatch等运行中的显存占用通常它无法采用双buffer更新因此针对大模型我们采用微调参数更新这带来了两个好处一是大模型的全参数微调训练成本高无法带来较高的实现性采用LARO微调可以提升模型的更新速度其次模型更新时只更新微调参数显存中保留多个LARO版本按更新顺序进行淘汰来解决显存问题而在小模型的更新场景中则采用全参数更新而kvcatch则通常是根据模型绑定的因此我们实现了kvcatch复用能力以及token的生命周期管理在模型更新时只更新模型参数而对kvcatch的显存容量进行复用上面主要介绍了我们单节点的一些优化方案下面结合一下异构硬件的分布式推理架构来提升整体吞吐异构硬件分布式推理它产生的背景其实主要是为了解决由于模型计算资源需求不同而导致的资源充足不匹配的问题这里的需求其实是指向IO需求GPU密集计算的需求CPU逻辑计算的需求等像传统的CTR模型中存在大量的特征计算以白定查询模型中它同时存在IO密集型的CPU密集型的以及GPU密集型的计算而最终导致了资源的错位争抢造成系统的整体利用于低因此我们设计了分布式分图异构框架协同了CPU与GPU的算力充分发挥硬件优势使硬件可以最优化的适配模型计算过程如图这里的图示所示原本由于CPU瓶颈导致GPU利用力低的问题我们经过这样的异构拆图后使得GPU计算能力得到较大的释放最后是KVCATCH POOKVMEMORI POO这一方案呢它主要产出的一个产出之后主要为了解决就是用户行为序列复用的问题这里首先介绍一下序列复用的背景我们在原始的生成式计算过程中用户在T时刻对广告系统发起的请求而广告系统在T时刻它保留着用户的T-1时刻的历史行为信息那么广告模型的系统内部则根据用户历史行为信息构建提示词并进行推理计算出T-1时刻的KV值而基于T-1时刻的KV来生成下一组的广告商品来展示给用户而用户则在T时刻对其中的某个广告发生了新的行为比如点击或者下单而在下一次T加1时刻呢我们同样的用户再次请求时则会重复上述的过程那么我们发现同样的用户T-1时刻的KV其实是与T加1时刻的KV前面大部分的行为其实是完全一样的而每次对行为全集进行计算其实是有大量的资源浪费的因此呢我们可以针对这一情况设计出针对user pin的KV memory缓存池来优化这一计算过程那么优化后的过程是在建架图这个流程T时刻的用户请求广告系统广告系统在T时刻呢它保留着用户T-1时刻的历史行为以及对应的KV catch推理服务则直接从KV memory pool查出用户历史的KV catch从而可以直接进入生成阶段来生成商品进行展示而用户T时刻产生的新行为对应的KV值则可以通过增量更新的方式更新至我们的KV memory pool这个方案可以让我们的在线推理计算的需求大幅度下降可以支撑我们上线更复杂的模型输入更长的行为序列而为了解决这个方案而为了实现这个方案呢我们其实需要解决很多问题其中最重要的两个一个是存储每一个用户的KV catch数量数据量一般在兆级别而用户又是E级别的其次是延时虽然KV catch不再重新计算了但整个KV的查找拉取的带宽延时其实也是不可忽略的而如何保障毫秒级的毫秒级的延时响应也是我们需要关注的而为了解决存储问题我们目前的系统采用了三级存储与分布式存储的方式我们的每台物理的物理的推理节点只缓存部分的UserPint的KV数据而通过上游的模型推理调动平台的一些推理调度策略采用基于UserPint的一致性哈希负载均衡使UserPint固定的请求到部分的推理集群中命中相应的UserPint对应的KV值而每个推理节点内部则采用了SSDSRAM以及HBM这样的三级缓存HBM与SRAM则采用了Railo陶杰策略SSD则进行了持久化存储而在存储方案上这里采用了类似PagedTension的PagedBlock进行存储方案每次用户的行为进行增量更新时将用户的增量KV以及生成的KVindex存入我们的存储系统中从而实现一个UserPintKV的一个增量更新能力在延时方面呢我们为了抹平KV数据拉取的一个延时这里采用了并行异步加载的方案我们在用户对用户进行行为序列特征计算量的同时对推理节点发起异步的KV加载请求KV数据的拉取与行为特征的计算进行并行来抹去KV的传输时间最终整体的推理时间线大概如下图这个样子特征计算与KV加载并行之后进行增量推理生成式的结果返回后同时异步的对KV增量来进行更新而以上就是我们实际的工程落地方案最后做一下总结与展望至此呢我们其实可以看到生成式技术在传统的算法领域空间发展巨大传统的领域发展空间巨大但是生成式的工程但是生成式技术也是不能直接套用的像典型的电商场景呢则需要通过一定的改造适配才能带来实际的业务提升同时呢结合具体的业务场景需要算法与工程协同设计才能达到大语言模型工业化落地的这么一个成果未来呢我们有有望可以在多方面的继续提升生成式的效果其中基于电商的垂直领域知识的电商理解基座模型以及端到端的向生成式召回排序一体的生成式解决方案则是我们未来需要探索的一个目标好最后感谢大家谢谢谢谢