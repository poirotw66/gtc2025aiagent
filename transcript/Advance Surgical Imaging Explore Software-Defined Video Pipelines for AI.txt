 Hi, my name is Bernard Furst and I am here today to talk about software-defined video pipelines, and especially in surgical applications. During the chat, my team is going to be with you. So, Juri, Daniel are from Kallstolz. I'm going to answer any technical questions about our product or our vision, and Endoid and Patlum, or our software engineers that can answer anything that you have or any questions you have related to Holoscan or what we have been doing. So, let me start with what is Kallstolz. Kallstolz is a medical device company. For the last 80 years, we have been building medical devices to support surgeons and drive innovation into the ORs. In a lot of countries, we're leading in the cameras that are used during interventions. So, small cameras that go inside the patient's body during the surgery and provide really excellent images to the surgeons during these procedures. The surgeons can see bleedings, ischemia, so when tissue is not quite right, perfused and other difficulties in order to treat them. And the imaging is always done in real time so that the surgeon, while being at the patient's side or in the surgical OR, can immediately treat these. So, we have been using traditional hardware-based imaging pipelines. But the big question is, why do we believe that these may not be enough in the future? Well, and there are certain points there that we encounter that we're going to be discussing during today's presentation. So, first of all, hardware-defined pipelines usually have a fixed function, and that limits, of course, the scalability and also the flexibility. But also, we want to provide more AI capabilities to help the surgeon make decisions and guide them. This ultimately, of course, saves patient lives and improves the overall treatment. And the big other thing is that hardware cycles are very slow. So, building hardware to medical device standards is a very tedious and very difficult job. And adding new capabilities comes usually with new hardware in these hardware-based imaging pipelines. And so, we're looking into different image signal processing pipelines. For example, as I mentioned, we have these hardware-based pipelines. In this case, there are field programmable gate arrays, so FPGAs, that process the raw data into image data. And when I say raw data, I'm talking about the measurements that the chip actually does. So, these measurements correlate to some wavelengths or something else. And then these measurements are converted into images the way that we know them, so basically RGB images. And FPGAs are really good at doing that. They're very fast and they're very high-performant. Doing a 4K image in real-time is not a big problem for an FPGA. But of course, if I want to adopt that, I have different camera chips, different AI algorithms, well, then I soon come into some struggles. So, there are some limitations there in the hardware-based platforms. There is a hybrid where the FPGA kind of talks with the GPU and pushes data from the pipeline to the GPU to allow some processing. Whereas the main time-critical performance is still done on the FPGA. This still involves a lot of hardware dependencies and very detailed architectures of how is the data pushed in real-time from an FPGA into GPU memory and then back. This is not off-the-shelf hardware. This is a very complicated task, especially because you're talking about multiple 4K video streams at 60 frames per second with 12 bits per pixel. And that in a couple of milliseconds back and forth. It's pretty much cutting edge. Of course, you can imagine if we would try to do that, it would also be really expensive. So, the other option is, well, let's do the entire software-defined image signal processing. That means we take the raw data from the chip, the measurements that each pixel actually does, and we push that directly into the GPU as fast and as efficient as we can without any processing. And this would be a software-defined ISP or software ISP. Usually, the challenge here is to do all of this in real-time and couple that with AI and UI components that are overlaid onto the video feed. We're looking into an image processing pipeline. I just want to take another step back here to explain what do we have to do here and why is this actually so complicated. Well, first of all, we get the raw readings from the chip. We retime that. Retiming means we synchronize. We make sure that we know how long the chip has been reading data. We know that it's correlated with the light source and all of those really detailed hard clocking events are happening. After that, we correct for black level. We correct for non-uniformity. That means the lens distortion has to be removed so that straight lines become straight in the image, else they would be curved and warped. Of course, that's bad for surgical procedures. Then we correct the minimum gain and again the black level. After that, the processing keeps going on. There are a lot of secret sauce filters, which is why our images are so good. There is a diffusion also that's also very proprietary. The whole message here is there is some very complex math going on here to get the raw image to represent what the surgeon is actually interested in. We then remove back dead pixels. Dead pixels always happen. You have to imagine that these cameras are used a couple hundred times and every time they're sterilized. And by sterilization, I mean they usually go into an oven and are baked. That's not very good for chips. And to remove any possibility of dead pixels, we have a bad pixel replacement because single individual dead pixels or white pixels or black pixels would irritate the surgeon. And the next thing, of course, we do more secret sauce for white level to make sure that it is somewhat true to white as white, yellow as yellow and so on. And now we can come into the big demosaking step. Demosaking is basically taking the raw chip data. So really every pixel corresponds to some wavelengths and make an RGB image out of it. So we take multiple pixels from the chip that correspond then to one RGB triple. And we do that for the entire chip in real time across the entire, let's say, 4K image or 8K image or 4 times 4K image. Surgery, everything is possible. We then do white balancing, chromo suppression and then color correction. Color correction is really important because usually the raw data is very green heavy. There's a lot to that because the color, the light source is green, but also we have more green pixels than other pixels. But what the surgeon is interested in is purples, reds, blues, because that's exactly what signals good and healthy tissue versus a cancerous tissue or badly profused tissue. And then we go into more secret sauce filters. These are have to do with like overexposing some certain aspects of tissue so that the surgeon can see these easier and finally goes to the monitor. At the same time, an FPGA also does entirely second video pipeline at a smaller resolution, but we need to do exposure control. So the one nuance in surgery is that the light source, so the only light source inside the patient comes from the same perspective as the camera. And basically you have a little flashlight that is flashing inside the patient. And when you come close to something, it is very easily overexposed. And if you go away, it's very underexposed. So we have to do exposure control in real time because every fraction of a second counts. The surgeon moves closer to a tissue and it would take a second to adjust. That's really bad. So this is also done in real time. And you can really take one of our scopes and you can move it from, let's say, 20 centimeters down to like five and back. And you wouldn't see that it's really adjusting the exposure, the entirety of the stretch. So this is all happening in real time now. It's usually done on FPJs. And the big question is, how do we move it? How do we, what are the challenges and what do we think we need to overcome? Well, at the moment, these pipelines are highly customized. They're inflexible. I mean, I'm over-exaggerating here because of course there is a certain amount of flexibility in these pipelines. But overall, if we bring in new cameras that let's say have more chips or higher resolutions or other kinds of imaging sources, it's really difficult to adapt to these. And I mentioned four times 4K. That's a real example. Cameras in surgeries today already have sometimes four chips that are each 4K. And we have to process that in real time. I can guarantee the next generation will have 8K chips. And our hardware is already deployed in hospitals. So if we wanted to upgrade to more capabilities and more resolution and more spectral measurements per pixel, it's going to be really difficult to do that in hardware without going in and replacing thousands of devices across the world. So we also have an issue with AI. So where do we do AI or where do we do the inference for AI models? At the moment, it's usually done at the end of the pipeline. So everything is finished. The surgeon sees it. That with the surgeon sees goes off to an AI computer. The AI computer does something and sends it back on maybe the same monitor, maybe a picture in picture or so on. It's not very optimal. It's not happening at the same time. It's not leveraging the whole pipeline, the whole architecture. It's basically like an afterthought. And to avoid that or to overcome that, we have been looking into what are software defined imaging pipelines. The big challenge is shifting all the critical image processing tasks from the hardware to the software. And this includes all the magic sauce filtering or secret sauce filtering, all the correlations, all the demosec in the reconstruction of the actual RGB image. Like I mentioned, all of that has to happen in real time, as efficient as possible without ever compromising having frozen frames or anything like that. We also want to be able to or we believe that software defined imaging pipelines can use AI on raw data or an RGB data. So it's kind of like interwoven in this whole imaging pipeline instead of an afterthought. And then we believe that we can streamline the development of new devices with the software defined imaging pipeline and using adaptable software solutions. For example, to go to diverse camera types or video sources or new chips or new ways of imaging and being able to really easily integrate this into a product and deploy it. Because ultimately that saves patient lives. The surgeon can see it. The surgeon can treat it. The surgeon now sees it and sees more than they would do with their own eyes. Well, that is really what we are trying to drive to. And we believe that will help the patients. Well, what is what is the big problem? We don't know if we can do this real time processing for very dynamical surgical scenes and environments. And this is why what we have been looking in and trying to understand how we're going to be doing this or how this can be done. And of course, we're leveraging the IGX and Holoskin for this. Why are we doing IGX and IGX Orin? Well, because we believe it's very, a very robust setup and it can allow for AI accelerated features and running AI in real time. The other really interesting thing with IGX is the way that the safety MCU and the board management are integrated into the same system, as well as the Connect X7 chip, which has this high, really, really high data throughput streams. And altogether, we believe that this is a really interesting platform to do these things. Why? Well, from one aspect, we have to be very proactive in our safety approach. So we have to be able to monitor if the system will fail or if something will happen to the image. And with the BMC and the SMCU on the IGX, we have a second independent system that is monitoring our system. So we run solutions on the IGX Orin side and on the BMC and SMCU side of the system. We can monitor the performance and whether the system is still operational. And we can also report to other entities in the network whether everything is good. The system also, like I mentioned, has this Connect X7 chip. A 4K uncompressed video stream needs 12 gigabits per second. Most computers have a 1 gigabit or maybe a 10 gigabit connection. The IGX Orin just has 200 bit connection, 100 gigabits connection through the Connect X7 chip. And even more interesting, what I mentioned before, is we need to push that data into the GPU memory. And the setup also allows that to push directly from network streams into the GPU memory. That's, of course, essential to build real-time imaging platforms. For the software, we're looking into a Holoscan SDK. It integrates seamlessly. It's all really nicely packaged and there are good examples there. But, of course, here, the third bullet point, we need custom operators. We need custom tasks to process our very specific imaging data. We need to unpack this raw data. We have to apply our secret sauce filters to make this image really good. And operators are usually optimized to take full advantage of the GPU resource. So we're hoping that this is going to be fast enough to do this. So where are we today? You see up here, for example, one of our cameras. It's a 4K camera. It has two chips inside, one for near-infrared imaging and one for normal white light RGB imaging. So this is already two 4K streams that come in. And we're looking into systems like that and collect the raw data directly from the chips, unpack them, pre-process in terms of, you know, crop them down to make them usable, and then doing a flexible manipulation of the data and the image formats to get a really good image out of it. And you see those little asterisks here? Cameras and chips deliver very interesting data. They're not always standardized. They have different number of bits per pixel. Pixels don't necessarily represent R or G or B, but some correlation of it. The resolutions can be all over the place. The frequencies and update rates are different. So we really have to deal with a lot of hardware-specific nuances here. But the way that Holoscan allows you to do that is to, for example, deploy a very custom CUDA kernel to mitigate these things. I'll get to that in just a moment. Of course, then real-time processing tasks. It depends on who you ask. For imaging, real-time means a couple of milliseconds. It doesn't have to be better than that. And in that time, we can do demosicking. So going from the raw chip readings to RGB. We can do format conversions. We can do advanced imaging and so on. This pipeline also allows you to just drop in AI models and share data. Share data at every stage of this pipeline. Remember in one of my earlier slides, you had the image processing pipeline and there were some connections to GPU in the hybrid. Well, you would have to define them and you would have to build them in hardware. But now, because everything is software, the new AI algorithm comes along and needs specific data in the middle of the processing. Maybe before we do black level subtraction or before we do demosicking, we can now just pick out that video stream or that data stream and run AI algorithms on that. Common algorithms that are deployed now already are anonymization of the patient data. So when the surgeon pulls the camera out of the patient, you could in theory see information. So we have to detect that and remove that from the video in real time. We have to detect anomalies. We have to detect anomalies. So bleedings, clips, things that don't belong where they are. Those are really interesting aspects that have to happen in real time. And of course, the whole procedure is performed by the surgeon with long instruments inside the patient and moving around. So it's really interesting to see what is the surgeon touching, how is he interacting. So if we can integrate that all really easily without additional hardware and without additional need for another box, that is actually really interesting. To render everything, we can use HoloVis and visualize that finally. Let me go on to say, well, what does real time mean? We usually define it as light to light, so photon to photon. So if you can imagine you flash something in the camera and then you see that flash on the monitor. So that flash to flash should be under 100 milliseconds. Kyle Strauss systems today perform at about 60 milliseconds. So that's really good. Most users don't realize that there is a latency. So above 100, 120, it's still okay. Let's say 180, 200, most users start suspecting that there is a latency. Everything above 200, you know that there is latency. You can see it because you're moving your, basically you're moving your hands in front of the camera and you can see that there is something off. The more there is off, the more you have to concentrate to move correctly and the more mistakes can happen, right? So you want to be really low. You want to be under 100 to allow the surgeon to not worry about this, to not think about this. And in current systems, like I said, we're at 60 milliseconds, but half are for the camera and monitor and half of her, all the filters and processing. Because FPG is really good in processing. They're really good in doing filters across entire image frames in very, very low latencies without needing to load the whole frame or be buffering a frame. So where are we, if we look at, remember, so here we're saying half for processing. If we now want to build a very simplistic, very easy pipeline without a lot of secret sauce in Holoscan. Well, we know that we're at, at about 30 milliseconds already. So this is really promising because this means that if we wanted to go to a software defined pipeline off the shelf components would allow you to do this in a reasonable latency. Now, of course, this doesn't take into account all the data coming from the chip needing to be reformatted, needing to be pushed into IGX and then the monitor out. So there you probably still have more than current systems, but overall we're in the same ballpark. And this is really exciting. A couple of years ago, we were not in the same ballpark. We were factor 10 slower in software than in hardware. You also see that with these, for example, this tool here, which is the insight, which really helps you benchmark everything. You can benchmark it down to every operator. So you had to blur out a couple here, but you see that the standard operators are all there and how many milliseconds they need. And this is all done on a 4K image. So it is reasonable to assume that in the next couple of years, we're going to be fast enough to do most image processing and software. And of course, these tools from NVIDIA really help us to develop it and make the development a lot easier. So, you know, I'm not the coding expert. So we got two engineers in the chat to answer questions about coding. I just wanted to walk you through how we think about this. One side and on the left here, you can set up your pipeline. It's very easy to basically concatenate your operators the same way this block diagram did. Of course, you're going to pay in latency if you do that. But then afterwards, once everything works and once you're in the right ballpark, then you can go in and say, which operators can I combine? Which mathematical operations can I combine into one? Not everything is able to be combined, but some of these things can definitely be merged and be even made more efficient. And then on the right side here, you see that we have one custom operator. And I just wanted to walk you through one of the custom operators and what were our learnings to do that. Well, this is a custom operator and I think it does black light or black subtraction. So basically, if you get the data from the chip, the chip will always give you some values, even if there is no light. And if you display that, then it's the image looks gray and washed out. See, that's bad. So we want to you want to have to remove that level. And you have to do that basically over the entirety of the image because you can't just like subtract everything because then you have a black image. And then you have to understand what is the average blackness of the image and then remove that. So in this case, we what's not on the screen is we have we have our own input operator that unpacks and buffers our raw data. But then it can then it actually starts being interesting. So here we see that the raw data comes in. We have a custom code operator kernel that converts the raw input data into the bare pattern format. So really taking the first step of abstracting the hardware out of this and making it useful for the next steps. We do set up our NPP streams here. NPP are these NVIDIA performance primitives. They're really, really good and optimized to run on hardware. I'm going to get to that in just a moment. We then have to resynchronize our CUDA tasks. Sorry, actually, we just call it synchronized CUDA, but it ensures that all previous CUDA kernels have finished and now the image can actually be processed. So this is just something, you know, these are good practices. We just wanted to step you through our key learnings here. We do use NPP, for example, for this black level subtraction because NPP is really excellent and very efficient in manipulating individual pixels in a spatial domain. So when it comes to frequency domain, which we have to do for some very secret sauce filters, it's not as efficient. It's not as efficient, but we do have to do some coding in order to get to the performance that an FPGA has. But at the same time, it's possible. And then finally, we can create an output of this operator. And this is again a tensor or a media buffer and holds the process data and it ensures that the formatting is correct. The formatting is correct and then it continues in the pipelines of downstream processing. So you see how simplistic this is. And basically any software engineer can understand this, learn this, and then really start building these custom operators. And this custom operator here does something very simple, but you could also say, well, I want to do, I want to take an AI model and stuff it in here. Sure, it's possible. It's not rocket science or you want to do some special filtering because you have a new special chip, you can put it in there. And suddenly it becomes all a software problem and the software development problem. Rather than I need FPGA engineers to re-engineer hardware and to deploy it and to ship hardware to different hospitals. So the key learnings here is really integrate. It's really easy to integrate CUDA kernels or NPP into Holoscan operators. We have a very fast and efficient development pipeline and we can do multi-step video processing. Maybe that's not optimal for latency, but it's very optimal for building the first pass and then going back and optimizing step by step. And it's very easy to manage the GPU buffers across these different operators. So to just wrap this up, I think it's good that we look at what are the pros and cons, what are the balances? And as I put here overly simplified, because I know a lot of people would be yelling at me if I don't put that disclaimer here, because this is not black and white. This is a very broad gray spectrum here. But in a nutshell, we basically can say FPGA is really good for optimized real time processing. The software defined pipeline here is good. It's not as fast and efficient as FPGA at this time. And there may be some jitter on the frames. What I mean with that is an FPGA will hard clock your frames and it will output every 16 point something milliseconds the next frame so that you get at 260 frames per second. Software may or may not do it. There will be some jitter on the output. The question here is for your application, is that critical or not? And I think that's very that's very customer needs specific. The flexibility here in FPGA, we just say, well, it's a little bit lower. You know, you can do a lot with firmware updates with new IP cores that you push to an FPGA. If your FPGA is large enough and not already full, full in terms of it doesn't have any capacity to do more processing. On the other hand, the flexibility on the software side is very high because I can just push a software update. And if I have to push a hardware update that comes under scalability and I'll get to that in just a moment. Latency FPG is low latency, pump out, they just do what they're here to do. Software may or may not have higher latency. It may also have jitter, as I mentioned before. But we're definitely going to be improving this by optimizing the software and the hardware. The scalability. Well, once your FPGA pipeline is set up, it's going to be doing that. If you want to do more, if you wanted to add AI, you will have to add something. So, whereas on the software side, I can scale to multiple sensors, to new devices. I can put it on different platforms like a low end or a high end platform. Let's say an AGX or something like the IGX with a dedicated GPU. You can scale there a little bit better. And the other interesting thing is if new requirements come out while your hardware is deployed, you can also just add on to it. Instead of replacing hardware, you can just say, well, okay, I'm going to put another box there. I'm going to fragment across my multiple systems. And then AI integration. Well, there is a certain difficulty on the FPGA side because it does need an additional computer. So you have basically an independent system doing the imaging, but you also have another system to do the AI on this video stream. Whereas in the software defined pipeline, I mean, that's obvious. It's already there. And we now have the new opportunity to do AI on raw data or an intermediate data. The development costs. I mean, again, super oversimplified development costs on FPGA side tend to be higher because you have to develop hardware. You have to spin the hardware boards multiple times. You have to ensure that it actually works. You have to validate the design of the hardware on the software side. You're going to do the same. You're going to validate your design. You're going to do verification validation of the performance and of the usability and the risks for everything. But at the same time, the development cost is probably going to be lower because you can also accelerate this and pump more features out at the same time. Then developer pool. And this is actually a really big topic because you're going to probably struggle to find really excellent FPGA engineers like the ones at College Dots. They are good. The use of FPGA comes with a lot of experience, whereas in the software defined side, there are a lot of software engineers that can come from other domains and move into your domain or in this case into a surgical domain to help build operators. It's a lot more general problem than very specific to how FPG work. The unit costs FPG are expensive, but compared to an IGX, they're probably still reasonably priced. Unless you then say I put an IGX in the end of the pipeline to do my AI. Well, then that just defeated the purpose of reducing the cost. One aspect here is you do need to ensure a continuous high performance and not too much jitter on your video stream and not freezing or lagging frames. You probably will have to over spec your computer in order to support a really strong and optimal software defined ISP for new sensors. I mentioned this a couple of times on the FPGA side is challenging, but not impossible on the software side. Most as soon as you have the data from the chip as a data stream as a network interface, for example, that comes in. You have your problem solved right there. Power efficiency, FPG and also these hybrid ISPs are much more efficient when it comes to fixed workflows. So if you're going to be operating on a battery or a handheld device, you're probably not going to try to smack a whole IGX on to that problem. So it's also dependent. There are multiple factors that make you decide which platform to use. Even in surgery, we have handheld devices that do image processing, for example, for intubation. But we also have these very high end systems that they use during surgery. These are different use cases and they demand different power consumptions and different solutions. And then so the hypothesis that we could kind of conclude to is trying to be very politically correct here because again, I know that there are a lot of fans of FPJs and software based ISPs. But I think this is kind of like a conclusion, at least I can live with the FPJ side. It's very robust and fast. It's a predictable cost for well-defined feature set, but it's somewhat limited for real time AI readiness. So fast and robust, we talked about that predictable cost for well-defined feature set. If I have to do X to Y, FPJ is going to be great. But if you start developing X to Y and while you're doing that, you have new features and new requirements, that's going to be a problem. So if you know exactly what you're going to be doing and how it's going to be doing and what the maximum is that your system has to perform at, well, then you're probably good with an FPJ. But if something like, well, maybe next year there's a new AI algorithm and I don't know how many resources that's going to take. Well, then maybe software is better because here we say fast and flexible development at lower development costs, but higher hardware costs. And here we can also deliver an increased feature richness because we can also be more dynamic in which features we deploy. By embracing software defined imaging pipelines, we believe we can unlock this new flexible, scalable and AI driven innovation in surgical domains or surgical systems. So we want to innovate faster. We want to enhance patient care. Ultimately, it's all about how can we support the surgeon to perform better, more efficiently, and more efficiently. And save more patient lives. I think that's a really important message. So it's not about driving technology in our case. It's actually to figure out which technology will help most patients in the most optimal way. And with that, I would like to conclude in the chat again. We have Yuri and Daniel from College Stolz to answer any questions. And we have any questions regarding our products or our thinking. And we have Endlet and Fatlum to answer any questions if you had any about my very brief excursion into Holoscan and how to do operators. Thank you. Thank you. Thank you.