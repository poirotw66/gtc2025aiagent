Heat. Heat. Good afternoon everyone. It's a pleasure to be here at Nvidia GTC conference and to present our work on advancing automotive factory planning at BMW with AI enabled digital twins. I'm Hashid Raiker and I'm joined by my colleagues Felix Doyer and Tobias Deago.
Hi. Hey there.
Together we will walk you through some of the exciting advancements we have made in the space. To start let's dive right in and watch a short video that provides insight into the virtual factory at BMW. This should provide some context for the discussion that follows. Let's take a look. Our world is rapidly changing with globalization, sustainability, and digitalization defining our time. To meet evolving market and customer needs, our development cycles become shorter and our system landscapes more complex. But what if we don't just keep up? What if we shape the future and build new digital worlds? Worlds that become reality? With the virtual factory of the BMW group, we bring together what belongs together. In this process, we unify our existing data streams and create an exact virtual 3D image of the production system. This serves as an entry point for global teams to validate and discuss their planning, which means collaboration in real time. time from anywhere in the world. Planning alternatives, space requirements, and danger zones can quickly be visualized and reviewed in the overall context of the virtual factory of the BMW group. With this, we contribute to a more sustainable and efficient planning process. Dive into the virtual factory of the BMW group. Collaborative, efficient, innovative.
Thank you, Harshett, for the introduction. In the video, You saw what the virtual factory means to us at BMW. Now I'm giving an overview about how we started with the virtual factory and how our partnership with Nvidia evolved over the years. In 2018, our colleague Ross Kranberger began using Unreal Engine for factory planning in Oxford. We still use Unreal Engine for specific use cases today. Two years later in 2020, the virtual factory group was established. In the same year, we took our first steps into Omniverse. At the GTC four years ago in 2021, Nvidia and BMW announced a strategic partnership to jointly develop a digital twin of our production system in Omniverse. In 2022, we had our plant in Hungary fully represented in Omniverse for the first time. To bring more users into the Omniverse, we transitioned from onremise hardware to Azure cloud in 2023. To further develop and maintain the system, we significantly increased our external development capacity through long-term DevOps partnerships in 2024. Finally, this year, we will continue with the development on Nvidia's cloud infrastructure. For the virtual factory, we see significant economic benefits through scaling the system. By increasing the number of users and the data available in the virtual factory, we gain network effects. As I mentioned in the previous slide, we started with the plant init hung From there we rolled out more and more plants into the virtual environment and applied specific use cases at each location. We are working on use cases such as der deriving AGV maps from our data running semi-automatic collision checks and using the virtual environment for human simulations to evaluate ergonomics. Those are just a few use cases among many others. Now in 2025 we are moving forward with scaling the previously implemented use cases by integrating them into our business processes. We utilize Omniverse for a variety of applications, including the creation of massive 3D models, training robots, and enhancing our planning processes. One of the key features we value about this software is the open USD data format, which offers high customizability to meet our specific demands. So, why did we decide on building up our virtual factory based on Omniverse? We were looking for a software and 3D data format which can cope with the complexity of our UK use cases. Our users, the factory planners, play a critical role in ensuring the production of our products, the vehicles. They are dedicated to continuously monitoring, optimizing, and deploying changes to our production facilities and processes. With 600 planners, each making an average of three changes per week across 33 factories, managing this vast amount ount of data and collaborative effort presents a significant challenge. So what do we achieve with Omniverse? Through fully through fully virtual factory planning and optimization, we leverage Omniverse and Open USD to create massive 3D models that visualize factories up to 7 million square ft and over a mile long. The multi-user collaboration capabilities allow planners connect in a global network enabling realtime collaboration. and faster optimization by increas by creating a cloud connected network of planners. Each BMW factory becomes a node in our shared global network of omniverse interconnected intelligence. And this is and this all is accessible through a web browser. This connectivity allows planners from around the world to quickly collaborate while working alongside AI guides to design and optimize every stage of production. This enhanced connectivity and collaboration lead to quicker feedback loops and improved iterations ultimately driving our production effic efficiency. Now I'm handing over to Harshett who will elaborate on what we use large language models for in combination with open USD and omniverse. Thank you Phelix for providing insight into evolution of virtual factory at BMW. Our virtual factory implementation at BMW centers around two key value propositions. First, the holistic file type. This is based on universal scene description with an underlying data integration pipeline which allows us to create a comprehensive digital representation of physical facilities. Second is customizability. We are leveraging the customizability of the platform through a suite of omnivous kit extensions specifically designed and implemented for BMW's unique needs. While these innovation bring many advantages, they also introduce undesired side effects. These challenges that we are tackling using LLM based intelligent persistent. Let's take a look at the first challenge. Let me walk you through our data integration pipeline architecture. At BMW, we deal with various source applications that are integral to different planning processes such as structure planning, process planning, logistic planning, human simulation and so on. Each of these application use its own 3D data formats such as Revit, JT, DAN, DGN and so on. Our data integration pipeline handles several critical tasks, establishing connectors to source systems, converting these diverse formats to a unified USD formats, managing localization, implementing rows and rights for proper access control, performing verification and validation of the converted data. Lastly, conducting thorough quality checks. The result is a series of USD files representing both static and dynamic virtual factories which has service area spanning more than 1 million square meters. To give a perspective, this is equivalent to 140 football fields. However, this immense scale introduces significant challenge. Our How would users effectively navigate in such an enormous virtual environment? I will now hand over to my colleagues Felix and Tobias. Could you kindly please demonstrate how we are addressing this navigation challenge with the help of AI?
Hey, so I have now the tool open. Can you quickly walk ush through what we see here?
Yeah, sure Felix. So this is our standard Omniverse interface where one factory more specific specifically the new planting debris and has already been preloaded
and the intelligent assistant is now directly integrated in this uh front end right
yeah exactly so the IIA is nothing different than all the other extensions the Omniverse is built off in this case we can access it from the top right corner we can see some general welcome information as well as some sample questions the user query then goes directly in that little bottom field
okay great then let's have a look on uh one specific specific pain point Hashet has introduced before the navigation and how the intelligent assistant could support here. I would like to look for um where the doors are going to be assembled. Exactly. So just type it in there. Okay. I asked a question. Okay. So now I got uh two responses um both in German as um the POIs were um created in German. Uh we have the Turinbo and the for Montage T. Um but I'm more interested in the Tanbo. So I assume I can directly jump there.
Exactly. Just click on the jump to button. And is this actually the station you have been looking for, Felix?
Yeah, absolutely. This is uh what I was looking for.
Okay, perfect. So, this was actually a pretty simple example. So, we just did that translation as you could see from doors to ton. Uh but now feel free to try something a little bit more difficult.
Okay, let's um let's see if we can find where the chassis is um assembled to the um uh to the uh body of the car. Okay, let's see how that goes. Okay, perfect. Yeah, it um identified the hawkite as the um as the uh point of interest. Uh this is exactly where the uh uh chassis is uh being assembled to the uh body of the car and yeah quite uh quite interesting that this works uh so well considering the LLM only has the information hide. Um so um yeah this um information seems so the LLM somehow seems to um get this corresponding um uh uh POI when I look for um body and chassis
that's exactly how it works. So let's jump back to the slide so we can dive deeper into that
right so as we saw what we actually did there is to really start from the pain points and needs of our users the factory planners in this case to navigate those huge factories and the intelligent assistance simply provides an additional approach to solve this complex task by using the semantic capabilities of an LLM we see two key benefits. On the one hand, we can use this semantic capabilities to detect the actual user intention behind the questions as for example has been seen with the body and chassis example to then find the marriage. On the other hand, the intelligent assistant also helps us to compensate for data inconsistencies. Of course, in an ideal world, there would be no such thing and we would have perfectly labeled data, multilingual support and so on. But uh we are far from that in reality and the intelligent assistant helps us to mitigate those issues by compensating for example spelling mistakes multilingual support and so on. So having a look at a very very simplified architecture it all starts with a user inserting their input prompt. This one gets then passed to the intelligent assistant which is based on a Python backbone. We also implemented a sophisticated guardrail to comply with our company internal regulations. We use a hybrid approach for this step which combines the LLM but also a filtering mechanism. This helps us to increase the meaningfulness of the results, increase accuracy but also to reduce token costs. For example, when we are in a specific building and have not loaded the entire plant, we only want intelligent assistant to search within that building. When the question is elaborated, it goes back and forth to the ser where the LLM is hosted. While the 3D data is directly pulled from the nucleus. Once an answer has been found, it will be provided back to the user via the output mask. As we saw previously, the jump to function is not done automatically but via button. We have done this way so that the user has full control where to jump to in the factory because sometimes more than one result is possible. When clicking on the jump to function in the 3D view, board, then relevant geometries get highlighted and the camera hovers to the correct spot. While the initial feedback from our users has been very positive already, what we originally wanted to do is to simply see if the intelligent assistant can be seamlessly integrated in the existing technology staving the way for future use cases, which then leads me back to Harshit. Um, so let's see what other use cases we have here today. Thanks. Thank you for that demo. Now let's address the second value proposition which I mentioned earlier, the customizability which is achieved through our suite of kit extensions built with within the omnivous platform. These kit extensions are developed based on the Nvidia SDKs, kit framework, kernels and kit foundation extensions which include rendering UI and asset management capabilities. Essentially everything in our implementation is an extension. Requirements from our internal customers have led us to develop numerous extensions, several dozen features in total, including live session capabilities, point cloud visualization, factory filter, and many others. This extensive customization, however, creates another challenge. We have essentially built tools that require specialized knowledge. How do we ensure that the every everyday users can effectively utilize these sophisticated tools. Let's look at another brief demonstration how we are mitigating this usability challenge using AI. Felix and Tobias over to you.
Okay, Felix uh we are back in our omniverse environment and I see you have already initialized a new conversation with intelligent assistant.
Yes. Uh we are here at the tilt for certain manual processes take place. I would now like to showcase how the intelligent assistant can help non-expert users to use our tools.
Sure. So again, just type your question in the input mask. In theory, there's no need to know exactly what you're looking for. But the more specific you are, the higher chances that it will return what you actually needed.
Okay, great. So on one of our projects, I need to share some data with one of our colleagues who needs to perform a human simulation which he uses to improve economics and optimize cycle time. However, I do not want to share the entire factory data set but just a relevant portion of it. So, I will ask the intelligent assistant um is is there a tool to cut out a portion of the factory. Let's see what it responds.
It will take some time to analyze your request but eventually if there exists such a feature it will let you know in the output mask.
Ah perfect. Yeah it uh started the cuboid cut feature and uh with this feature I can um uh specify or define um an area of the scene and cut it out as a separate USD file.
Yeah. Yeah. And as you can see there's also now the possibility to get into some additional Q&A with intelligent assistant if you need some further support while using the suggested feature.
Perfect. Then let's get back to our slides.
So yeah, the intelligent assistant serves as an additional method to make our historically expert tools more readily accessible. The key advantage we see here is to understand what functions the user actually needs. So we are slowly moving away from telling the software what to do, but rather telling what outcome we want. This not only helps us to increase acceptance of our tools, but also to significantly increase productivity and efficiency. As shown here, the very simplified architecture remains very similar to the previous use case. The user again uses the front end to insert the questions. Then we use a hybrid approach. This time, however, in combination with rack or retrieval augmented generation, a methodology frequently used nowadays in LLMs to to input outside information and facts. However, for this system to work, of course, the knowledge repository needs to be large enough so that the rack approach can be fed with sufficient data. You can imagine that there has been a lot of groundwork done over the past few years to now read the rewards. Finally, as depicted here, the intelligent assistant now directly interacts with the 3D viewport. For example, as we saw, by displaying the 3D cuboid cut function directly. So to conclude, building on the previous use cases, we wanted to show that we can have this tight integration with our actual omniverse functionalities and we are also able to integrate outside knowledge repositories. Thank you Felix and Tobias for those interesting demos. We hope we have provided valuable insights into how AI is enabling and enhancing our digital twin implementation at BMW by addressing the navigation complexities of large scale virtual environments and simplifying the user experience of our expert level tools. AI is proving to be an essential component of our digital transformation journey. These features what you saw are currently being rolled out to production with many more under development. With this we are at end of our presentation. We now would like to extend our gratitude to our partners NDIA, D systems and CDW whose support and collaboration have been instrumental in the success of this project. Also a special thank to our colleague Mara Gesler from the communication team. We are now open for questions. Thank you very much for your attention. Heat. Hey. Hey. Hey. Heat. Heat. Heat. Heat.