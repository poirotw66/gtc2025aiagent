 We'll be right back. Thank you. Good morning. My name is Jean-Baptiste Kempf and I'm the CTO of Scaleway. I've been working at Scaleway for two years and we've been deploying a lot of GPU clusters. Some people know me about the work I've been doing for 20 years in the open source community, but today we're here to talk about networking in separate clusters, especially on SpectrumX. So today we're going to speak a bit about what is Scaleway, who we are, why I'm here today to talk to you about all that, but also about what are the GPU superclusters, what are the technologies that we involve in Finiband, what is SpectrumX and all those related techniques and terminology so we're not completely lost during the presentation. Then we'll explain about how the technology works and how we deployed it and what were the challenges we faced and how we overcome all those type of challenges for the future. So who is Scaleway? Scaleway is a European cloud provider. We are a bit small, but we're on a very large growing size. We have a full service, right? All the AS, all PaaS, all the storage, all the networking, all the services, the managed services, everything you expect. But we've had a very strong focus on AI and we've been some of the providers in Europe who deployed the most NVIDIA GPUs. We deploy more than 5,000 GPUs and this is going to go a lot in the future. What we focus on is about European sovereignty, which means that everything we deploy is either open source or inner source developed in-house, right? So what our goal is, is to be able to have the complete stack of the ecosystem, of the cloud ecosystem in-house. And we've had a big focus on AI because we are part of a telco group who is focusing on AI a lot. And so we've deployed a lot of clusters for inference training, large clusters, thousands of GPUs, but also on the inference as a service, right? So really all the gap and also all the tool for data mining, data collection, data platform, and so on. We've deployed a few clusters with different types of technologies somewhere on InfiniBand, which is the most classic clusters we see, but we've deployed a few ones on Spectrumix. And this is what we're going to talk about today. You have also to know that we're also providing just normal H100 and other graphic cards directly as VMs and as containers. So let's talk a bit about the networking technology. A lot of the focus has been for the last two years, three years about training, right? And large foundation models, frontiers model, but also now about making some fine tuning, retraining, smaller models. And so we see that on the training side, there are a lot of different use cases, right? We, of course, have people who need thousands and dozens of thousands of GPUs, but we also see people now who are asking us some smaller clusters, right? We're talking about 64 GPUs, 128 GPUs, and they need a bit different than the large AI startups who need thousands of thousands of GPUs. And so here, it's quite difficult to know because how do we do smaller clusters and how do we adapt here on the clusters for different type of users who need some different types of workloads? And this has been a challenge. And especially as soon as you start to have multi-tenants on the same clutter, this is difficult to do with the classical InfiniBand. And this is why Spectromix is interesting. We'll talk about that soon. So we see a trend for smaller clusters for fine tuning and retraining. And we see a lot of rise of inference, but so far having one, two or four cards on one VM is enough. And we don't really need mega clusters just to do inference. Of course, this is not always the case because, of course, we've seen some models which are at 640 billions of parameters and they need maybe large multiple machines GPU clusters. And what we see also is that we have a lot of clients who want to be more integrated in our cloud ecosystem, right? They've been used about like all the IAMs, all the Terraform integration, all the Kubernetes integrations. And the old way of using Slurm on large InfiniBand clusters is a bit difficult for them. So this is basically what we need to address. And so let's go back to the basic. What is a GPU server cluster, right? Some of you might not know that. So of course, GPU clusters are a large number of machines which all of them have around six, four, six, eight GPUs on the same machine. But eight GPUs are not enough, right? So there we need to be able to interconnect them. And we need to interconnect them in the way that the training is not, is almost as fast as you were adding thousands of GPUs on one machine. Of course, this is not possible. So this is why everything is interconnected on extremely fast and extremely low latency networks. When we talk about GPU clusters, we are talking about several types of clusters of network. One which is a major one is called the East-West network, which is the one that is interconnected all the GPU together. It's an extremely fast network, extremely low latency, that is designed for the maximum throughput without redundancy. It's often tiered to a Cloud3 network. We'll talk about that later. The North-South network is basically what we call the front-end network. It's a network that connects all the machines to box outside of the cluster, inbound, with internet, the login node, the management nodes. It is usually fast, right? We are talking about running hundreds of gigs. But it's also mostly redundant. And of course, we have an out-of-band network, which is basically the network to connect to the PDUs, the BMCs, basically the management, which is a network that the speed doesn't really matter here. And finally, we have the storage network, which is another network and can be of different type of typology, right? And it's often the same and converge with the North-South network. But we're seeing things that are a bit different lately on that. But that's not really the topic of this presentation. So here, what we're going to talk about mostly is the East-West fabric. So the East-West fabric is the most important because this is the network that makes all the GPUs look like they're on the same machine. Well, they are not. You don't really need to understand all the schematics, right? But you have to see that the one, the schema that you see on your left, which is the classic infinibund one based on with 96 switches, is the standard topology for 2000 GPUs. And what you see is that all the nodes here are clustered into 32 type of nodes in what we call the SU. And then you have several SUs and all of them are connected to a spine, right? So that it's a type of hierarchy, right? We do SUs with the two GPUs, which have this very quick internet connect. And then we connect to the next SU with a spine. And this is usually what we do for 2000 GPUs. You have in Philibein 32 spines and all the SUs are connected there, right? So here we have eight SUs for 2000 GPUs. When we move to Ethernet, right? And Spectrum is a type of Ethernet. It looks mostly the same. Here the difference is that we are using 48 switches for 2000 GPUs because the SUs are larger. They usually have 64 nodes, right? So 512 GPUs classically. And so you need half of the spine. So this is able to grow a bit more than infiniband. Usually we can grow to, I think, 8000 GPUs with those types of things. And here what you see is that you have one layer for the spine and one layer for the leaves to connect all the server together. So you see a type of like two layers. But of course, one of the things that we can do is to add more layers and two or three layers to something that we call a close five fabric. And basically you will have a super spine level, right? Just not just SU at the spine level, but a super spine level. I don't think, and I don't think it's realistic today to have more than three layers. The schematic can be like either fat tree or rail optimized. The communication libraries need to be aware of how the structure is. And especially they are really aware to reduce the latency when they are connecting the nodes. In Finland, in Spectrum Mix, sorry, with the east-west fabric, so we could go beyond, right? And so this is the same schema, right? But we go above, right? And so here you see that basically there is some super spine at the top and you can connect them and you can go up to 524,000 GPUs, which is quite a lot, I think, but also quite a lot of money. So this is massive, right? Because you can have 64 group of 64 super spine and so you can have 4,000 spines. What do we require on those type of fabric? We need to have GPU memory to GPU memory connect, right? And this is RDMA, of course. And this needs to be the highest throughput possible, but with the lowest latency possible. The application, of course, doesn't understand what is happening behind the scene, right? For them, it's just memory between GPUs and they would address it like it's the same machine. Everything needs to be controlled and transfer fast on the fabric because the GPUs don't expect a higher latency. You need to have a jitter that is constant and very well controlled because else you're going to have some issues because the GPUs are not going to understand what's happening because the application is not aware of that. And of course, this is an ultra bursting network, right? You can go from 0 to 100 to 0 to 100. It depends so much on the application that is running on the GPUs that is very difficult to predict. And also, you can have that in like a few messages, right? You can have 0 to 100, 0 to 100. And that's going to happen all the time in all the direction between all the GPUs because like a NUMA machine, right? You need the GPUs to think that basically they're all together in the same machine, which is not the case. What you see is that there are a lot of flows that are very large flows between a few endpoints. And it is quite complex to basically hash around multiple links, right? Because you don't know exactly what's going to happen. And so you need, in order to have a very intelligent, a very good flow, you need to have some cleverness inside the fabric. So that's why it's very important to have a very good fabric. What is Spectromix? The idea of Spectromix is to have exactly all the good things that made Infinity Burns great, but do that inside FNH. So it's basically based on ROKI, right? So RDMA over common internet, but with an enhancement to have adaptive routing as well as contained tail latencies. So if I had to explain that to my top management, it's Infinity Bend over internet. Of course, that's not true, right? But this is the idea, right? So it's a full lossless architecture. There is adaptive routing, as I said, in the fabric. And it's a contained tail latency compared to traditional ethernet, right? Because you wouldn't connect all that with a classical ethernet. It uses the concept of flow routing in an internet fabric rather than packet by packet so that it can sustain the better performance on throughput and latency because each packet is part of a flow rather than just like a normal small packets, like a UDP packet. Ethernet does not guarantee the order of the packets. So it's routine one by one by the fabric, and the DPU is doing the heavy lifting to reorder the packets so that the application doesn't see not anything, right? It's type of packetization and unpackedization that is done by the DPU so that it looks like a normal RDMA transfer. The standard ethernet flow cannot not comprehend the message AI flows, especially because it's way too big, way too bursty, and needs a very closer jitter control. So using the classical algorithms like ECMP, you will have a lot of congestion in the link. And also, you're going to see uneven link usage, which means that you're going to saturate some links while some are not saturated. And what you're going to see is that once it's hashed, a flow will just stick to one path and do not do any reordering. This is very a big problem, right, on this output because you're not going to use fully all the links. And of course, the tail latency, you will get some of course link saturation and retransmission and so increase your latency, right? So you don't want to do that, right? And so that's why on InfiniBand, basically, you have link flow control that ensures that no packets are lost during the fabric. The link receivers grant the buffers, received by the VL, and all the credits receive a commitment to accept the packet. That means that no packet is sent on the fabric if the receiver cannot accept the packets. So once the packets enter the fabric, the fabric is responsible and ensures that the packets go to the destination. This is quite different from the classical ethmic. And all that is done inside all the fabric. It's a global view of the fabric that is sent and everything is on transit on the fabric. So no packet transit if the fabric cannot guarantee that it will reach its destination. And especially that it's going to do that in a lower latency case. So here is how SpectrumX work, right? This is an example of a SpectrumX fabric. It's, of course, reduced example representing only a few GPU boxes, right? So what you see is, of course, the four GPU boxes that are connected by four leaves and to four spines, right? The spines are in the middle, the four leaves are next to the spine in the middle center. And then you have the boxes on on the two sides, right? The boxes are sending the traffic on the wire, the leaves and the spin, and they are being dynamically rooted, the packets on all the applix to ensure that you have a consistent usage and distribution without having congestion, the classic congestion you have on the ethernet. Because it's ethernet and not infiband, the ordering is not preserved, but the blue field, the DPU, the networking cards are doing the head order, as we said. What you see here is that all the traffic for Rocky is going in this fabric. There is all the traffic for Rocky, as well as the control traffic being displayed. If there is congestion on the link, it will be paused and the traffic will be distributed unanimously over other links. And that same as DPU, it's if it's not open to, it's if it's not possible to ensure that the traffic is going on the fabric, it will be paused and we will wait for the congestion to be cleared in order to start again. The thing is, there is no perfect solution and there is no white glove perfect solution to deploy Spectromix. This is something that not many people have been doing. Of course, we are seeing a lot of people want to do Rocky and Spectromix, but today there is no simple solution. It's more complex to deploy, mostly because there is a lot of moving parts, different type of firmware versions and so on. Right? So we had to work with a partner to deploy some HGX boxes and do some of the deployment services. Right? Here we worked for the first cluster we did, we worked with HP. HP has been like a provider of Scaleway for a long time, so that's why we decided to work with them. And they have like a lot of experience in the HPC web. So the solution that we deployed is Spectromix and is like really future-proof, right? It's based on internet. So for us, it's very important. It's very cool because we're going to be able to reuse the standard Ethernet knowledge that we have. We're going to talk about that. But also, it's a better way to do multi-tenancy. We can control the clusters and modify the cluster as we wish, dynamically. And it's quite different to monitor those type of HGX Spectromix clusters than the classic DGX. And so we had to modify quite a bit of the tools that we use for the classical DGX Infiliban clusters. What we see also, and that's a bit of a problem, is that a lot of the customers were a bit afraid of using Spectromix compared to Infiliban, because they know about Infiliban. So this is something that needs to be addressed. So we solved by moving to Spectromix, we solved some of the issues, right? One of the things is that it's difficult to get good Infiliban engineers to correctly use and automate p-keys, right? And you need p-keys to run correctly and to slice currently an Infiliban cluster. Day-to-day is a bit complicated with Infiliban because it's very closely to the physical layer. And sometimes, we have dust that bring down some links, and it's difficult to diagnose. So it's really a knot, but that's also one of the drawbacks for people who are a bit smaller companies like ours. Using classical Ethernet is great, right? We can use all the classical existing Ethernet network, all our networking engineers understand what Rocky and VRF are. It's easy to understand how VLAN works. It's something, and it's easier to get some networking engineer up to speed when we use Spectromix. It is very easy to extend existing VRF to converge front-end for the storage. And it's easy to get other storage product to connect to all the fabrics because we just need to extend some VXLANs and VRF to the tenant front-end VRF. It's also possible to merge different clusters back-end using those same ideas. Almost all the engineers that we have understand Ethernet, so it's easier to bring and to train and to document that. And finally, it's easier to troubleshoot because Ethernet is done to be broken and fixed. And finally, it's pure standard Ethernet, right? So for cables and optics, it's also a bit cheaper and easier. So why I say all that? Not everything is rosy, right? It's been like the first cluster we moved to Spectromix. It took a bit of time, a bit more than we expected. But we believe this is one of the very interesting and good solutions for the future. So, because it's a very new architecture, the complexity with implementation was a bit higher than what we expected. The good thing is now we've done it once, although we can do that easily for the future. And also, some supply was a bit difficult, but most of this is now fixed. A lot of getting all the networking, bluefields, fmwres, switches, GPUs, HDX, and all that at the right drivers, fmware, and up to speed was also challenging, right? So to get NCCL to be able to fully run and fully utilize the GPUs and Spectromix took some time. Some customers had some issues because they were using some two old libraries that weren't able to use that correctly. Also, the converged content with GPUs created a lot of queries for RDMA enabled workloads. The impossibility to use MoFed and having to go Docker of Fed also created a few complexity for high-speed storage providers. Of course, all that is fixed, right? And this is not a problem if now you're going to deploy or buy a cluster from us. But it took a bit more time than we expected also. While everything was easier to deploy as SREs and network engineers, the supply part was a bit more challenging. And of course, what we deployed at the beginning, the area 1.1 was not perfectly stable, but that was fixed with and upgraded to 1.3. And finally, we had to move out of BCM because we needed some multi-tenancy and full integration into our ecosystem. And that was a bit difficult because we had to deploy to develop a ton of new toolings. And this is what we need to talk about is that today, no cluster manager is fully multi-tenant today. We need to be fully aware of all the hardware in the cluster to be able to do the deployment, to understand how it works. It needs to be VRF aware. It needs also to be a centralized solution. And for us, it needs to be able to scale depending on how many tenants we have on the cluster in order to get a larger cluster. And today for BCM, you need to have one BCM instance per tenant. And this is difficult to automate. And also, this is not really the type of architecture we want to have. But also, because we are a cloud provider, we need a custom solution so that it's correctly integrated into our billing, into our storage, the non-GPU storage, into our F3, into our IAM, user management, and so on, which in the older Infiliban-based BCM clusters, we had to do manually. And here, basically, we integrated all that. And the fact that we're using Spectromix helped us. With multi-tenancy, we need to have one very specific internal tenant for all our housekeeping, right? When we need to maintain, repair, test nodes. And this is what we consider as a REST tenant, right? And that's how we manage, basically, the fact that every upgrade, every position we need to do, we need to do that with our tenant that we call REST. Each tenant on this cluster needs its own VRF to ensure full isolation and no data leakage, of course, between tenants. And, of course, we do the same on the storage side. And so, for them, right, they just see a subset of the nodes. And for them, they feel alone in the cluster. Of course, they're not really, but because of how the VRF and the networking is set, basically, there is no risk of data leakage. The storage OSCO, as I said, needs to be segmented by tenant, but this is done depending on the storage solution. We've tested with DDN and VAST to be extreme direct, and we managed to do that. But so, our cluster management is doing the same, at the same time, the tenant multi-tenancy and the splitting of all the networking at the same time as a storage. And, of course, manages all the virtualization of the logins and outs. So, like, every tenant, whether they're big or small, whether they take 16 GPUs or 2,000 or 4,000, they feel like they have their own clusters and it works perfectly. So, on the external access, we use BGP, of course. But when we change the tenant, we need to do some operation on the BGP side so that it works correctly for them. So, we did a few hacks, right? So, in order to, basically, in order to debug mostly, what we've done is that we've had a schema for the IP address. And so, when you look at the IP address, you know exactly where is the location of the link and the card so that it's done systematically. And, of course, that goes to all our systematic DCIM solutions. And this also helps us to debug easily. And this works if we have half a million of GPUs, which is not going to happen soon, but we hope. So, Spectromix QoS, we use, of course, priority for control, PFC, and explicit congestion notification, ECN, to enable correct QoS on the fabric. For multi-tenancy, it seems to be a bit tweaked on the buffer pool allocation, the DSSP, and egress. So, that needs to be sure that we are fair for all the tenants, depending on the traffic that they have. The lossless fabric is the rocky control traffic on the fabric, and the lossless traffic is RGMA between the GPU. So, that's exactly the same as we do with InfiniBand with Priority Z Auto 7. There can be some cases where the bluefield increase Q gets stuck, but then we have the watchful that takes care about this issue if this arises. And, finally, the rocky flow control in Spectromix is very close to the flow control that we see in InfiniBand. So, we are sure that the flow of the traffic stays fast and safe. And, if we need to have retransmit, this is handled by the BF3. As I said before, the routing in the fabric is made with AI-type flow in mind, right? We have massing flow with few endpoints. So, it's complicated to use a standard ECMP flow hashing algorithm because they are not met for low flow counts with few endpoints. And, the result, as I said, is an even utilization of links and not very good congestion control. So, we need to dynamically route the flow to select the least congested ports and ensure the flow of the fabric between all the BF3 on each of the ends. So, the AI application expects, of course, the packets to be in order. So, the standard Ethernet routing and fabric coupled with Xon, Xoff, POS will resist non-ordered data on the receiver. So, that's why the GPU takes the case of doing the reordering of the data about doing all the heavy lifting of the fabric to be sure that the AI application doesn't see the issue. And so, that they see the normal seamless integration. Which means that the work on the DPU is key inside SpectrumX. So, here is how we create some tenants, right? So, this is like, as I said, the true multi-tenancy is achieving by using one VRF per tenant. It's very easy to automate and to use in a fully SDN network. And this is what we've done on Scaleway. It's very classical routing that we do. There is nothing specific here compared to what we do on normal machines. We just had to tweak a few buffers parameters to ensure multi-tenancy, as we discussed. So, we want to be sure that the RDMA traffic has the maximum bandwidth. So, for adaptive routing and get correct adaptive routing, we just need to be switched on on the spines related ports after the configuration that you can see. This is very simple, as you can see, because we just need to have a few comments. And there is nothing. It just feels like a normal extension for any networking engineer. So, it's easy to do. SpectrumX on the DPU requires, of course, reconfiguration and correct configuration. It's very important to use the latest firmware bundle. We had too many issues in the past. The North-South networking need also a bit of tweaking. But it mostly depends on your storage vendor to be sure that the RDMA is also working with your storage. And finally, we had some issues about the first integration with storage providers. And now, all of them are completely up to speed. So, be sure that you have all that at the right versions. So, now, finally, right? That was the main goal, right? Flexibility. Flexibility. So, it's very easy to slice and extend the clusters. We can do that mostly dynamically with our admin console, with click, click, click. We can... It's very easy to onboard new tenants. It takes a few minutes. It's easy to reconfigure. We can scale more. Like, maybe we could, like, add more GPUs on the same clusters if we have the space and the energy. And everything is and can be automated like we automate everything at Scaleway. And so, the big question, right? And this is important, is about the step time. And so, what we see is that step time is lower on Spectromix with 449 compared to 499. We also see a better networking utilization. And this is, like, this shows that you can have, like, extremely good performance also with Spectromix. And that the clients shouldn't worry too much about that. And this is going to keep being a challenge for a year or two where people just, like, know InfiniBand. And they just don't expect Spectromix to be as good. But it is. Thank you for your time. And if you have any questions, I'm easy to reach at Scaleway. Scaleway.com, a European cloud provider. Taste. This is going to be taki clear. Aqui. Salmonixx.com. оп ers. miljoommes.com. This is going to be seen in a Creek, os signs and j nurses. Of course, the product down a bit. perder. You too. Yawn.0 You can find some transparent Stopped inards You'll call éc siellä Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.