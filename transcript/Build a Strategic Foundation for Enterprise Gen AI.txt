 Thank you and welcome everyone. Is everyone having a great day at GTC so far? Excellent. Everyone loved the little robot at the end of the keynote. Always so fun. Jensen got to announce the new DGX Spark product. I've been at DGX, or sorry, I've been at NVIDIA for nine years. DGX is in my mind. Right before we launched the original DGX one at GTC right here and now the new Spark product came out. It's like, honey, I shrunk the supercomputer. So we're going to give you some examples today of what it takes to really build an AI infrastructure. I'm going to do that from the NVIDIA side. I've got great colleagues from the Mayo Clinic that are doing this real time every day. And so much has changed in the past nine years. And one of the big questions we always get here at NVIDIA is, how do I get started? What do I look out for? Or what's the best practice? And that's a big part of the DGX platform here at NVIDIA. The core value of what we do in DGX is not to be the world supplier of AI systems, but to build the best of every generation, to lead as an example. And for our customers that want that direct relationship with NVIDIA to run the exact same thing we run internally, that's what we build in DGX. So a lot's changed in the past 24 months. You know, everyone had the chat GPT moment, and then it was like, oh, this is great. Then, oh my God, they have all my data. That's bad. You know, so a lot of our customers started to want that same functionality, but they wanted it in their own data center. So how do I host large language models? And then at the end of last year, where we saw DeepSeq, everyone heard different things out of the DeepSeq announcement. You know, first of all, it's like, oh, it didn't take that much to train. Well, we can believe or not believe certain numbers on that. But the important thing, and Jensen talked a lot about it this morning, is reasoning models are good, very good for solving complex problems, but it takes a tremendous amount of compute to actually run those models in production. And as we think about what all of you might deploy in your own environments, whether it's for internal users, you know, using all your data for AI for those applications, or whether it's for external customer applications, I would imagine everyone's going to use a mix of AIs. What Jensen talked about on, you know, a standard language model, that pre-training, things that our customers have done, many of you have done, you know, throughout the years to train models, those models are very good at giving you facts, things that exist, basic math, you know, what are facts about the company? You don't need to reason about those, those are facts. You can get those really quickly. Reasoning model, you know, has to go through and look at lots of different options, collects a lot of data. In some cases, you know, in Jensen, you hit it on the keynote, a reasoning model may run overnight to get you an answer for something, using a ton of compute, looking at a lot of research to give you that one answer. Super powerful, but you can't use reasoning models for everything, and you can't use classic LLMs for everything. In the future, it's going to be a mix of things. Now, if I look at NVIDIA's journey on, you know, what we run internally, and you know, what we run internally is what we build. I have the pleasure of building DGX systems and then making sure all my engineering colleagues, all my software colleagues get access to those, which are some of, of course, my harshest critics, because, you know, they don't have to call customer support, they don't have to email me, they stop by my cube and they tell me what they like and what they don't like. We invested many years ago, way back in 2017, because we saw a phenomenon inside of our own company. We wanted to centralize a AI infrastructure for all of our users, but way back then, we had all of our solution architects, our developers, they all things under their desks. They all their own GPU accelerated little servers under their desk, and, you know, I've been in the data center market my entire career. I'm not going to say how many years because of this hair color is not natural. But when we tried to get people to move, we had the classic IT problem of nobody wants to give up what they have. And so Jensen being a very forward-thinking leader said, I'm going to put up something they can't possibly have underneath their desk. And that became our original internal supercomputer. We called it Saturn V. It was 125 DGXs. And within the first week, it was 100% utilized. We learned something out of that. So everyone's like, oh, wow, I can get all this compute capacity. And what ended up happening is the person that got in the earliest in the morning got most of the compute capacity. So I said, oh, we've got to do better. We've got to put software on this to manage the system better, to schedule things better, to give people better access to it. But the concept holds true today. And so many customers that I talk to in terms of, you know, when should I get started? The advice for that is get started now. Jensen talked about a roadmap. It's going to change every year. The good thing on all the NVIDIA products that you use, whether you buy DGX or one of our great partner systems, the NVIDIA software, CUDA, all of our libraries, completely backwards compatible. My colleagues from the Mayo Clinic talked to me about it ahead of time. They had one of the original DGX1s. The software that ran on DGX1 nine years ago runs on that Blackwell that you see on the floor today. So you don't have to wait to adopt newer technology. You know, adopt it when you're ready to run these things. Now, what's different and where our customers have really changed their outlook of how they build and deploy systems is you used to think about, I'm going to build a training system and I'm going to build an inference system. And for a while, NVIDIA actually had specific products in the market. Like these GPUs are really great for training. These other GPUs are really great for inference. But when you think about what's happening in AI, the market is changing so fast. The models are changing so fast. We switched strategies a few years ago to a universal GPU because this AI factory that you have should be able to do all of the things for you. And if you've been to the show floor and you saw our very beautiful DGX GB300 racks and similar racks from our partners, you know, that's a 3,000 pound rack level system. Has giant hoses in it for liquid cooling. Extremely efficient. And people look at that and go, oh, that's massive. That must be for training. But it actually does everything. More people are using that for inference in the future than for training. There's some show floor demos that we're running real time on a set of current generation GB200 systems. They're running about 11 miles away from here, 10, 11 miles away from here. I use that number because there are eight of them interconnected and there's 22 kilometers of optical cables just connecting those eight systems together, which is if you drove your car from here to our data center, it's about 22 kilometers. I learned that last week. That was kind of interesting. But that same AI factory, the internal NVIDIA team that trained the models, that built the software that are running the show floor demos, those same demos are running on those 72-way racks today. So when you're thinking about an AI factory, think about it as a life cycle. I'm going to run some things. I'm going to run inference during the day and maybe my scheduler at night when I have lower number of users switches more capacity to training. And one AI factory ought to be able to do that. In the DGX product line, we tend to talk about that as the DGX SuperPod. That's our all-in collection of hardware, software, networking, storage services to give you something that's pre-built. And that's what we have running in the data center that's running our demos today. Some new product announcements. Jensen hit on some of these, but every year, new product. Last year, this time, we talked about our new Blackwell series, the B200 and our GB200 systems. Those were up in production. Customers have those. Customers are rolling those out right now. Later this year, we'll have the new 300 series. Same systems, you know, with the GB series, it just gets faster. Same form factor, same chassis, just new GPUs, CX-8. On the B300 series, we actually made some pretty significant engineering changes in it to make the systems more modern. You see the little gap at the bottom. Every DGX you've ever seen is all gold on the front. You see, Charlie, there's a little gap at the bottom. What is that? Industrial design, did they run out of gold foam? We didn't run out of gold foam. These new systems are all actually designed for hyperscale data centers, for modern data centers. So it's all front serviceable now. It's a little thing, but as our customers are deploying these in the hundreds and the thousands, cold aisle service when you need to do something is so much better than being in the blast oven hot air. We also did something great in both platforms on the power subsystems. We've heard from everyone data center power, not data so much data center space, but data center power is an extremely scarce commodity. And when you look at provisioning large scale AI systems, one of the things that you see in those systems is here's how much power it takes me to run steady state. And then there's also peak power numbers. And if you're a data center planner, depending on your level of comfort and how you over provision things, you may have had to over provision power for peaks at 30% over provisioning, 50% over provisioning. Well, when you've got all that extra power provisioned and you're not using it, you're paying your colo provider for it, you're paying your data center for it, but I'm not getting value from it. So we did some really interesting things and we can talk to you more about it on the show floor to improve the energy efficiency inside of the system, to have storage systems that smooth out those peaks so that your usable power actually equals your provision power. So we have zero stranded power in the systems. So every watt that you're paying your data center for, you're actually getting value out of. And that's new in these systems. And of course, there's lots of technical and electrical details, but we've got a great architecture team to help walk you through that. I talked to you about the super pod that's running in our data center in South San Jose. Those are actual pictures of it. I got to go visit it last Thursday. Pictures don't do these things justice. If you've actually been in a modern data center, it's very interesting. They're all co-location facilities, massive buildings. And like I said, I've been in this industry for a lot, a lot of years. And you walk in, I see everyone else's cage, like, oh yeah, that's a normal cage, lots of servers. And then you walk into our cage and it looks radically different. It looks like this. There's four layers of cable trays and piping and automated valves, everything above it, all to do one thing, to manufacture AI, to manufacture intelligence, or to borrow from Jensen to manufacture tokens. The reason that we do this and we build these things ourselves is so that we can help customers stamp out the exact same thing in their own data center. And that's really one of the big things that we've done with the DJX platform. I don't build all the different server configurations. Our great OEM partners do that. We build exactly one per generation. And everyone that's using DJX systems around the world have the exact same things. And our own internal users have the exact same things. So even though if you look at details, this looks horribly complex, we can actually cookie cutter put this stuff out. We literally pre-built the data center infrastructure. And as soon as the racks left, our manufacturing facility literally rolled into place. The cables were all pre-done. The cabling team came back and plugged in all the cables. And the racks were up and running from the time they rolled in less than 24 hours. You know, this is a complex supercomputer. People talked about, you know, a year or two to provision a supercomputer. To roll in a rack and have it up and running and running show floor demos in 24 hours is something that not only we can do internally, but that we can help you do the same thing. Now, as we went through all of our own learnings of what it takes to build and run these systems, we learned a lot of things. That picture that I showed you, that's a beautiful picture, but we had to go through a ton of engineering iterations on that to actually get to the point that you saw. And as we talk to our customers, a lot of the challenges that they're talking to us about are the same things that we see internally. We need to build faster than we have people to necessarily run these things. The tools to manage all these things are, you know, disparate. They don't talk to each other. With the new GB series, they're liquid-cooled systems. So the server has to talk to the data center in real time. Turn the pump on, turn the pump off. Is there a leak? Things that never had to happen before. A lot of different tools. And the biggest thing that I told you about, the value in the system is how quickly do I turn it on and how quickly do I get tokens out of it? And so we looked at a lot of our own expertise and said, what is it going to take to make this easier for all of our own customers? And we've had a lot of tools over the years. Some of you are users of those tools today. You may know our base command manager product that ships with every DGX. If you're running a SuperPod, you're already using that today. And then we acquired a company earlier this year that was announced a while ago, Run.ai, which is a scheduling front-end, a workload front-end to help you manage all the GPUs that you have. Because at the end of the day, your users of the system just want to run work. They want to run a service. They want to do a training job. They don't care about the infrastructure. They just want it to work. And so what we did in this new software that we announced today, NVIDIA Mission Control, is not only pull those tools together, our data center provisioning and our workload provisioning tools together, but we also took all of our internal knowledge and built that into other services. So we're running thousands of these systems internally. We know when we see certain signatures, where the errors might be, you're running a multi-node job, what worked, what didn't in the job when you have a failure. How do we restart things more quickly? And so we said, you know, with the Mission Control product, we're going to deliver a single software stack to all of our customers, first to DGX customers, but next quarter to all of our OEM customers. Because no matter if you're using DGX, you're using one of our partner systems, you need the same tooling to really make it easy for you to run these complex systems. Because at the end of the day, your life isn't just about running infrastructure. I know that was my life for a long time. I'm glad I don't do that anymore. Because people don't call me at two-way when something's not working. But all these systems, if you want AI to be valuable, we want to make it easy for you to use. And so whether it's building the cluster, bringing it up, configuring it, doing all of your health checks, or running it, the Mission Control stack has all of those elements. You can use all of them together, if you want. For our SuperPod customers, some of them have no tools today to run, you know, large-scale GPU services. Great, you can take all of Mission Control. If you're already mature infrastructure, and you've already got some tools that you like, you can take the layers that you want on there, and NVIDIA can help you put those things together. One of the big things that we built into this is intelligence. Like I said, DGX has always been a platform for all of you in the audience that have used the platform. By having it, I've always said, you're part of my family, because you're just as much of somebody that should yell at me if something doesn't work, just like my internal users are. But when you call support, the support team has DGX sitting right next to them, because we built the things. But all the knowledge that we've learned over the years, everything that we've gotten, you know, errors from customers, errors from internal, that's now in a database, in a rule set, and that we continually update through the product. So when you do see something in the field, you're running a new application. At a great customer in Europe last year, tell me, you know, they tried something new, they tried a new AI model, and numerically it just didn't work. They called NVIDIA, they were using DGX, and they got to talk to our own language model researchers that were able to go into the platform and go, oh, yeah, that doesn't work. And then they told me a week later, they had new software from NVIDIA that did work. So we take all that knowledge, deliver it to you as a service in the product, and you can get that automation, the same automation we use to run tens of thousands of systems, is available to you as a knowledge stream in the product. We've got a session on Friday from another one of my team members that will double click on a lot of these things. But just know if you're trying to build one of these new infrastructures, you're looking for tools, we actually have something that will get you up and running quickly. Now, we haven't just done this all on our own. We've had great customers testing different pieces of this, deploying different pieces of it this year. DeepL, who has a talk today, has been a DGX SuperPod customer for for a while building really great real time LLM models. Block, who many of you in the US would also know as Square, the payment processing system, is doing some tremendous work in the LLM space. And iGenius, who's also here at in at GTC this week, is doing some really interesting real time regulated secure financial market LLMs, all leveraging DGX SuperPod, all in different markets. So no matter if you're in healthcare, or like Mayo or any one of these other industries, there's somebody that's probably doing something similar to what you're trying to accomplish in your own environment. We've got lots of use cases, the Nvidia teams are all able to help you. If you're trying something new, we've probably seen it before. And we can give you help in the field, you know, that you're most, most closely associated with, to really help you get up and running quickly. And that's our goal, to make AI accessible, easy, and to get you up and running quickly. With that, let me turn it over to my colleagues at Mayo to explain how they're actually using DGX. Hi, everyone. Thank you for having us here. So first of all, I want to introduce Mayo Clinic. Mayo Clinic is a large healthcare organization. And you're going to ask me why a healthcare organization talks about AI. We are dealing with very, very complex patients. And we want to make sure that we are always using the best tools out there in order to help us move the science forward. Fun fact, 10 steps outside of my office in Mayo Clinic Rochester, there is the first CT scanner in North America, correct? So we have, as part of our history, trying to be adapting technology early, understand how this technology works, and optimized in a way that we can benefit, it can benefit our patients. We're about 90,000 people at Mayo Clinic working with one goal, making our patients feel better, better, better, better, better, better, better, better, better, better, better, better, better, better, better. The patient needs come first. This is like the main thing that we are doing. And we are investing in AI. Actually, I've been lucky to be using DGX systems since 2017, when they had 16 GB of RAM. And this was quite challenging, trying to do 3D medical image analysis with that amount of RAM. Now, it's not a problem anymore. We are focusing on digital imaging, robotics, telemedicine, and then we have what we call the Mayo Clinic platform, which is an area that we can host all our data under the glass. So other companies can come and organizations can come and validate their algorithms in large cohorts. So we are investing a lot, not only in building these tools, but also validating and making sure that we have the best tools for our patients. Now, it is really hard to describe all the use cases. We are a very diverse organization. We have a program called Generative AI that started about a year ago. We have a whole department called AI and Informatics that has been there for a couple of years. And then each department within Mayo Clinic has its own embedded AI science team. And our goal there is to figure out how to discover and translate AI to clinical practice. One of the important things that I want to emphasize is for what we are doing, we have to go above and beyond the hype, correct? Things are working. When it comes to healthcare, they have to work continuously. And not only that, integration in the clinical workflow is an important component of success, correct? So we cannot deploy an AI algorithm and say yes to the radiologist, for example, wait five minutes and the results will be there, correct? The doctor wants to have the results at the time that they make the decision. And hopefully these videos can convince you that when we say complex cases, we literally mean complex cases. This is a case with polycystic kidney and polycystic liver disease. A human just takes like three hours to segment a polycystic kidney with our AI algorithms that have been built using this technology. It is just a few seconds to generate this segmentation. And not only that, we can segment each individual cyst and give above and beyond information, information that the clinicians will not have to make the decision. So again, our use cases are quite diverse. I'm biased in imaging. I'm part of the radiology department. But let's talk about generative AI. As part of generative AI, I would like to raise some examples of how we are using our compute infrastructure. We are building foundation models for pathology, radiology, and genomics, and text. These are all things that exist. There is a lot of open source tools out there. However, what we learn is like as you build these models, you can understand how you can push the performance and make it usable for like our clinical clinical workflows. So we are both trying to figure out how to build these tools and then also how we can validate them as part of the of like our like a clinical workflow, which is an important component. Correct. So we want to understand how to build and how to deliver the solutions in meaningful ways. So this is the overview of what we are using this infrastructure to do. So my colleague Marie is going to like show you how she's helping me execute and build these tools. And trust me, it's not an easy task. Thank you. Thank you, Panos. So why did we choose DGXs? So we have a long history of supporting high performance and high throughput computing at Mayo Clinic. We've been doing it for over 25 years. And most of our clusters are built like a traditional Bayouville style cluster. They're providing both clinical patient care workload, as well as a lot of research workload. And when we started working on AI work, you know, we were using some GPUs that were, we had spare cycles in some of these clusters. But now we needed to move into a place and you really develop the right tool for the right job. Cloud was not the right fit for some of this intense training. And we did have some things working in the cloud, but it just didn't meet all of our needs. So again, we needed to select the right tool for the job. And these DGXs, it's not your dad's HPC, right? So these things are built purposely for supporting AI and generative AI. So at Mayo Clinic, starting in December of 2023, we started moving into this, you know, big system with these DGXs. We started with a SuperPod configuration with six DGX H100 machines in December of 2023. And now we have a 17 node DGX. That cluster has grown with both H100 and H200s. And it's got two petabytes of luster parallel storage connected to it to keep those GPUs fed. We just recently installed a two node base pod configuration with H100s and a half a petabyte of luster in our Florida location to support cancer research in Florida. And as we're speaking, we're planning to install a 16 node SuperPod configuration with the B200. And it's going to have 34 petabytes of DDN storage. That one will literally start getting installed at the end of this month. We're receiving the final pieces of everything right now. So what did we learn? A lot when we started this in 2023, we underestimated a lot. We underestimated, you know, how long it was going to take to get this equipment and underestimated how complex this installation was. To kind of tell a story, we submitted the purchase order to NVIDIA on December 5th. And we expected delivery of all this equipment by the 31st. Well, not realistic, but, you know, we got really friendly with our NVIDIA partners, you know, through that holiday season. And things started coming in, we got an installer and, you know, really got moving on getting everything configured. But at that point in December, we didn't even have a data center. You know, our data centers are power constrained. And so, you know, while we were doing all this, you know, ordering for this equipment, we had colleagues at the data center that were trying to find a co-location facility for us that had enough space, power cooling, and the certifications to keep our data safe. So engaging a qualified installation support partner is essential to help coordinate all that shipping and, you know, make sure that everything is being received properly. We have the privilege of working with Mark III systems. They're just, they're amazing. So it's just a pleasure to work with them. Clearly outlining your deliverables, roles and responsibilities, because it's the little things that'll get you, like IP address space, you know, that kind of got us for a while. And then again, planning for adequate data center capacity and getting to know your people in the co-loc that do the smart hands and feet. Anticipating growth. We started with six. A year later, we've got 17 in that data center. Also, anticipating equipment failures. You know, you heard today, if you were at the keynote, there are literally thousands of components here, cables, transceivers. This is not a perfect world. So, you know, we had to also scramble to make sure that we had the right stuff there. Team training is essential. I mean, this isn't, like I said, this is a very complex environment and get the team trained and train them early because there's a lot to learn and it takes some time. And then leverage your TAM. This is, you know, another essential person to have on your team to make sure that, you know, you're getting best practices, you're getting knowledge transfer. They're there to support you. And again, I'll say the NVIDIA team has been just amazing supporting us in that space. And then again, don't lose sight of your business stakeholders. I mean, we have to work very closely together to make sure that we're marching forward together and, you know, meeting our business and serving our patients. So that's all I have. Thank you. Thank you. Thank you. Thank you. Thank you so much. And again, thank you for bearing with us. This was a very full session. The panel is open to questions. So I'm going to come around and see how many we can get in the next couple of minutes. I saw this gentleman's hand up first and we'll come around. Hello. Thanks for this wonderful session. So are there any metrics that you also look at to understand the success of your deployment? That's for me, right? Yep. Yeah. So we do. So our HPC team collects metrics. So we're trying to mature our space right now. So right now we're looking at utilization. Who's using the cluster? You know, are we making efficient use of those GPUs? How much CPU is being used? You know, how well are we using our memory? So again, we're reporting those metrics on a monthly basis to show utilization. It helps us plan for growth. You know, you could see that we are more than double our size. But there's a lot more to learn. Thank you. And if you would also just go ahead and fill out the survey as well while we're waiting for questions. Thank you. What specific Gen AI use cases do you have for which you are using the DJX super pods? So maybe I can take this one. So an example, for instance, we are building foundation models for imaging. This would be like x-rays and CTs. So we are trying to understand like what goes in to build these models. So we have a lot of like try and fail type of things. So we have like our workflows that we train the models, benchmark our models and try to iterate fast in order to like see where we need to go. This is one use case scenario. We are building small digital pathology models or like domain adapting large language models using our technology. So again, we are a very diverse organization. A lot of like open issues in cancer treatment and other areas. So, but you know, like this would be a couple of the use cases. A lot of them require like, you know, like imaging moving like gigabytes of data to like train these models. Hi, thanks for the session. You mentioned that there are a lot of departments that want access to the equipment and that there are lots of diverse use cases. I'd be interested in hearing your experience with sort of offering this equipment in a multi-tenant way and you know how that works. If it's been easy, if it's been hard. Yeah. So it's a challenge with giving people access because everybody wants to be on this cluster or these clusters. So we have a combination of, you know, an approval process, you know, an intake process and we follow that process to make sure that, you know, there's a business use case that's been approved, especially for the people that have so kindly funded these for us. So, you know, we run that up the chain, you know, if digital pathology or AI and I teams, you know, need access to the cluster, then we'll grant that access. And then our HPC team will get the provisioning done and they'll help with the training. So they do a great job. This is a very high touch area. And so, you know, there's a lot of materials that they make sure that the user will be successful, get them onboarded and really walk them through the process. I can't say enough about my HPC team. I got a couple of them here. So they're great. Did that answer your question? Good afternoon. Thanks for the good session. So I have two questions, Charlie, one is that software tools that you provide this mission control, can they be extended to other parts of the enterprise with the legacy systems so that we have integrated controls? Yes. I mean, they, we just announced this new product today, but the, we don't expect that we are going to be your overall data center management tool. You know, different people will be users, users of the tool, like the, the AI practitioner, we use that practitioner in our face, but even like our troubleshooting, our break, fix automation in that, that already has natural integration to something like ServiceNow. So if you already have a centralized data center management monitoring tool, you can just use that tool and export all the events out of our own tooling. And then when you need to double click, you can go into our tool. So I've done data center long enough that I know trying to get people to change the tools they're already running is a futile task. And so everything we've built is open, integratable. It has APIs into it. So you could never actually use one of our, interfaces and see the interface. You could just call everything through APIs, but that's the design goal of the stack. Yes. Yeah. And the second question is, uh, as regards, uh, like my, oh, bought their own systems, uh, in the enterprises, do you see this trend that, uh, uh, for the enterprise AI companies will be buying their own systems and putting their own data centers as against going to the cloud? Yeah. And we, we see both use cases, you know, I don't have any customers that don't have some cloud and, you know, the more and more companies are running systems on prem because it gives them that same central infrastructure that I talked about. You know, we started at Nvidia back in, in 2017, because they want to have the control over it. A lot of times the data is native internally. I, I know being a patient of, of different medical imaging practices, uh, my first MRI, they gave me a stack of DVDs, the data that's coming off of that. That was for one person. So all of that data that, you know, our customers have, you know, whether it's medical, whether it's auto, it's financial services, like if your data already lives in your enterprise, already lives in your data center, you always move the compute to the data. The other way around is just too hard. That said, if you're a cloud first company, if all your data is living in the cloud, you probably don't want to bring it on prem. So, uh, most of our customers do both and it really depends on their workload, but we, you know, we see a tremendous uptick in people starting in cloud, but then having a centralized infrastructure, uh, on prem. And we also, when we talk about on prem data centers, uh, I think Marie talked about it, you know, co-location partners, the old way of doing things, the way that I grew up doing things, uh, like a company actually had part of their building and it was their data center. Uh, but with these new modern systems, with all their requirements, most people aren't building their own anymore. When people are talking about on premise systems, they're getting it, they're getting their data center from a data center partner. Equinix is one of our big data center partners. They're hosting that GB 200 cluster that I talked about. So even at Nvidia, we're not building buildings anymore. And if we don't have to do that, there's experts that can do those things. Um, and those data centers also are very connected to cloud environments as well. Um, so generally it's always a mix. Hi, thanks. Uh, question for Marie, um, as you took a sip, sorry about that. Uh, you mentioned that, uh, the public cloud wasn't a fit for this project and you ended up in a colo. Uh, and, and when you were going through your list of sort of lessons learned, I was sort of processing that as wow, you know, sort of looking at them at least on the surface, you know, a cloud service provider would have sort of abstracted or minimized almost all of those pain points away. Can you talk maybe a little bit more in detail about why the cloud wasn't a fit for your application? We only have a minute. Yeah. Um, so we are actually hybrid multi-cloud and so we have a capacity in both Google and Azure and, you know, in Azure, you know, we had, we have a person on our team, uh, that also has been helping get those models working out there. Um, took a while, but it's been pretty successful. I think that's working pretty good. Right. Um, in the Google space, we've got some immaturity in some of the, the features and capabilities and that cloud team was really not fully integrated into like the infrastructure teams that are supporting the on-prem and the, um, the, um, Azure cloud. So it's partly our fault and partly a technology issue and feature capability issue within Google, along with some, um, pricing dynamics that were kind of haunting. Thank you. I just want to say thank you again, Marie, Pangeadis, Charlie. Um, if the, if they're willing, they might be able to take some questions after they de-mic at the end. Thank you so much for coming to this session. Have a wonderful GTC. Thank you, everyone.