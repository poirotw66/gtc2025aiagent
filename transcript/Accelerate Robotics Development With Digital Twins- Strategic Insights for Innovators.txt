 We'll be back next time. Thank you. Hello, everyone. My name is Eli Khouri, Simulation Engineering Manager at Idealworks. Over the next 40 minutes, Anthony Rysk, our Innovation and Engineering Manager, and I will show you the future of robotics. Today, you'll see exactly how AI and simulation at Idealworks can help accelerating robotics with digital twins. 55 seconds. That's the strict production time required for each BMW car to roll off the assembly line. To maintain its position as the top automaker, BMW's plant managers must ensure that every aspect of production meets this demanding standard. In such a competitive industry, every detail matters, not just on the assembly line, but also behind the scene. This brings us to logistics. The often unseen yet critical backbone of the production process. Efficient logistics ensures seamless coordination between material supply and assembly execution. BMW recognized early on that logistics optimization was the key to achieving its ambitious market goals. By leveraging automation, the company successfully streamlined its supply chain, ensuring that every part arrives at the right place at the right time, keeping production running at peak efficiency. Our journey began there at BMW in 2015. It began with a need for an industrial robot solution to handle logistical goods in warehouses and factories. The journey continues with Idealworks being born in 2020 from within the industry and for the industry as a hub for technology and innovation. In 2023, matching our mission and vision, Agile Robots SE, a spin-off from the DLR, which is the German NASA, became a new strategical investor. Idealworks is an active member of the NVIDIA Partner Programme as a solution advisor consultant. Has been a pioneer in the adoption and development of expertise in Omniverse since 2019. From AMR vendors to robotics ecosystem provider, Idealworks powers the future of automation with robotics ecosystem. What does our robotics ecosystem consist of? Any fleet, a platform allowing our customers to design automations and control their logistic processes. Second, robotics. The automated physical agents that are commanded by any fleet and that execute the user's desired tasks. A vast portfolio of robots, including our IW Hub and integrable partner robots, can be selected based on our client's use case. And finally, IW Sim, our digital twin creation technology that integrates with any fleet to give our clients a new perspective of their environments and their automations. Join us in a quick dive into our three pillars to discover how each contributes towards shaping the logistics of the future. Any fleet is the backbone of our robotics ecosystem. It empowers our customers by putting them in control, allowing them to design and customize their automation workflows to perfectly fit their unique use cases. Plant managers use various robots to streamline tasks, but each provider has its own fleet management system, creating integration challenges. Any fleet solves this with VDA 5050, ensuring seamless integration across robot types so customers can focus on automation, not fleet management. Any fleet provides an intuitive multi-platform web-based interface, empowering users to seamlessly monitor and control their plans, both on-site and remotely. Any fleet provides an intuitive interface, and the other features of the IoT devices. Furthermore, integration with IoT devices such as call buttons, proximity sensors, and automatic gate enhances control over the environment, enabling more seamless and automated operations. Any fleet acts as the central hub, seamlessly integrating robotic solution warehouse systems, IoT devices, and sensors, all focused on the customer's needs. Any fleet's needs. Any fleet's core services that include task assignment to pick up the right robot for the right task, traffic management to avoid congestion and optimized routes, and task scheduling to plan mission execution. Any fleet streamlines operations, boosts efficiency, and provides real-time insights empowering businesses to optimize workflows, increasing productivity, and focusing on growth while simplifying automation and system management. Jumping over to our IW Hub. Here's a video of it operating in its birthplace at BMW plant Dingolfing. The IW Hub is a transportation robot that falls under the category of autonomous mobile robot, or AMR. Given the pickup point and the drop-off point, the IW Hub is able to navigate autonomously to deliver the package while ensuring reliable performance in complex environments, with a high operational efficiency. Our CE-certified robot can operate safely around humans in industrial areas insured by industry standards. Here's an overview of our robot strength. The IW Hub has a robust payload capacity of 1,000 kilograms, moves at a speed of 2.2 meters per second, and offers an impressive battery life of eight hours, making it reliable for extended operations. It's also highly adaptive with features like dolly transport for versatility, pallet dock, and conveyor belt extensions for streamlined handling, and turntable extensions. And it's equipped with smart capabilities like 3D obstacle avoidance, intelligent rerouting to avoid disruption, and autonomous navigation in mixed traffic ensuring seamless integration in dynamic environments. Now moving on to our third pillar, the simulation. Handling and optimizing logistics is a delicate process that requires a great deal of knowledge of the logistics environment. There's no room for error when it comes to changing the environment that must operate 24-7. Our solution for that is to scan the environment of the customer, create a high fidelity digital twin, enable them to use it as their playground. As simple as that. But how can a playground be useful? Initially, we developed IW-SIM at Idealworks as an internal tool to integrate digital twins, robotics, and sensor simulations. By emulating IW-SIM, it quickly became a powerful development and testing platform, allowing us to refine our software with greater accuracy and efficiency. As we scaled IW-SIM to support multiple robots in a digital twin of real client environments, we unlocked even more potential. This time for our customers. Now they can use the digital twin to optimize planning, design new layouts, and collaborate with unassetized environments without disrupting operations. Beyond planning, IW-SIM also enhances automation testing. By integrating simulated fleets with any fleet, businesses can visualize and fine tune their automation strategies in a virtual space before implementing them in real world. This minimizes risks, reduces downtimes, and ensures a smoother deployment. Ultimately, IW-SIM is more than just a testing tool. It's a bridge between digital simulation and real-world performance, helping our clients innovate with confidence. With that, we conclude our product portfolio. Within only five years, we've got 1,388 robots out there, running 176,000 missions every day with a 98.5% reliability rate. That's not just a vision of the future, it's happening right now, and we're proud to be leading the way. We've made our mark across multiple industries, automotive, manufacturing, and logistics and warehousing. Major players like BMW, Toyota, Akofend, Miele, Zalando, and many more, trust us to empower their automation and efficiency. Our solutions aren't just theoretical, they're driving real-world impact every day. Now, I'll hand over to Anthony to dive deeper in the main topic of this presentation. Today, we will present to you our perspective on the future of robotics. See, Sense, and Simulate. Three simple words that are about to revolutionize how we do robotics. We will show you how we build, innovate, and create the future of robotics ecosystems. Starting with the first pillar, seeing better with VSLAM. In short, 2D SLAM is a method where a robot simultaneously builds a 2D map as the one we see on the right, and estimates its position within that map. Here we can see the overlying 2D LiDAR scans in red as well. But even the best SLAM might hit limitations in vast, repetitive environments. The kind of environments we deal with every day. Here is an example on how this problem might occur. On the left, you see our omniverse-created environment. And on the right, the occupancy grid layered with LiDAR scans. You can notice how, as the robot moves down this aisle, traditional SLAM starts to crumble. Losing localization step by step, and drifting into the danger zone. As you can see at this stage of the navigation, the robot wandered into prohibited areas beyond the yellow line. Which constitutes an undesirable behavior when it comes to operating next to production lines. A lot of companies have their secret sources to attend to this issue. From our perspective, one of these solutions could be VSLAM. If you close your eyes for a moment and imagine your favorite place. Your living room, perhaps. You can navigate it with perfect ease, because you've built a mental map in your mind from this place. Now imagine a robot doing the same thing in real time, using simple cameras and advanced algorithms. That's the magic of VSLAM. Visual simultaneous localization and mapping. Now take a look at this video. In the small window, you can see the robot's environment. And you can notice how much the configuration will be changing on runtime. While on the bigger screen, you can see the stability of the extracted visual features that guides the robot and allows it to maintain localization despite all of this havoc. Even as we radically change the entire environment in real time, throwing it into chaos, the robot never loses its grip on reality. It navigates, it localizes, and it carries on, just like a normal human does. Robots equipped with only a few cameras, sometimes as few as one, can build detailed maps, understand where they are, and adapt to changes instantly. That means fewer blind spots, fewer collisions, and more intelligent movement. It is clear why traditional VSLAM, stuck in just 2D dimensions, falls short. But once we unlock the full 3D perspective, these robots begin to see the world just like we do. And diving a little bit deeper into the VSLAM, we start with this diagram. So we have multiple streams. It could be RGBD, RGB, or stereo. So the robot can see the world from different angles. We fuse those feeds together, aligning them in perfect sync. And then the front end takes place. This is where visual odometry tracks the robot's movement frame by frame in real time. Meanwhile, in the back end, advanced optimizations polishes and refines that motion data, reducing drift and boosting precision. All of this comes together in a living, breathing map. It is constantly updating the robot's location, orientation, and environment. This is how we turn raw images into actionable intelligence. And how our robots navigate with unwavering confidence. Now here's where we can actually supercharge our systems. Everything you saw before, visual odometry, feature tracking, pause estimation, can get to the next level with NVIDIA GPUs. And here we're going to talk more about NVIDIA QVSLAM. It's an NVIDIA GPU-based visual odometry. By parallelizing key point selection, image processing, and everything from triangulation to bundle adjustment, we're tackling the most computationally heavy parts of SLAM in real time. It's like giving our robot the horsepower to instantly sift through vast amounts of data without ever missing a beat. So every movement is precise, every update is immediate, and every map is incredibly detailed. Now, it looks impressive and makes perfect sense. But does it actually deliver? And more importantly, do the numbers back it up? Let me show you our study. A head-to-head comparison of QVSLAM against traditional VSLAM algorithms. So we can see how it all stacks up in the real world. Our study was carried out in our R&D center in Munich. Looking at this graph, we report the translation error on the Y-axis for multiple VSLAM algorithms. And we can see in green how the GPU-enhanced VSLAM scores as low as 0.95 error on the translation side. But when it comes to navigation and localization for autonomous mobile robots, it is important to treat rotational error on its own. As with rotations comes a lot of uncertainties and noisy data, such as motion blur. We can see that despite parallelization, we are able to keep a competitive rotational error of less than 1 degree per meter. But perhaps its most appealing benefit is its runtime speed, with a mind-blowing low of 7 milliseconds, nearly 7 times faster than its counterpart, making it perfect for real-time on-edge deployment for robotics use cases such as ours. And it might be a candidate for futuristic enhancement with temporal aggregation for even more robust results. And now, we will go back to Eli to see how simulation can even enhance better this whole flow of VSLAM. Like everything else in life, VSLAM also has its limitation, especially when it comes to large-scale deployments. This is why our ecosystem provides an enhancement through simulation. The industrial application of large-scale VSLAM greatly benefits from simulation, offering unmatched control and efficiency compared to real-world environments. Simulation in Omniverse allows full control over lighting, textures, and obstacles, enabling rapid, automated data collection with assets randomization. Unlike real-world testing, it provides perfect ground truth without costly motion-capture systems, significantly reducing overall costs through optimized environments and accelerated development cycles. Moreover, simulation enables the creation of diverse edge cases and failure scenarios, without physical limitations or safety risks, making it a superior approach for testing and refining VSLAM systems. Now, back to you, Anthony. So, we move on to the second pillar, better sensing with depth estimation. And here we can think about how we as humans perceive the world. We seamlessly blend stereo vision and depth cues to create a rich 3D space that guide our every step. Now, that same remarkable ability, understanding and navigating our surroundings with precision, is the key to empowering autonomous systems. We know that there are a lot of models out there relying on monocular depth. But based on our internal research, we found that stereo depth estimation is a better approach than monocular ones. In fact, with a single camera, you're relying on 2D clues. No rich geometry, which often means less reliability and meter level inaccuracies. It's like looking at the world from a postcard. Now, compare that to stereo, where two camera views come together just like our own eyes. We gain robust multi-view geometry, centimeter level precision, and a true mimic of a human vision. In essence, stereo isn't just a step forward. It's how depth perception was really meant to be. And just take a look at this scene. It is from our R&D center. Our software is live on the IW Hub, processing point clouds in real time. On the top left, you're seeing the raw depth data from a standard camera. On the bottom left, it is the ESS enhanced point cloud, an NVIDIA module. Notice that critical objects like forklift forks, often the cause of collisions, particularly vanish with normal camera depth. You can see it here on the right side, on the top right side. You can compare these two approaches side by side. On the top right, the depth image is generated by a normal camera, and it is shown. And we can see that the inconsistencies in this data when it comes to forklift render it unusable by robotics navigation stack. Now, we will be able to see on the bottom right how AI depth looks like. And we can clearly see the difference in data quality, rendering this something usable by our navigation stack. Now let's talk real-world performance. This isn't just a neat demo. It is our actual solution fully integrated into the software stack running on our operating system in real time. You can watch as the system instantly detects the forklift and automatically steers clear. This footage is captured straight from our onboard camera. Real technology solving real problems right now. And here's the secret ingredient. NVIDIA ESS. It's been trained on more than a million synthetic data points, plus 100,000 real-world images. That means it's not just fast because it's optimized for working on the NVIDIA GPU with TensorRT, but it's also accurate, reliable, and ready for the unexpected. And once again, we let the numbers do the talking. ESS keeps temporal error impressively low, while monocular approaches such as depth anything ends up with three times more, around 12 centimeters. That difference isn't just a statistic. It's a game changer for consistent real-world performance. It is worth noting that the temporal error of NVIDIA ESS is almost three times better at the depth image generated by a normal consumer-grade active stereo camera. While CRE stereo, which is a temporal iterative component, holds a consistency of 1.4 centimeters. As for endpoint error, we are six times better performing than our monocular counterpart. We're even getting a two times enhancement on what we would get from an active consumer-grade stereo camera. Also, we are being very competitive, almost equal to the results that iterative stereo modules such as CRE stereo give out. And here where the magic happens. The runtime speed for ESS is a mind-blowing of 50 milliseconds, making it perfect for real-time, on-edge deployment for robotics use cases. Despite the better consistency of iterative models such as CRE stereo, they remain unusable for real-time edge deployment when we have strict time constraints. Moreover, the time taken by ESS to generate a depth image is very close to the time that is required by a normal camera to calculate it. Now, we go back to Eli to talk about more the simulation. Thank you, Anthony. We've highlighted previously that our simulation solution, IWSIM, is a robust development and testing platform enabling precise and efficient software refinement. In this section, we present two use cases where using Omniverse played a direct role in optimizing robotic software and empowered the adoption of our automation solution. Here's the first use case. Features like obstacle avoidance and material handling require situational awareness. To reach this maturity in complex and dynamic environments, the robot must undergo long sessions of training on a multitude of scenarios presented as data sets. Collecting these mountains of data from real-world, from factories and deployment sites tends to be an impossible task due to the complexity of these environments, the access to data without breaching data privacy of customers, and the resource-expensive labeling process and obtaining ground truth. Using Omniverse's replicator, we were able to create and customize our own Idealworks synthetic data generation extension that we designed to randomize our customers' digital environment by shuffling their high fidelity digitized intralogistic assets. We then can vary the camera position from the IW Hub perspective to generate data. In this video, we see our Omniverse extension in action on the digital twin of our R&D center in Munich, visible on the left window. The result, visible on the right window, is lifelike RGB images, their respective depth images, semantics, bounding boxes, and various annotations serving as ground truth for training. This will feed data to our data-hungry AI model while cutting through the complicated task of collecting real-world data from factories and deployment sites. Jumping to our second use case, we tackle another use case of our simulation technology on that, and this one is valuable for our clients. Since automation is undeniably key to optimizing logistic processes, why hasn't it been universally adopted across all industries? At the end of the day, everything revolves around the customer. Their goals are the central focus of the technology. The main challenge isn't convincing customers to adapt automation for optimizing logistics. It's ensuring a smooth transition. Giving the customer time to adapt to automation while making data-driven decisions, while minimizing downtime and production disruptions is key to gaining their trust. As part of our commitment to ensuring a seamless transition for our clients, we introduce IWSIM. By scanning customer environments and creating digital twins of their facilities, we generate a virtual replica where extensive testing and optimization can be conducted without disrupting active processes in the real plant. Our solution engineering team, in collaboration with the client, designs the use case by identifying the type and number of robots, configuring zones and points of interest, and seamlessly integrating our automation platform, any fleet, with IWSIM. Combining a configured automation with a multi-agent simulation, the result would be the following. A virtual factory of simulated robots commanded by our fleet management and automation platform, AnyFleet. What you see on the left is a live simulation running multiple IW Hub robots in NVIDIA Omniverse. On the right side, it's the AnyFleet interface managing robots' commands and monitoring robot statuses, paths, and behaviors. AnyFleet interacts with simulated robots as if they were real, since they operate on robot software. As the video clearly shows, the simulated robots are assigned missions from AnyFleet which they execute in simulated environments with respect to the configuration of the automation. Let's talk about the advantages of this approach. What it actually provides is a high fidelity accurate representation of automation in the client's plant, access to AnyFleet's analytics for performance measurement and forecasting, hands-on experience with our technology to bridge skill gaps before full implementation, and finally the ability to test different automation strategies before deployment. With these advantages, the customers are capable of a smooth adoption of the technology with a measurable impact, a trained team while being aware of the upcoming edge cases. Anthony, could you please give some credits for our team? Anthony, could you please give me a shot? Yeah, sure thing, Ali. Now, as incredible as this journey has been, we didn't do it alone. Behind every great idea, every breakthrough, there's a team of brilliant minds pushing the limits, challenging the impossible, and making it all happen. These are the people who poured their passion and their talent into bringing this vision into life. So today, we take a moment to recognize them, because without them, none of this would be possible. See, sense, simulate. These aren't just words. They are the foundation of a new era in robotics. With vSlam, AI models such as depth estimation and synthetic data, we're not just improving machines. We're giving them the ability to understand, to adapt, and to create. And this is just the beginning. The future of robotics isn't something we're waiting for. It's something we're building right here, right now. And with these technologies, we're shaping the next generation of robotics ecosystems. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.