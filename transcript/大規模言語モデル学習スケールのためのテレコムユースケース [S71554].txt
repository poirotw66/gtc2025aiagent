こんにちは NTTコミュニケーションズの 辻崎と申します本日は大規模言語モデル学習スケールのための テレコムのユースケースとして発表させていただきますどうぞよろしくお願いいたしますこの発表は私 辻崎とメインの主張である 鈴上の共同の発表となっております前半のパートについて 私 辻崎の方から ご説明させていただければと思いますまず この発表では この生成AIに対する テレコムのアプローチとしてGPU over APNというものについて ご紹介いたしますGPU over APNというコンセプトは 既存のGPUリソースを有効活用するためまた さまざまな社会課題を解決するための 効率的なアプローチであるというふうに考えておりますまた それらを活用するために NVIDIA社のNEMOフレームワークやメガトロンといった分散フレームワークについて いろいろ検討を重ねておりますのでそういったテクニック アプローチ そして 実際にはそういったテクニックが実現するようなGPU over APNでの実現する実証実験の結果というものについて ご紹介できればというふうに考えておりますまず 最初 イントロダクションとして 生成AIの課題とテレコムのアプローチについてご説明いたします近年 ご覧の皆さまも非常に課題感を抱えられているというふうに思いますが生成AIについて GPUの需要というものが非常に拡大しております特に この2,3年で起こっている生成AIブーム データ利活用 画像処理など多くの分野でNVIDIA社のGPUをはじめとした 多くの計算資源が必要となっていますもちろん GPU自体は非常にパワフルなチップ リソース 計算資源ではあるもののこうした生成AIの需要に対して 今 GPUが全然足りていないというような背景があります特に GPU 1枚で計算できる もちろん GPU 1枚はパワフルな処理ができるんですけれどもGPUが1枚で計算できると解決できない課題を解決するために 複数GPUサーバーを並べて同時に使うという支援が非常に拡大しておりますでは 実際になぜそんなに GPUが必要なのかということを少しだけ簡単にご説明いたしますと昨今 生成AI 何かを聞いたら回答してくれというような生成AIを皆さま思い浮かべるかもしれませんがそういったAIモデルの学習に こういったGPUのような計算資源というものが使われるというような背景がありますこのAIモデルの学習については 基本的にはそのモデルAIの重みというものに対して たくさんのデータを入力してその重み自体を少しずつ変更させて 学習をさせていくというようなプロセスが重要になってきます一方で 皆さまが感じていらっしゃる通り生成AIについては 1週間 2週間 1ヶ月といった非常に速いペースで世界が動いていることもあり より高精度で より良いプロダクションのモデルというものを素早く提供する必要がありますここで重要になってくるのが そのモデルを開発する より早く学習を進めたいという要件になりますこういったアプローチをする際には 基本的には学習させる大量のデータというのを複数に分割し 複数のGPUで同時に学習を進めることによって全体の実行時間 モデルの開発にかかる実行時間を減らすというようなアプローチが必要になってきますまた 昨今の生成AIでは 非常に巨大なモデルというものが出現してきております要件人に書いた通り より大規模なモデルを使いたい場合には例えば RAM 3.1の405ビリオンのパラメータのモデルに対しては8ビットの精度で405GBほどありますので今の比較的新しいHopperの世代であっても 80GBのデモリーつまり1枚のGPUには乗らないような 巨大なLLM 生成AIというものが出現しておりますこういったものを学習するためにはそもそも学習をするために 複数のGPUにこのモデルを分割して学習するというような手法が 必要になってくるためこの要件2 要件1をそれぞれ組み合わせ巨大なモデルをより早く学習をするそして市場に出していくというプロダクションの観点からGPUの需要が非常に拡大しているというような 現実がありますこれに対して もちろんNTTコミュニケーションズはさまざまなソリューション あるいはデータセンターのようなビジネスを展開しているんですけれども単一のデータセンター やはりデータセンターを1つ作るにも非常に時間がかかりますので単一のデータセンターで お客様の需要に合うようなリソース特に電源や熱量 ラックといったものを 提供することができないというものが課題になりつつありますこういった時に もちろんクラウドの利用を考えられる お客様もいらっしゃると思うんですけれどもクラウド自体も非常に大量のGPUを 一遍に取得するようなものがなかなか難しくなってきておりまして今 あるそれぞれのデータセンターに 分散したようなGPUのリソースをより効率よく使っていくというような 必要が現在出てきておりますこれらを解決するために 我々はGPU over APNというコンセプトを打ち出し実際にこれが使えるんだ実際にこのようなパターンで このような需要に応えられることができるんだというような実証実験を進めているというのが 本日の紹介になりますこのGPU over APNは 昨年の10月にプレスリリースで世界的に報道させていただいたところでは あるんですけれども今言ったような 単一のデータセンターのリソース 計算資源ではまかないないものを ネットワークでAPNというネットワークでつないぐことによってオンデマンドにGPUリソースを確保しこういった足りない計算資源を 有効活用するということをコンセプトにしておりますこのため 先ほどご紹介しましたような 分散学習における課題例えば 従来のGPUを使った 生成AIの学習については単一のデータセンター内で複数のGPUサーバーを使って 学習を進めていたものというのを複数のデータセンター間で 余っているGPUリソースをそれぞれ この間のIoNAPNというもので つないであげることによってまるで一つのGPUクラスターかのように 使うことができきちんと効率よく学習を進められるということを 実証するようなコンセプトになっておりますでは 実際IoNAPNというのは何かというところを 軽く少し宣伝みたいになるんですけれどもご紹介させていただきますと こちらはですね 我々NTTグループが提供する高容量 高品質低遅延で 低消費電力のネットワーク回線のサービスになっておりますと実際 こちらのIoNAPNは 既に昨年の3月に提供を開始しておりましてお客様からご要望をいただければ 提供できるようなものになっております実際 このIoNAPNを使うことによって こうしたGPU over APNというコンセプトを皆様にご提案しているという状況であるんですけれども我々はこのIoNAPN 単にお客様のご要望の端定に つなぐだけではなくてGreen Next Centerという 我々が今建設中のデータセンターに関してもそれぞれをIoNAPNでつなぐことによって お客様が必要なときに必要なニーズを我々データセンターとIoNAPNセットで ご提供するということを目指しておりますしかし このGPU over APN 一言で すぐに実現できたら素晴らしいなというふうな感想を持っていただけると 幸いなんですけれどもなかなか実際の技術的な部分については 課題があるということを我々は感じております実際にその課題をどのように解決するかというのはこの発表の後半での本心になるんですけれども課題のところだけ簡単にご説明いたしますとまず こうした分散データセンターのコンセプトというのは今までなかなか普及していなかった背景もありどのような分散学習フレームワークを使うかということが 非常にキーになります特に最初に説明した通り 複数のGPUサーバーで分散して学習というようなコンセプトになりますので このマルチノードでスケールする学習フレームワークへの対応というのが必要になりますこちら NVIDIA様とのパートナーシップで この後紹介する2問で解決いたしますまた この分散学習フレームワークに対して 我々独自のLLMを持っておりますのでつづみという独自のLLMを持っておりますのでこちらについてどのように適用したかというところについて ご紹介いたしますまたIoN APNは既にお話しした通り 高品質で低遅延なネットワークだというふうに言わせていただいているんですけども もちろんその中で物理的な限界というのがありますのでそういった部分の長距離通信遅延を どのように解消していくかこれをNVIDIA様のGPUダイレクトRDMAの 技術と組み合わせてどのように改善できるかといったことについてこの後 ご説明させていただければと思っていますはい ここからは鈴上を発表いたします実際に先ほど津田木から紹介いただいた課題に対して分散学習フレームワーク NVIDIAにもメガトロンで解決したというところについて 紹介させていただきますまず最初にですね NVIDIAにもメガトロンの概要について説明させてください右図のようにですね 中心となるものはですね緑色のところで記載された メガトロンコアというふうなものになっておりますこちらはですね トランスフォーマーの 大規模学習ライブラリとなっておりまして基本的なPyTorchベースで書かれておりまして基本的なアテンションやMLPのブロック等が 記載されているようなものになっていますその上にですね NIMOやですね メガトロンLMといったようなですね学習実行のフレームワークが 存在しているような形となっておりますこの2つの違いについてはですね 段的に申しますとNIMOはですね アウトオブザボックスで クイックにですね実行が可能となるような形となっており一方でですね メガトロンLMに関しては 自由度やカスタマイズ性が非常に高いものになっておりますですので クイックにですね 実行した場合はNIMOを選択して内部のですね 学習等をですね カスタムしたいというふうな場合はメガトロンLMを選択するというのが 良いかなというふうに思いますで こちらですね NIMOやNIMOはメガトロンを利用するメインといって説明させてくださいこちらですね 多様なモデルアーキテクチャを サポートしているという点がまず大きな大前提となりますGPTスタイルのですね デコードオンリーのものからですねT5のエンコーダー デコーダー もしくはですねエンコーダーオンリーのバートのようなものもですねサポートされているというふうな形になりますまたですね 学習のコース化もですね 非常に大きなポイントとなっておりましてこちら Mixed Pressionのですね BFloat16ですとかFP8というものがサポートされておりましてこれによってですね 学習の高速化や メモリ使用量の削減といったところが期待されるような形となっておりますまたですね 学習のですね 並列分散学習手法のサポートもですね非常に充実しておりますそうですね 基本となるデータパラレルからですねパイプラインパラレルや転送パラレルというものがですね実装されるような形となっておりますで こちらがですね 実際にどのようにメリットとして感じられるかといいますと右上の大規模言語学習モデルのですね学習知見のところから引用させていただきましたが非常にですね LLの学習というものはですねコストがかかるようなものとなっておりましてこれをですね メガトロンのようなですね高速化のライブラリーを使うことでですね非常にコスト的なですね メリットが大きいという風な観点が挙げられます例ですと 12.3億円からですね 9.3億円までですね3億円節約できますよという風なものもですね実例として上がっておりますまたですね 下回りのオーケストレートのサポートもですねSlamですとかKubernetesとサポートされておりますまた管理サービスのですね 1DBとも簡単にですね 連携するようなことが可能となっておりますので非常に使い勝手が良いかなという風に 個人的にも非常に思っていますでですね 学習のスケーリングについて 実際に適用可能化についてもですねこちら メガトロンLAのリポジトリのですねところに検証結果載っておりますこちらの検証結果といたしましてはGPTのモデルを使いましてマルチノードに学習したスケーリングの検証結果が 実際に掲載されておりますH100のですね GPUが48個をですね6144個までですね 実際にスケーリングさせたような結果が載っておりますで 右のですね グラフのように 実際にこう想定されるですね スケーリングと非常に遜色ないような形で 実際のですねフロップストーンも追いついておりまして理想的に学習がスケーリング可能であるというところもですね実証として分かっているという点が 非常にポイントかなという風に思いますでですね ここからNIMOをですね 使うという点についてちょっと1点注意点がありますので 紹介させてください現在ですね 2025年3月時点でですねNIMOはですね バージョンとして1.0からですね2.0へバージョンがですね 変更されるような転換期であるためちょっとご注意が必要です本資料はですね 以降は基本的にはNIMOのですね2.0ベースでお話しさせていただきたいという風に思います1.0とですね 2.0の違いに関してはですね大きく1.0ではですね YAMLベースの設定方法になっていたものがですね2.0からはですね Pythonのコードベースに変更となっておりますこれによってですね Visual Studio Code等のですねものでサジェストされるようにですね UXが非常に改善したかなという風に思います右下の画像のようにですね 実際にですね ラーマのものがですねサジェストされているかなと思いますまたですね 従来に比べて設定やカスタマイズ性がですね非常に綺麗に整理されているかなという風に思いますので開発者目線としても非常に使いやすいものになっているかなという風な印象を受けますはい で 実際にですね 2.0でどのようにですねLLMの学習コードを記載するかについて説明させてくださいこちらの右のですね 学習コードの例のようにですねこのように数行で実際にですね 学習を実行可能という風な形になっております全てですね コードベースでの設定となっておりますまたですね 実際に実行するといった点についてもですねNIMO Runというようなですね 通がありましてこちらでもですね Quickに実行するという風なことが可能になっておりますレシピという形でですね 主要なLLモデル 例えばですね GPTですとかNEMO TronやLamaといったようなものをですね実際に簡単に学習が可能になっていますまたですね ツールとしてもコマンドラインのインターフェースとしてNEMO LLMプレトインといったような形でですね簡単にですね 実行も可能になっております実行する環境についてもですね エグゼキューターというものを変更することで様々な実行環境で動作可能になっていてもですね 非常に魅力的かなと思いますローカルをはじめとしてですね 例えばスラームですとかあとスカイパイロットと呼ばれるですね インフラの抽象化レイヤーでありますところでですね Kubernetesでありましたりクラウド上での実行というものができるかなという風に思いますはい ここからですね 冒頭でも述べさせていただいたように独自アーキテクチャをですね 実際にニモでですねどのように継続事前学習するかという点をですねステップごとに分けてですね 紹介させていただければという風に思いますはい 全体のステップはですね このようになっておりまして最初にデータセットの準備からですね実際に独自アーキテクチャ等をですね どのようにネモでですね実装するのかといった点を踏まえまして実際にですね あるモデルをですねのパラメータをインポートして 継続的に事前学習するという方法と実際にですね ニモ上で並列分散学習のですねパイプラインパレルですとか テンサーパレル等を設定して学習を実行するという風な形になっておりますはい まず最初にですね データセットの準備について説明させてください実際にですね ニモで学習する場合はですねデータをJSON L形式で キー名をテキストとしてですねこのバリューに学習データ 実際にはですね 今日はとても晴れていますねという風な文章でありましたり そういったものを設定するような形となりますで このですね データセット自体をですね 前処理する必要がありますで その前処理のですね 形についてもですねニモのですね コンテナをまず実行いたしましてそこからですね プリプロセスデータフォーメガトーンというですねスクリプトがありますので こちらで実行します事前にですね 独自モデルのトークナイザーをですね用意するという風な形でトークナイザーのライバルが 例えばですね ファギングヘイス形式のものであればこのタイプにですね 実際に独自モデルの場合はですね ローカルのパス等を指定して設定いたしますそうしますと 前処理後のデータですね.binaryと.idxというような形で前処理後のデータセットが必要化されますのでこれを実際に学習として利用する形となります次にですね 実際にモデルを実装するというところについて説明いたします一般的なですね 昨今のLLMのですねデコードオンリーモデルの場合はですね例えば右図のですねラーマ実装のようにですね 各種パラメータ等を調整するだけで非常にですね 簡単に実装するようなことが可能になっております例えばですね モデル内のですね ノーバイベーションであったりアクティベーションのファンクをですね 実際に調整をしたりですとかあとはこう ヒルーンサイズを実際に調整するですとかそういったものをするだけでですね一般的なデコードオンリーのモデルはそのまま動くようなものが実装可能となっておりますイメージとしてはですね 例えばファギングフェイスのモデルをですねNemoで動かすといったようなことを考えるとファギングフェイス形式のConfig.jsonのですね設定値とこちらのNemoのですねConfigをですね にらめっこして合わせていくような形で調整が可能となっております設定可能なパラメータ一覧はですね元のですね Megatron Coreの方で記載されておりますTransformer Configでありますとかモデルパラレルコンフィグ等を設定確認して実際に合わせていくという風な形となっておりますで こちらのようにですね一般的なデコードオンリーのモデルのものは説明したよりパラメータを調整するだけでですね非常に簡単に実装できるのですがそうでないケースももちろんありますそのケースもですね Nemoではですねサポートされております例えばですね Falconモデルのような形の場合はですねMegatron Coreでですねカスタムモデルを実際に自分で実装するという風な形になりますカスタムモデルスペックと言われるところではですね実際に使用するモデルのですねモジュール等を設定してですね変更する形となりますで 変更したモジュール等を利用して実際にフォワード処理がどのように走るかというところも実装する形で独自アーキテクチャのLLMというものをですね実装していく流れとなっておりますはい で 実際にですね今ある既存のLLMモデルのですねパラメータをインポートする処理について説明させていただきますこちらのですねIoモデルコネクタというものをですね継承したインポーターを作成するような形となっておりますこちらの例ではラーマー形式をですね対象として説明いたしますまずですね 最初一番目にあるようにファギングヘイス形式のですねラーマーのモデルからですねNEMOのモデルに変換するということを考えますとこのような形式でモデルインポーター等をですね実装していくような形となっておりますで 実際にですねパラメータですねPyTorchのStateDictのですねキーをですねHFの形式からNEMOで使用する形式にマッピングを作成して適用していくような形となっております左がですねファイリングヘイス形式で右がNEMOのですねキーに対応しておりますそれぞれDictとしてこのマッピングを作成して変換していくような形となっておりますでですね 単純にキーの2ネームだけでは対応できないというケースがありますのでその場合はですね 変換関数を追加で実装するという風な流れとなっております例えばこちらの例ですよGLUのようなものになるのですがTorchのですね コンカテネート等を利用して実際にNEMOで使うような形式に変換して継続的にですねパラメータを学習するというようなものは可能となっておりますはい で 最後にですねNEMOで学習を実行する場合はですね先ほど実装したインポーターをですね利用するいますのでインポートですね チェックポイント関数というものを実行して 既存のですね例えばファギングフェイスモデルをですねNEMOの形式に変換いたしますで そうしてですねトレーナーのチェックポイントにですね変換したNEMOモデルをですね指定してやることでですね継続的な事前学習が実行可能という風な形となっておりますで また補足としてですねNEMO等でですね分散並列学習手法を利用する場合はこのですねメガトロンストラテジーというものを利用いたしましてストラテジーを設定してテンサーパラレルとかパイプラインパラレル等の値を設定いたしますまたプレシジョンもですねメガトロンミックスプレシジョンというのを使ってですねこちらですとBF16のですね設定をして実際に学習が可能となるという風な形となっていますはい で ここからですね実際にこう冒頭述べたGPU over APの課題の長距離の通信についてのNCCAのチップ数についてご紹介いたしますはい まずNCCAというのはですねNVIDIAコレクティブコミュニケーションライブラリとなっておりましてマルチGPUのですね デファクトな通信ライブラリという風なものになっておりますこちらはですね 分散学習で利用されるオールリユース等の通信はですねこのライブラリで実装されるような形となっておりますインフィニバンドですとかイーサネットやNVリンク NVスイッチ等ですねの利用で最適な経路等が設定されて実際にマルチノード間のですね高速な通信というものがこのライブラリによって実現するという風なものになっております予備実験といたしまして実際に長距離RDMAでですねどのようなパラメータを変更すればですね効果が出るのかというところをですね検証いたしましたはい 環境としてはですね秋葉原と三鷹間のですね約40kmをですね我々のIon APN等で接続した環境となっております実験内容としてはですねRDMAのパフォーマンスツールであるIB-Lite VW等を利用して測定をいたしましたここではですね 長距離RDMAを受けるパラメータのですねメッセージサイズとですねもう一つQPサイズQPサイズのですねものをですね 調査いたします左のですね グラフが横軸がですねメッセージサイズを変更させたものになっていまして縦軸がスループットとなっておりますまず最初にですね メッセージサイズを変更する増加させることによってですね一度に通信をする量が増えますので遅延の影響というものが非常にですね解消されるというふうな結果となっておりまして実際に100Gbps程度のですね値が出るようなことが確認できましたはい で 右のですねQPサイズの変更についても調査いたしましてQPサイズをですね 増加させることによりまして通信の多重化というものが可能となると思いますので実際にですね 遅延差については多重化させることでですね通信のスループットというものが向上させるということを確認いたしましたここからですね RDMAにおけるですねメッセージサイズ QPサイズの増加がですね長距離に大きくこうですねスループット向上に寄与するということが分かりましたのでこれを踏まえましてニッケルのですね改善ポイント等もこちらで検証しております秋葉原とですね 三鷹間 先ほどの同じ環境でIBライトのですね 結果を踏まえてニッケルで関連性のあるパラメータの影響を検証したというふうなものになっております事件内容としてはNCCAテストを利用いたしましてニッケルにおけるですね先ほどのメッセージサイズと関連のあるニッケルバッファーサイズというですねパラメータとQPサイズと関連があるニッケルIB QPSパーコネクションコネクションごとのですねQPサイズを変えるというパラメータを変化して調査いたしました左がですねニッケルのバッファーサイズを変更させた例となっておりましてこちらはですね バッファーサイズを増加させることによって実際にニッケルテストのですねスループットも向上するということを確認いたしましたまたですねQPサイズと変更と関連があるニッケルIB QPSパーコネクションもですね右図のように変化させて増加させていきますと実際にニッケルテストでのスループットもですね向上するというふうなことが確認できましたこのようにですね長期RDMにおけるニッケルのスループット向上というのですねパラメータ変更することで向上するということを確認いたしましたこれらの結果を踏まえて実際にLLの学習でもですね適用させていきましてスループット向上を改善したというふうなものになっておりますここからはですね実際のLLの学習の実証実験の結果について報告したいというふうに思いますこちらがですねGPU over IPの実行環境となっております左からですねデータセンターの川崎 秋葉原 三鷹を設定しております川崎とですね秋葉原間はですね約25キロの距離となっております秋葉原と三鷹間はですね約40キロの距離となっておりますそれですねIon APNの100Gbpsの回線で繋いだというふうな形になっておりますまず最初にですね2拠点で分散した場合の結果について報告したいと思います秋葉原間のですね単一データセンター内のこの黒色の矢印のですね通信とですね秋葉原 三鷹間のですねAPN経由の通信というものを比較したような形となっておりますはい こちらがですね2拠点に分散させた結果となっております我々の弊社のですね独自のLMアーキテクチャであるつづみのですね7ビリオンの事前学習を用いて検証いたしましたNVIDIAのですねH100を2台使ったエンディノードでですね実施しておりますで 処理完了のですね所要時間をですね実際に計測してその比をですね比較したという風な形となっております秋葉原内でトロドメーターですね単一データセンターの通信と秋葉原三鷹間のですね分散データセンターAPNを経由したものとですね分散データセンターの比較として実際にAPNの区間のところですねインターネット想定のTCPの通信として置き換えたものの3社で比較をしております単一データセンターのですね学習の所要時間を1とした場合ですね分散データセンター我々のですね定所数GPオーバーAPNのですね学習時間は約1.005倍という風な形でほぼほぼ互角のようなですね性能でですね学習するというところが検証結果として分かっておりますまた一方ですねAPNの部分ですねインターネット想定のですね待機制限をかけたですねTCP通信と比較いたしますと約ですね4.7倍という風な形でですねやはりこの実際にですねデータセンター化をつなぐ通信の品質というものがですねGPオーバーAPNのようなケースでは非常に重要になってくるといったようなものがですね分かるかなという風に思います続いてはですね先ほどの2拠点から3拠点に分散させたようなですね結果を紹介させていただきます1拠点内の単一データセンターの通信はですね左側にあります川崎内でですね実際にH102機をですね3ノードに分けて実験いたしました一方ですねGPオーバーAPNについては川崎 秋葉原 三鷹間のですねそれぞれ3拠点をですねAPNでつながり合うもので実験したという風な形となっておりますはいこちらがですね実証結果となっております先ほどの2拠点と同様にですね我々の駿沼ナビリオンのですね事前学習を用いて検証したという風な形となっておりますこちらもですね単一データセンターの学習の所要時間を1とした場合ですね分散データセンターGPオーバーAPNの結果はですね約1.1倍とほぼほぼ互角のようなですね結果が得られました一方でですねこのAPNの通信部分をインターネット装填のTCPとして置き換えた場合はですね約9.2倍の時間がかかるということでですね非常にIonAPNのですね高品質な低遅延なですね通信というものが重要になってくるかなという風に思いますはい最後にですねまとめさせていただければという風に思います我々は本発表でですねGPオーバーAPNによる分散データセンターについて紹介提案させていただきました実証結果においてですねIonAPNの高速で大容量低遅延の特徴を生かして分散データセンターのですね実用性や有用性が確認できたかなという風に思いますGPUクラスターをですね複数のデータセンターに分散配置することでですね単一のですね例えば電源設備ですとか土地のようなですね制約の問題や課題を解決して柔軟かつ効率的にリソースを利用するという風なことができるかなという風に思いますまたですね今回利用させていただきましたNVIDIAの生成AIソフトウェアストックNIMOやメガトロンニッケルのようなですねものを使いまして大規模でマルチノード学習におけるスケーラビリティをですね今回確認できたかなという風に思いますまたですねNIMO等を使いまして独自のアーキテクチャであるLLMの実装等ですね実際に継続事前学習を行うといったようなプロセスも含めて非常にですねクイックにできたかなという風に思いましたはいNIMOでですねアウトオブザボックスで簡単にですね生成AIを実行できるという点はですね非常に魅力的かなという風に思いました以上でですね本発表を終わりにしたいと思いますありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございましたありがとうございました