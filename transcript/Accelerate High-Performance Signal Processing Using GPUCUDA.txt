 Hello all, I'm Anders, I work with Signal Processing Architecture for Demanding Sensor Applications. And I'm Jacob, and I design signal processing and other sensor applications. We will today present our work on using GPUs for radar signal processing. We are excited to be here at GTC and share with you our experiences from accelerating high-performance signal processing by using GPUs and QDAM. We represent the company's job and let us first show you a short introductory film. Thank you all for today! I love all the information! I can show you how to support all thecatminton and camera phones. As we often met on the file, we aprovechar a few to sure everything yes to use on a remote- parameter and our Finished Emma Amelia employment studio app. We alsologies Host of Literally� Galax Promed wieder a tour. Klavierありがとうございます! We have immediate help! We have a great whole dynamic way to Elohimцию site. We got it. So what is the background of this work? NVIDIA and Saab has teamed up and worked together since spring last year, where Saab has used NVIDIA technology in two proof of concepts. If you're interested in using large language models and how to go from clicks to conversations, we can really recommend our other session at GTC. So why have we made this signal processing proof of concept? The world is changing and we are facing a new reality. One example of this new reality is swarms of closely spaced drones. These drones should of course be detected as early as possible. We must therefore have a radar that can detect with both high resolution and high sensitivity. This drives the need for compute power for the signal processing. And the question is how a GPU will perform in such a setting. So what have we done more precisely? Well, the intent has been to switch our signal processing from a CPU-based to a GPU-based system with a focus on computational engineering efficiency. And remember that this is a proof of concept, where a new and small team has taken new technology and combined it with one of our existing products, and supported by guidance from NVIDIA managed to show great results in a very limited time. So the stories we tell today is how we have been able to boost the performance of current signal processing in just a few weeks, and also how we can enable new signal processing possibilities in order to meet that new reality I mentioned. Why are we looking at GPUs now? GPUs have an inherent suitability for data-intensive calculations, like those in our radar signal processing. And we have actually tested GPUs for this before, but then they showed not being optimal for us. It required much engineering effort and detailed hardware knowledge to truly utilize the performance potential. But the GPUs have evolved and are now promising. They have much higher performance potential and they have a technology stack that obstructs the hardware details. We will therefore make this proof of concept to get answers on if we can get both high computing efficiency and high engineering efficiency. But before we go into details, let's have a short introduction to radar applications. First I will briefly explain what a radar is. Then I will give you a hint of the signal processing challenges that may occur in high-end radar. And finally in this introduction I will give an example of a multi-channel radar signal processing application with its characteristics and constraints. The radar is an acronym that stands for radio detection and ranging. Some major application fields of radar systems are air and traffic surveillance, weather observations and ADAS for increased traffic safety. Here we look at so-called pulsed Doppler radar. It operates with pulsed transmissions to detect targets that measure the range, angle and Doppler velocity. Typical main components of a radar system are shown in the figure. To the left we have the antenna which transmits pulses and receives echoes. The raw received data is fed into the signal processing which extracts detections from the data. Then the target tracking keeps track of the detected targets which finally are presented on the display. In this proof of concept we focus on the signal processing. This stands for the overwhelming part of the computational performance requirements. What kind of signal processing challenges might there be in a high-end radar? High-end radar systems could be based on so-called AESA antennas. AESA stands for Active Electronically Scanned Array. The antenna reminds of a honeycomb. It consists of hundreds of thousands of antenna elements. It can be fully digital with an AD converter for each antenna element. It works like this. The relative phases of the waves from the different antenna elements are set to create different beam directions using interference without any moving parts. The corresponding thing can be done in reception then creating different beam directions or looking directions. AESA makes very powerful signal processing and system operation possible. But this comes with a cost. The massive amounts of data from antenna means a huge leap in the computational performance requirements for the signal processing. The leap could be several orders of magnitude. But this is not enough. The signal processing should still fit in the same box as for traditional signal processing with the implementation constraints that follow. The implementation constraints for embedded processing are in terms of size, weight and power. And there are also requirements on cost. These constraints are together represented by the acronym SWAP-C. The data is also a reason why GPUs are so promising. If they can deliver a significant increase in performance. So what does a typical signal processing application look like? Radar data arrives as a stream of matrices of data samples. The matrices are relatively small but arrive at a high rate. They are grouped into so called data cubes which are fed into a chain of filters as you can see in the figure. The filters in the chain are digital beamforming which creates beams or looking directions from the input data. The dominant calculation is typically matrix multiplication. Then we have pulse compression which is a match filter that increases the resolution in range. This is often done using FRR filtering. After that is Doppler filtering which increases signal to noise ratio and measures Doppler velocity. And FFTs are mostly used here. Finally, Anyway, We have CIFAR detection which detects target returns against the background of noise and other interference. This is done using a sliding kernel in the dataset. Note that the datasets that the filters operate on are often small in relation to the number of CUDA cores on GPU. And also note that the processing direction in data may change from one filter to another as indicated in the data cubes in the figure. There could also be non-functional requirements on the implementation. For large antenna arrays there could be a throughput requirement of several terabits per second. And short system feedback times require the end-to-end latency to be less than one second. After this introduction of radar signal processing, Jacob will now tell you about the results from the actual proof of concept work. And now I will guide you through what we have done for the past few weeks. Me and my team have together worked on this proof of concept and we have worked with a variety of tools. For low-level linear algebra calls, the libraries Qsolver, QBLAS and QFFT were used. Qsolver and QBLAS contain many low-level algebra calls that are optimized in performance to utilize the GPU to its capacity. QFFT on the other hand contains different approaches to an implementation of the FFT algorithm. We also had cases where we required custom functions and they were written in QDA directly to utilize the GPU. When we initially started the proof of concept, our first target hardware was the A100. This was the best way to start since we had quick access to the card on the site and as of now it is only outmatched by the H100. But as the proof of concept continued, we were able to gather more GPUs for evaluation, such as the RTX 5000 ADA, L4DS and the RTX 6000 ADA. Since we work with antennas that operate in various conditions and terrain, we are limited to work within an embedded environment. As a consequence, we have to consider the limited power consumption, cooling and durability when selecting hardware, and not just maximizing overall calculation performance, as Anders mentioned with the SwapZ concept. In short, the A100 shows the fastest calculation for 64-bit required algorithms, and some algorithms require 64-bit calculations in order to not lose precision. The A100 also exceeds the other listed GPUs by far when comparing memory bandwidth. That allows for more data to be transferred between the GPU cores and the GPU memory per clock cycle. The downside is that there exists no embedded version of A100 as of now. The L40s was the second fastest for 64-bit calculations after the A100. Similar to the A100, the L40s does not exist in an embedded version, and has an even higher watt consumption than the A100. The RTX 5000 ADA has fast 32-bit calculations, in some cases even faster than the A100 and the L40s. The RTX 5000 ADA is a strong contender since it has an embedded version, but it should be noted that the embedded version has fewer CUDA cores. The RTX 6000 ADA is slightly faster than the RTX 5000 ADA, but unlike the RTX 5000 ADA, it offers no embedded version as of now. So, what happens if we use a GPU in a signal processing context? I think it can open up many new possibilities. Let's take for example systems that today use several CPUs to accommodate processing requirements for high data throughput. The GPU in such a system could allow for a reduction in CPU boards. We also have systems with single CPU boards. In those cases, the latency could be reduced due to faster processing speeds. We can also assume that this leaves more room for extended CPU functionality in other radar-related applications, since the signal processing tends to use a lot of CPU resources. And the implementation itself has so far proved to increase the engineering efficiency. Considering that this was developed within weeks in a small team consisting of two to three people with no previous CUDA experience, the results are promising. We should also consider that working with one GPU and one CPU reduces complexity, both for hardware design and for the software engineer, when the alternative is to use a multi-CPU solution to tackle processing speeds. We also wanted to prove the GPU solution further by integrating the GPU server with a real antenna. And these tests showed excellent results in both latency and target precision. And these tests showed the CPU that could be used as a computer system. So, the actual signal processing that was implemented on the GPU consists of a batched Doppler filtering pipeline together with a CIFAR at the end. The current CPU version was used as a reference implementation. These Doppler filtering steps and the CIFAR were developed using CUDA. And QFFT was used for the FFT function. The CUDA code was integrated within an existing codebase with ease. That shows great compatibility since it did not require a reconstruction of the entire codebase in order to move certain parts of the signal processing to a GPU. For this test, a server with one multi-core CPU and one RTX 5000 ADA GPU were used. This setup was first developed and verified using simulators in replacement for the antenna. The same server was then connected to a real antenna in order to be verified with real targets. Using the RTX 5000 ADA gives a 4x speedup in total from the current CPU that is available in an embedded context. The speedup is only limited due to being in a current codebase that requires a lot of data to be copied between the GPU and CPU. If the design was to change, for example, moving more functionality to the GPU in order to avoid copying back large data structures to the CPU, the speedup would be more than 10 times faster. The final steps to extract the target are still implemented on the CPU, and those final steps require multiple process batches to extract information. The GPU can easily do the signal processing in real time and free a lot of resources from the CPU. This leaves more room for other software applications more suited for a CPU to be run with extended functionality. It is also possible to use cheaper CPUs, and this is because the computing requirements are mainly set by the signal processing application. And the signal processing could also be extended due to more computing power on the GPU. The GPU is also so fast that it generally has a longer life cycle before the processing demands hits the limit. Of course, this also means that hardware upgrades can be less frequent and still meet future requirements. And the fast algorithm processing also reduces the latency for when the targets are processed and presented. This shows that it is possible to increase performance by using GPUs, without rewriting the entire code base. As a result, the engineering complexity is reduced in terms of both hardware construction and possibilities to get more demanding algorithms running in real time. To achieve the same results on a CPU, it would require multiple CPUs in such a scenario, and therefore increase the design complexity. The next step was to try out a new, more demanding algorithm. The next step was to try out a new, more demanding algorithm. We have to adapt to the new reality, and this requires new solutions. And in our case, we are interested in detecting closely spaced drones. This requires algorithms with higher resolution capacity in order to distinguish between the drones. And for this drone scenario, the SPRI algorithm is of interest. However, this algorithm is known for being compute-intense. Fortunately, it can be calculated in parallel to increase computational performance, which makes it perfect for a GPU to accelerate such performance. The SPRI algorithm has been around since the 80s, when it was first published. It is known to be computational-heavy to calculate in real time, until now, at least in an embedded context. We had a great start, since the majority of the required steps for the algorithm was already available in NVIDIA's Qsolver and Qgloss libraries. There were also some more custom steps in our SPRI implementation, and they were implemented directly in CUDA. But it became clear that a general eigensolver function was missing in the libraries. We then requested such a function from NVIDIA, and they were able to provide it, which got us going again. When running the algorithm on the GPU, it outperformed the CPU implementation with a 10x performance increase. This was a great achievement, and it confirmed our initial faults. All our GPU hardware candidates showed great real-time processing results. Processing multiple batches in parallel significantly improved throughput. The A100 delivered the highest performance for large-scale parallel tasks. On the other hand, the RTX 5000 ADA had a balanced cost and performance, and also excellent for real-time processing. The L4ES is specialized for certain workloads, making it a strong contender for ESPRI. In general, this implementation demonstrated consistent performance with batch processing across all GPUs. The algorithm showed such great results on the GPU that we also decided to try it out in a real antenna, with similar great results. The major difference between GPU execution time seemed to be how fast they were able to calculate 64-bit precision. The A100 clearly excelled in this regard in comparison to the others. In summary, real-time processing was achieved, proving feasibility for ESPRI 1D. And for the next steps, we could explore further optimizations and new development opportunities. We could for instance explore possibilities to simulate 64-bit calculations using 32-bit precision. We could use MATEQs to enable more engineers working with these algorithms on the GPU. As you know, we're always looking for ways to push the boundaries of what's possible with sensor data. That's why we're excited about the NVIDIA Holoscan. We're planning to put it through its paces and see what it can really do. And what makes Holoscan so compelling? First and foremost, it's built for speed. Holoscan leverages the processing power of NVIDIA's GPUs, enabling it to analyze vast amounts of sensor data in real-time. You're not limited to a single deployment location. Holoscan is designed for both edge computing, processing data right where it's captured, and cloud deployments. Giving you the flexibility to choose the best solution for your needs. Holoscan provides a complete end-to-end solution and offers support for the popular languages Python and C++. Along with a robust SDK packed with tools and libraries to streamline your workflow. It embraces a bring-your-own approach, allowing you to use your preferred sensors and AI models. And we're eager to explore how these features can be applied to our contexts. Right. Today we have shown you how we have gone from a CPU to a GPU-based radar signal processing in a real system in just a few weeks. Yes. And we have also shown that the GPU-based system has given us significant performance improvements, with possibly more to come. Since it also opens up for new signal processing opportunities that have previously been out of reach. We have also seen signal processing hardware complexity can be substantially reduced. In summary, we can say that GPU-based signal processing makes it possible to squeeze out very high radar detection performance from a given swap-see. And this is very important if we look into the future. The new fully digital many-channel radar systems together with advanced signal processing enable unprecedented system performance. But this comes with extreme processing performance requirements, which also will scale up over time. At the same time we have the strict swap-see constraints. We can talk about putting one Pita-Flops in a shoebox. This is a challenge we must deal with for the future. Thank you for listening. Thank you.