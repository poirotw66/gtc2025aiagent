 Now let's get started with our session. Edge Computing 101, Introduction to Smart Edge and Autonomous Robots. Our speaker today is Chen Su, a Senior Product Marketing Manager at NVIDIA. He leads the product positioning and go-to market for NVIDIA's edge computing platform, NVIDIA Jetson and IGX. He's going to introduce Smart Edge and Autonomous Robots, and we are looking forward to his talk. So, thank you so much. Hello. Perfect. Good afternoon, everybody. Welcome to GTC. Woo! We are super excited to have you here. I know people travel all around the globe to come to Santa Clara this year, and we have an amazing week ahead of you. And my job in this station is to give you the introduction for edge computing. Specifically, I want to talk about the trend that we are seeing today in the market, the challenge that we hear from customers, building AI-based applications at the edge. How NVIDIA are working with our ecosystem partner to provide a solution to help them. And eventually, if there's any use cases that are going to excite you, I will bring some of them to you. Okay? So, before we jump into the topic, I do want to highlight some, read my audience a little bit. So, I want to know, how much do you know about edge computing and robotics? If you are early phase of learning the concept, please raise your hand. Okay? Probably, I would say, 40% of the audience. How many of you are researching which use cases makes sense for edge computing? You are kind of knowing, okay, that's probably 20%. How many of you are currently evaluating the solution, if you don't mind raising your hand? That's probably down to 10%. Then, implementing solutions in production. Who is doing that? Okay? Another 10% of the audience. That's perfect. So, it seems like we have a room of people. Majority of you are looking forward to learning the concept of edge computing, trying to figure out what it is. And the rest of people have a little bit of an understanding of the concept. We like to go deep into it. So, my talk will serve all of you because I will first build the foundation for edge computing, introduce the benefits. And then, after that, you're understanding some latest use cases and the trend of technology, as I said. And then, for the people who are actually working on it, you must face some challenge. I will share with you some common challenge that we see today. And then, some NVIDIA's edge AI solutions. At the end of the day, I will give you some GTC resources. We have amazing sessions, and we plan for everybody. So, no matter you are in trend level, you are, at this moment, trying to learn from the best practices of the industry leaders, you can enjoy those sessions. So, that's my agenda today. Let's jump into it. So, first and foremost, what is edge computing? So, different from, we are taking a look of edge computing. If you don't just think about it, you have to mention cloud computing first because that's everyone is familiar with. Today, you're browsing the web page online. You are using ChatGDP to ask some questions. You are doing a lot of those things to, you know, YouTube's video searching, things like that. It's all coming from the cloud, right? You have a server somewhere, locate coverage is global because those server or data centers are located around the world. You don't need super fast response time, but this has to be reasonable because you don't want to open a web browser that takes you one minute. So, the average response time is millisecond, but sometimes minutes or days depending on your workload. If you are doing communication for a large number of files, then that will take a long time. You do require a large amount of bandwidth because the communication from where you are to the cloud, you know, is pretty long. And also, you probably have massive data you need to upload and download. And that is cloud computing. So, on the other hand, we are willing to introduce you to edge computing. So, edge computing brings the computation closer to network edge and where the data is gathered. And that's usually, if you think about it, it could be a small embedded devices that somewhere on, let's say, a robot is a robot controller. This could be IPC that control your industrial manufacturing floor. This could be a small edge server that put on your office, in the office, right? You want to storage your data over there, provide access to all your employee, to some tools, and you use and share those resources. So, the response time usually is very fast because most of the cases, you are dealing with mission-critical application or you're trying to get the data and get the compute result very, very fast to you. And hence, the compute happening at the edge, it significantly reduces the bandwidth. So, you don't need to send all your data to the cloud. Probably just some insight to generate the dashboard result for business analytics. That's it. So, the benefits of edge computing is basically to help you reduce the latency, right? It's have an ultra-low latency compared to cloud computing. And it's also have reduced bandwidth. As I mentioned, you reduce data transit and the storage cost. So, you no longer need a lot of storage at the data center level because you filter the data first and you send the data afterwards. So, as a result, your data center can save a lot of storage. And it also allows organization to add more sensors into your end application because you can perceive a lot more information at the edge. And another reason or third reason that people often consider edge computing is because they really care about their privacy of the data. People often say data sovereignty is super important for them, especially for government, for application that's related to medicals, healthcare, right? People don't want those private data uploaded to the cloud. And so that if you have edge computing, so you ensure the data stays with you and it's all private and you can protect your intellectual properties over there. And in the age of AI, that edge computing combined with AI inference, that superpower, really give you not just the compute as you normally do, like you can actually have the AI inference happen as long as the compute resources have at the edge. That means you can inference from the data, like the text data, no matter it's PDF, or your image data, video data, image video can be coming from a manufacturing line, can be coming from a security camera. And all of those data come together to give you the real-time insights to help you make better business decisions and achieve higher value. And at the end of the day, with edge computing, what you can really accomplish is you deliver actionable, real-time insight and intelligence with continuous improvement. There's a small glitch. When I click this, but it's all okay. And we have a different type of edge computing. And one thing that I really want to highlight is those edge computing are used in the broader terms. A lot of people from different industries may hear something different. The first thing that I want to highlight is provider edge. That's basically the telecommunication networking tower that you see over there and also could be the provider edge that come to your home. And those are designed to do content delivery, gaming, AR, VR application, or AI as a service. And the second edge computing is enterprise edge. So you have basically edge server put somewhere in the warehouse or your office. So you have intelligent warehouse. You have micro data center over there. You have remote office branches. You have smart retail store that you put something over there so that you can have the computer resources to aid your business. The third one is industrial edge. And we have a lot of those applications today in the warehouse, logistic center, in the factory floor. And also we have considered medical devices as industrial edge because most of the time they need safety certification. And some of them need the ruggedization for sure because you put it in a remote area or in a wide temperature range area. So you definitely need that type of protection over there. And lastly is embedded edge. It's ubiquitous. So you, from the early days that when you were a student using a calculator, it's embedded edge devices. To today, you have robots that's moving in the environment and in your factory. Or today, you're building a human or the robots. And from the drone that you play with and use for inspection for the bridge and stuff like that. And from the traffic interception where you put traffic control devices over there to guide the traffic by perceiving the camera information to give you a real-time control of all of those. Those are all embedded edge devices. And they are super, super tiny and small. They're embedded into the devices to provide you much instant response to whatever the sensor is perceived from the environment. And what I really want to see is edge computing and robotics is everywhere. If you look at the use case among verticals, this gives you a pretty good understanding of where you can see edge computing use case looks like. So we have transportation logistic. Today, we have some customers building AI-based avatar into a digital signage to improve the customer experiences. This can guide people in the retail store in the big mall. You see a security camera. You see all of those type of things you implement in a logistic warehouse. And for industrial application, you see AI-based inspection. You see robot arm that is powered by AI that can look at the thing that they want to pick up and place it into the right location. And you see materials handling. You see shipment in the warehouse from point A to point B. And in the smart retail, you see automated checkout where you don't need to scan the barcode. We have an application where you just need to scan the objects to detect what it is. And you just check out. And you can see agents that help you look at the store traffic, understanding what's going on, where people want to shop by hours. Then you can understand what happens in the retail store. And you have a smart city application that a lot of customer or city administration use that to check out your license plate, check out the traffic in certain area. And it's doing also smart parking application where when you park the car, you don't necessarily to go there, use the metering. You can just, they can just read your license plate. And then you have the online account and it's just charged you based on what you have, save time and save your hustle. And in healthcare industry, it's life-saving. So we see customer-building AI-based endostropy, medical imaging. And you can also see customer-building AI-based surgical robots that can aid surgeons to perform better precise surgery. And last but not least, we have real-way application as well. We are a lot of customers today that they put edge computing devices to help inspect the railway in the real time as it's operating. And also looking in front of the railway to do the track inspection and see if there's any obstacles going into the track to prevent things from happening in real time. So all of those prove that edge computing is happening everywhere. However, this year is very particular. We are seeing a tipping point for edge computing powered by generative physical AI. So before I jump into that concept, I want to give you a history view of what's happening in the industry of AI. You are seeing in 2012, this is the first time AlexNet is available and we as a human can finally classify a cat from everything else. And soon we jump into perception AI where we can use a lot of perception-based models like classification, object detection, natural language processing to perceive the information that we are seeing. It just gives you a result like, oh, this is what it is. But it's not generalizable. And just about like three years ago when chat GDP happened, we know that the industry come into a completely different era, which is powered by generative AI. We are seeing the first time human can prompt and talk to AI to generate what you want. And it's not limited to text-to-text. It could be text-to-image, text-to-video, protein structure into text, any format. And in about a year ago, we are seeing a trend of not just using one large language model. You have a few different models that work together. Could be a large language model as a dispatcher to talk to a vision language model. That vision language model work as a vertical expert to give back the information. And you have multiple agents that work together like a human worker. And those are agentic AI. And as of this year, we are seeing those powerful generative AI model coming to the edge. Because we have the compute capability. We have the model, our foundational model large enough. And we can fit them into smaller devices that one day you are seeing every physical device is going to be as smart as what you are seeing for chat GDP. And I have proof point because a lot of our robotics customers, especially in the human noise space, they are testing, validating vision language model, vision language action model, where you can actually give a prompt to the robots and robots can take action immediately. And we are seeing this going to come to the edge. That means billions, trillions of devices are going to be smarter than ever. And we are in the new age of edge computing. And there are a few technology trends powered by generative AI. I want to see them one after another. The first one is obvious AI powered robotics. I talk about traditional robot arm. I talk about AMR that you can see from Amazon Robotics deploys in their warehouse. And today we are seeing humanoid robotics become very, very popular. It's a hot topic in Silicon Valley and all over the world. And you also see in the traditional industry like agriculture, John Deere built and delivered their autonomous tractor that helped farmers to be more efficient. And AI powered robots will be everywhere. And this is what we call inside-out physical AI. What does that mean is you put the compute or AI model into the robot itself. So it's performed outside to give that action, deliver that result. And the second trend that we are seeing here is called Genetic AI. Genetic AI last year was a concept for the cloud, a lot of company building application. And today we are seeing a lot of vertical AI agents. And they are coming to the edge. For example, you have a traffic AI agent that can help you give an incident report, finding out what's going on in your traffic interception, to directly give administration human readable insight. You have a customer service agent that when you order food from a drive-thru, it can talk to you like a human and help you order different type of food and even customize menu. You are seeing a hospital monitoring agent that when you don't need so many people in the ICU, and you can let the agent help you monitor what's going on. If a patient just falls down to the ground, it can give you immediate alert. And you don't need to program it. The agent being able to listen to the human language, you can prompt it to send personal alert. Lastly is any vertical. You're going to see a ton of AI agent application powered by what we are seeing today. And because it's at the edge, most of the application we see perceive the information from camera. That's most common. Later, radar, sometimes you need to overcome the fog, the situation where the camera just cannot view that far and use that level of precision. So a lot of the sensor data will come into the edge and use AI agent workflow to gather and give you more insight. Last but not least is real-time sensor processing. That becomes super critical for what we are seeing of today's application, especially in the field of edge HPC and medical imaging and factory defect detection. All of those are the trend that we are seeing, and we are seeing customers are building the new infrastructure at the age of physical generative AI. And you might wonder at this moment, already, I have so great technology trained over there, but how do I, how can I build those applications? What is the tricks? What is the necessary technology needed to build the application? And NVIDIA provides you three computer solutions for physical AI. That's different from what you are seeing today on data center. For data center, you need a data center computer. You train your model. You do pre-training. You do post-training. You do long thinking. You do all of those. It's all happening in data center. But edge is different because you have to deploy in the physical location. It has to be safe. It has to interact with people that are very, very hard problems to be solved. That's why NVIDIA provides those three computer solutions. First, you do have to have a foundation model coming from data center, powered by NVIDIA GPU. And then you move to the model, test, verify that in a simulation environment. That simulation environment needs to be photorealistic, needs to be physical accurate, and it has to mimic what's going on in the real world so that you save your time, energy, and materials building before you deploy it into the edge. And that gives you tremendous value because you can verify a thousandth time of what's going to happen in the simulation without actually building it and testing and face that failure and then redo everything from scratch. That means faster time to market. That means higher safety standard coming from that type of solution. Lastly, you have that model that's validated in the simulation, powered by Universe and Cosmos. You put into the very powerful edge computing devices for inference. So no matter which form factor is that devices, you should be able to use that to build a solution. And once you test and you probably see more data, you can send the data back to the data center and improve your model and that process iterate. And NVIDIA providing you different edge computing platform and specifically software application framework that help you tackle inside-out robotics problem, outside-in physical AI, which are visual-based agent AI problem, lastly, sensor processing problem. We give a name for different software application framework. Number one is NVIDIA iSEq. How many people hear about NVIDIA iSEq before? Pretty good. I have 70%. You all know that iSEq is NVIDIA's robotics platform to build, you know, simulation, iSEq sim, reinforcement learning environment, iSEq lab, and also tools that comes to the edge, which is powered by raw space framework, and you have the computer, of course, to tackle that. And the second is NVIDIA metropolis. How many of you have heard about metropolis before? Oh, just a few, probably 10%. Metropolis traditionally is used for just doing vision AI application, and today we are seeing that type of application become even more powerful, and we are evolving metropolis framework to be able to handle virtual-based agentic workflow, such as video search and summarization, things like that. And the third one is NVIDIA holoscan. How many people heard of holoscan? Perfect. That's like 20%. So holoscan is NVIDIA's application framework for sensor processing, and a lot of people coming from medical field first get the benefits of using that because medical imaging need ultra-low latency. Like many times it's below like 30 millisecond, very fast, very fast, from glass to glass, and you have to not just perceive the sensor data, but you have to couple with AI compute for inference. So NVIDIA providing those framework to help you, and my goal today is to give you a little bit introduction for each of those framework. And number one, let's jump into NVIDIA ISIC. I have a video that's going to help you understand how it works. The era of robotics has arrived. One day, everything that moves will be autonomous. Researchers and companies around the world are developing robots powered by physical AI. Physical AIs are models that can understand instructions and autonomously perform complex tasks in the real world. Multimodal LLMs are breakthroughs that enable robots to learn, perceive and understand the world around them and plan how they'll act. And from human demonstrations, robots can now learn the skills required to interact with the world using gross and fine motor skills. One of the integral technologies for advancing robotics is reinforcement learning. Just as LLMs need RLHF or reinforcement learning from human feedback, to learn particular skills, generative physical AI can learn skills using reinforcement learning from physics feedback in a simulated world. These simulation environments are where robots learn to make decisions by performing actions in a virtual world that obeys the laws of physics. In these robot gyms, a robot can learn to perform complex and dynamic tasks safely and quickly, refining their skills through millions of acts of trial and error. We built NVIDIA Omniverse as the operating system where physical AIs can be created. Omniverse is a development platform for virtual world simulation, combining real-time, physically-based rendering, physics simulation, and generative AI technologies. In Omniverse, robots can learn how to be robots. They learn how to autonomously manipulate objects with precision, such as grasping and handling objects. Or navigate environments autonomously, finding optimal paths while avoiding obstacles and hazards. Learning in Omniverse minimizes the sim-to-real gap and maximizes the transfer of learned behavior. Building robots with generative physical AI requires three computers. NVIDIA AI supercomputers to train the models. NVIDIA Jetson Oren and next-generation Jetson Thor robotic supercomputer to run the models. NVIDIA Omniverse, where robots can learn and refine their skills in simulated worlds. We build the platforms, acceleration libraries, and AI models needed by developers and companies. And allow them to use any or all of the stacks that suit them best. The next wave of AI is here. Robotics, powered by physical AI, will revolutionize industries. NVIDIA Jetson Thor Perfect. I think you enjoy Jensen's narrative. And as Jensen explained, NVIDIA provides all the building blocks for you. And it's no matter the library or component, wherever you want to use, and even the entire workflow. NVIDIA provides those workflows in a way of different robotics form factor. If you're building AMR, you have IC perceptor that have all of the necessary components we glue together to help you accelerate your AMR application development. We have IC manipulator that build for AI-based ARM. And lastly, we have Project Root that build for humanoid robotics. So people can choose anything that they want and build the future robots. Right? Really cool robots. And we have a lot of sessions during GTC that you can hear from Google DeepMind talking to how they leverage those tools to build future robots. You can hear from various of humanoid companies building new humanoid skills with NVIDIA's IC Groot. And you can learn all of those here. And next one I will talk about is a genetic workflow, specifically for video processing. So I will do a video to give you that introduction. NVIDIA Analytics AI agents, built on an NVIDIA Metropolis blueprint, including NVIDIA Cosmos Nematron Vision Language Models, Lama Nematron LLMs, and Nemo Retriever. Metropolis agents analyze content from the billions of cameras generating 100,000 petabytes of video per day. They enable interactive search, summarization, and automated reporting. And help monitor traffic flows, flagging congestion or danger. In industrial facilities, they monitor processes and generate recommendations or improvement. Metropolis agents centralize data from hundreds of cameras and can reroute workers or robots when incidents occur. This is a short video, but it actually tells you what AI agents can do. And first of all, it can do real-time spatial AI. It can analyze and understanding the environment and the context. Second, it can inject video from multiple cameras. Lastly, it can understand human prompt. You can do Q&A. You can do even custom report generation from VOM Rack. And all of the data come to the agent powered by NVIDIA GPU. And that way, you build vertical-based agent to help you solve specific vertical problem. It could be a warehouse agent. It can be a traffic agent. It can be anything. And if you think about a metropolis, as I mentioned, it's not just for video analytics and vision AI anymore. It's the tool and the development platform for building AI-powered infrastructure. We talked about virtual AI agent just now, but you can actually use many technology that in the suite of development tools to build multi-camera tracking for worker safety and factory efficiency. And you can also build it for smaller scale automated virtual inspection use case. And when you coupled the inside-out ISIC workflow and outside-in metropolis workflow, if you think about the entire space that we are operating in, there's a very critical problem that we all need to solve is how to deal with those smart, intelligent, autonctal machine working with human. And that's why NVIDIA invests a lot of time and our engineering hours into providing safety-based test solution. And if we look at safety into four pillars. Number one is simulation predictive safety. So before you even think about implementing that safety algorithm, you actually first test it out in the simulation environment. And then it's come to the safety foundation where you build the safety protocols and functional safety into the devices. That's powered by special SOC and accelerators and also the functional safety island and also safety MCU that can be onboard into your physical machine. And you have a specialized software that's like safety extension package to enable safety diagnostic and safety communication. And after you finish all of those, you deploy the, the, your, your model onto the robots itself. It should also have AI based safety perception. So this can tell, oh, there's human come to me. There's some optical in front of me. It should be able to do safety stuff. Last but not least, there are situations where the robots just cannot see through things. For example, it's might have occlusion where you have two robots running in the corner. The wall blocks the view of the robots. Then you need to have the outside in safety to guarantee they can talk to both robots to make a stop. And that is what we call proactive AI safety. All of those technologies are important to empower the robots to work alongside of human without having any problem. Last but not least, I do want to also talk about sensor processing. It's become very important today, as I said, for those medical devices or mission critical application to have ultra low latency sensor processing pipeline. It just make very clear today when I, when we talk to all those customers that look at their application. One common problem they have is there are just so many different sensor data coming into it. There are proprietary protocol. There is a way that they designed to do it. It's just take too long to build that entire pipeline from sensor to compute. And, and one more thing that's coupled with that challenge is you also need to deal with AI. So AI needs to build into the pipeline so that the end to end is very fast as well. So NVIDIA have a Holoscan platform that provides you Holoscan SDK, reference application, and a sensor to compute, a sensor to internet technology called Holoscan Sensor Bridge to really help you accelerate that entire sensor to compute. And one thing that I want to highlight a little bit more is the sensor to compute. It's about Holoscan Sensor Bridge. So the Holoscan Sensor Bridge is essentially a bridge that converts your sensor data into ethernet. And you might have multiple sensors, no matter it's radar, radar, camera, any type of sensor data you need to send to your compute environment. To your compute devices. Today, as I mentioned, too many proprietary protocols. And the fast way that human build today is use ethernet, right? That's standard in data center, but it's not a standard at the edge. And ethernet is super scalable. You can design for one Gp per second, 10 Gp per second, 100 Gp per second, any bandwidth. And you can easily scale this to multiple devices, even tens, hundreds of devices using this technology. And this does not only deal with one way. It's not just perceiving the data. You can also transmit data back. So you can also control your actuator, such as motor, speaker, audio, anything. So with this technology, what you can accomplish is think about your future edge devices are mini data center. All your sensors located in different places can talk to the central compute through ethernet. It's super fast. And it's also solved the problem of system complexity in the compute level. So your compute carrier board, your compute design can be just having ethernet as your major communication protocol. That makes the whole thing much faster. And if you think about that, your entire hardware becomes software defined from sensor to compute. So as application developer, you just need to focus on building application on top of it. Without thinking about figuring out so many details and take months, even like years, to build your embedded application. So if you think about the entire application framework that we are talking about today, eventually they need to come to the edge. There's a destination for the hardware. And no matter what application framework I talk about, they support all NVIDIA edge computing hardware that I listed here. For underpress edge and provider edge, you have edge CPU like RTX, L4, L40s, that have the highest compute that you can possibly see, of course, because it's a beautiful data center and workstation. So you have a little higher power management that you need to deal with. But the application that it built for it will work for like a recommender system, computer vision, general TVI, raw planning, all type of thing. And when you come from industrial edge, you need that recognition, you need that long-term support, you need functional safety. That's where NVIDIA IDX platform come into place. It still have a relatively small form factor, but with a lot of computing in it, up to 1705 tops. And you can build applications such as healthcare, medical, image devices that are listed here, manufacturing inspection, proactive safety, high-speed sensor fusion, all type of application that you can see. Last but not least is embedded edge. NVIDIA JASM platform is designed for embedded AI application, where it has a lot of different form factor and a range of products from different memory size you can select from. And the power is super small. It's only 7 to 65 watts. So it's super power efficient to deliver a very high AI compute, up to 275 tops. So with that, you can build applications such as a computer vision box, an autonomous structure, AMR, inspection camera, and it can be any form factor that you can imagine. And when you think about those edge applications, you do want some sort of good software to protect you, to give you the long-term support, to manage your infrastructure, to help you, guide you through the AI development. That's what we call NVIDIA AI Enterprise. NVIDIA AI Enterprise today works on top of NVIDIA edge computing platforms such as NVIDIA EGX, Enterprise Edge Computing Platform, and NVIDIA IGX. And if you think about NVIDIA entire edge edge stack, if I put it from top to the bottom, this gives you a snapshot of what we discussed today. It starts from application framework. I emphasize today on Metropolis for Vision AI, i6 for robotics, and Holoscan for sensor processing. But essentially, a lot of the application framework that you see as a cloud, you can be leveraged at the edge. And it also has the layer of AI development and infrastructure management powered by NVIDIA AI Enterprise. And at the end of the day, we have a partner that works with us to provide in-depth management system, and all of those products should work on top of our NVIDIA's hardware platform. And we specifically work with our ecosystem to provide EGX, IGX-based NVIDIA certified edge systems, so it works backs on NVIDIA Enterprise software. Lastly, I have one and a half minutes. I want to give you some guidance on what you can see in GTC, if you are interested in this type of topic. You can take a picture right now for edge computing and AI. Those are all the featured talk, ranging from industrial AI safety, sensor, computing, physical AI as reality, and also GintiGai workflow. You also have workshop if you are AI practitioner, you want to test it out. We also have physical AI and robotics sessions where we have featured robotics talks and workshops and trainings to help you gain in-depth understanding. Last but not least, if you want to work on computer vision, video analytics, and virtual agentic AI workflow, those are all the talks to help you. And don't forget to connect with the expert session. So if you are really building a groundbreaking application, we have NVIDIA expert and engineers are in those connect with the expert session, and you can talk to them for your problem. And I'm pretty sure you're going to find some good guidance on building amazing application for the future of AI. And with that, I'll wrap up my talk. Thank you so much for listening. Have a good one. Enjoy GTC. Thank you. Thank you, Chen Su, very much. Really great talk. Thank you very much. That concludes our session for today. That concludes our session for today. Thank you. Thank you, Chen.