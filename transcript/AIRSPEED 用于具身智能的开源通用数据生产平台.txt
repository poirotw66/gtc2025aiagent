大家好,我是来自于深圳市人工智能与机器人研究院的夏萱副研究员我今天演讲的主题是ISBID,用于聚生智能的开源通用数据生产平台我将通过以下五个方面来展开讲解首先是简单讲解一下ISBID的开发动机接着来探讨一下聚生智能的数据瓶颈并且简单的介绍一下聚生智能数据生产的现状然后是来介绍我们的ISBID平台以及相应的开源实例首先进入ISBID的开发动机首先我们对聚生智能的数据市场规模有一个简单的判断以互联网为例,当前的互联网用户有50亿那么假设每个用户的数据估价我们给出600美元总的估值将达到3万亿美元而对应到机器人领域的话我们判断将来机器人的数量会达到100亿每个机器人的数据估价如果是1000美元的话总估值将会达到10万亿美元也就是互联网数据的三倍以上这是一个巨大的市场而具体看到当前的工业机器人为例的市场规模我们可以先简单介绍一下中美两国的制造业人数假设这些制造业人数我们用机器来替代的话每台机器的售价也知道的话那么我们用机器的台数乘以售价会得到整的一个市场规模我们假设在这里面我们只用1%来计算我们的数据成本那么会得到425亿和175亿两个数字我们再把它们除以中美制造业占全球的占比就会得到当前的工业机器人的潜在的数据的市场有1000亿人民币因此我们希望开发一款平台来聚拢巨生智能的数据价值首先我们要迈过巨生智能数据的扩取门槛要迈过这个门槛的话我们至少需要这个做到三点首先是广泛的兼容能够支持各类数据采集技术聚拢分散的数据第二是有一定的理论支撑看究数据的采集底层逻辑聚拢标准共识同时我们希望能够开源共建推动社区广泛对决共享聚拢接口的规范而仅仅迈过门槛是不够的我们希望能够提供一款更通用的开源数据生产平台通过软硬件捷偶来帮助低成本采集高质量的数据通过仿真平台的对接帮助快速生产高质量数据通过广泛的设备支持帮助全面获取高泛化的数据最后是提供一种聚生智能的数据及构建方法来生成通用的数据集接下来简单探讨一下聚生智能的数据瓶颈以清华大学的一篇研究论文为例他们在模仿学习领域发现数据的scaling low依然是有效的因此我们也遵循这个假设scaling low在聚生智能领域依旧有效也就是说性能越好的模型它的模型尺寸需求越高训练时长需求越高训练数据的需求也越高那么在这个基础上我们来探讨聚生智能的数据评定是什么可以通俗的通过三个问题来进行探寻首先是我们究竟要采集什么数据其实是怎么采集数据然后是采集多少数据这就对应了聚生智能的数据来源技术路线还有模型需求都分别是什么我们进入第一个问题聚生智能的数据来源是什么这张图对聚生智能的数据来源做了一个划分可以按照本体构型进行区分可以按照数据类别进行区分也可以按技术路线采集技术还有应用领域进行区分那么既然我们是在聚生智能领域进行探索我们可以发现聚生智能的数据来源是由聚生智能技术路线来决定的因此我们进入第二个问题聚生智能的技术路线当前是怎样的这张图展示了当前的具体的技术路线的一个划分它可以划分为分层决策聚生智能和端到端决策智能和端到端聚生智能而分层决策智能呢又可以进一步划分为三种一种是大佬去对各类函数进行API的调用例如大佬去调用定位导航的API位置估计的API轨迹控制的API然后对任务进行执行第二种是大佬去调用各种技能的API这些技能可能是由深度学习网络来进行训练得到的小的模型例如专门的抓取的模型按压的模型第三种是大佬对小佬API进行调用这个小佬也可能是一个大模型它学习到了通用的运动的能力而端到端的模型呢则是将大佬和小佬合并起来形成一个从输入到输出端到端的执行的一个方案而我们也可以看到从左往右呢我们对相应的技术它的泛化期望性是由低到高的那也就意味着它的数据量的需求是由低到高的同时呢我们可以发现我们把这两种技术路线中的数据需求方提取出来的话就只有四种训练需求方首先是大佬的训练需求再是技能的训练需求然后是小佬和端到端模型的训练需求因此我们就可以进入第三个问题聚生智能的数据需求是什么也就是这四种大佬技能小佬和端到端的训练的数据需求他们的这个训练方法数据类型都不相同例如大佬的训练呢主要集中在是大模型的训练上因此有相应的大模型的训练方法而它所需要的数据类型呢是互联网数据和指令微调数据这里列出了它典型的数据集和典型模型对于技能的训练来说现在当前主要是用模仿学习和强化学习来进行训练它需要的数据类型是操作数据和感知数据小佬的话呢在这个基础上可能也会使用到一些监督微调方式端到端的训练呢则是以上所有训练方式的一种总和那么经过这种盘点呢我们可以把各个不同的训练给具体的总结出来首先大脑训练的数据需求呢它实际上是对物理世界常识和机器人领域知识的一种需求而技能的训练的数据需求呢它需要的是人类的演示和机器人感知的数据进行训练同时呢我们需要在多种场景下进行采集希望它能泛化到多种场景中而小佬的数据的训练需求呢是在技能的训练数据基础上加上人类的语意标注这样的话呢就对这个技能的组合可以做语意的控制反而是泛化到多种任务上去最后是端到端的训练数据需求那么就是以上的大佬加小佬的所有的训练数据在一起同时呢我们希望能够泛化到多种型号上去那么我们可以把以上的分析给形式化成一个公式这个公式呢展示了居生智能的总的数据需求期望大佬数据加小佬数据然后乘以型号类别我们可以把它继续展开就成下面的公式同时呢我们基于一个基本假设就是Scanning Low性能越好的模型它的数据质量数量还有独样性的需求就越高我们就可以在基本假设和这个公式的基础上去进行相应的变量的分析我们可以把最关键的变量提取出来DPSTM为什么它们是最关键变量呢因为它们一个是被放大基数一个是连续相成的放大系数这意味着它们只要一个轻微它们其中一个有轻微改变的话都能对总体造成剧烈的波动而我们想让D越大越好的话其实就可以分为两种方法第一种是尽可能的把这个基数给放大第二种是把连层的放大的系数给放大但是在实际中呢我们会遭遇困难首先呢对于基数来说我们要尽可能多的人类演示和机器人感知数据的话它的成本是很高的第二种呢我们想要ST和M也就是场景任务和型号的类别越高的话这个相应的这个难度也越大所以呢当前聚生智能数据瓶颈我们可以分成三种一种是成本黑洞也就是难以承担大量的高质量人类演示和机器人感知数据的采集成本第二种是数据孤导难以采集到足够丰富的场景任务和型号类别的数据第三种是评估空白难以评估当前数据是否有效的提高了数据集的性能潜力那么在这个基础上呢我们来分析一下聚生智能数据生产的现状是怎样这里这张表列出了当前数据生产服务的类别以及相应的问题和我们做的一个发展方向的预测首先呢就是聚生智能机身厂商它可能是自己来进行数据采集的或者数据生成的这个服务这样的话呢他们要么是研发专门的数据采集设备要么是针对自己的机器人进行合成数据的研究因此呢他们都会推高相应的成本以及难以适配其他机器人因此他们的发展方向应当是提高接口的兼容性以于机器人的兼容性第二点呢是做数据采集的服务商他们也可以细分为三种服务一种是产业转化服务他们的问题呢是缺乏采集技术的研发能力并且受政策一项大第二种是采集方案服务例如各种VR摇操座的服务供应商但是呢他们的设备适配轴体厂产品覆盖面也较窄第三种是人力资源服务他们是仅提供采集的人力服务没有多少行业话语权因此他们的发展方向呢应该是推出更通用的采集平台实现软硬件的揭露并且拓展服务范围实现数据采集生命周期的全覆盖第三种数据生成服务商他们可以提供数据的合成服务和仿真平台的服务而数据合成服务呢它的市场会相对较小客户的沟通成本也高需要进行定制化的开发而仿真平台呢相应的竞争压力会比较大因此他们的发展方向应当是兼容更多的仿真平台与推出多样化的功能差异竞争最后是数据云计算服务商当前的普遍的云计算服务是缺乏聚生智能的数据管理平台的因此它的发展方向应该是推出标准化的管理平台兼容多种格式的数据集所以当前聚生智能数据生态位的构建还需要完善很多各方面这张图呢展示了当前聚生智能产业链上的各个制造商的位置上游的供应商呢包含摇操作设备制造商数据采集设备制造商机身本体制造商网织平台开发商而下游呢就是指的聚生智能模型开发商或者是聚生智能机身开发商由他们再去面对终端的消费客户如果下游开发商挑过数据生态位直接去做数据生产的工作的话会提高他们的研发成本而如果他们直接去找上游供应商呢上游供应商又大多不具备数据生产能力因此难以满足下游开发商的这个数据生产需求所以呢就需要有一个中间的数据生态位能够提供数据采集生成和处理的服务更往下游呢是对接数据生产的需求往上游呢去对接软硬件整合的需求我们的判断是一款通用的平台足以占据聚生智能数据的生态位这里有一个图展示了当前各种数据采集设备它的一个优势和劣势横轴呢这里是数据通用性重轴是数据资料那么我们可以看到不同的数据采集设备它采集的数据各不相同成本也不相同效率场景的适用性都不相同所以呢它们采集的数据将形成一个一个的数据孤岛而从这张图我们可以看见现有的采集技术是无法实现数据质量和通用性的统一的但是呢你也不能说某一种数据采集方法就一定比另外一种好因为现有的数据均包含了人类能力的完整的语语信息一个能力足够的模型应当是能够把这些人类能力的完整语语信息给学习出来的所以呢在聚生智能的模型和算法收敛前数据的孤岛问题是不太不太可能解决的我们提出呢应对这个数据孤岛问题那就是当前的不论是哪种采集设备的数据都应当应收进收直到聚生智能的模型和算法收敛那么这些数据将可以得到统一的利用他们的资产的价格将在关键时刻得到体现接下来正式进入到ISB的聚生智能数据生产平台的介绍既然我们想做到所有的数据应收进收那么我们就要做到最强的兼容性因此呢ISB的聚生智能数据生产是支持各类的真实世界数据采集方式各类的仿真环境数据生成方式以及聚生智能数据集的自动化的构建从而呢确保广泛的技术兼容性同时呢我们的架构设计呢也要确保广泛的软硬件接动性这里展示了我们ISB的架构设计它通过摇操作接口机器人接口仿真接口去对接不同的摇操作设备不同的机器人或者采集设备和不同的仿真环境同时呢对于接收到数据呢我们有数据采集服务数据生成服务和数据及构建服务全自动的去构建出聚生智能的数据集这样的话呢ISB的就是一个联通四方的平台不同的摇操作设备机器人设备仿真平台和数据集都可以通过ISB的方便快捷的形成数据的流通这样的话我们有高的软一键兼容性和高的技术兼容性我们的ISB的数据生产平台就有高的生产柔性在以上技术特性下呢我们ISB的就有一个优势第一个优势就是万物皆可达我们首先支持各类的数据采集的技术包括摇操作类的数据采集和视角类的数据采集摇操作类的又可以分为位置类摇操作视觉类摇操作光怪类摇操作我们的这个ISB的它会分三阶段去达成各类的技术兼容性而对于这四条路线呢我们是期望能够实现任意机身形态的适配控制任意末端执行器的适配控制任意距离与视角的适配控制和任意场景与操作的适配控制从而通过多技术支持实现任意的数据采集ISB的第二个优势是万物皆可生ISB的可生性包括轨迹合成资产合成决策生成还有预测生成通过这四条路径呢我们可以实现任意操作轨迹的合成任意可交互资产的合成任意智能体决策的生成以及任意物理规律的生成通过多技术途径就实现任意的数据生成而Invidia的生态呢为ISB的提供了可生性的优势ISB的可生性包括轨迹合成资产合成决策生成和预测生成那么在轨迹生成方面呢ISACSIM平台可以支持虚拟药操作数据生成以及各类的Policy模型的接入来合成更多的这个操作数据在资产合成方面Invidia提供了Edifice 3D技术支持各种快速的3D的资产合成在决策生成方面Invidia的NIM微服务能够支持高效的智能体决策生成在预测程程方面呢Invidia的Cosmos4D模型支持高保证的预测因此呢仅凭Invidia的生态就可以实现任意的数据生成顿利展示了ISB的一个工作流程我们的ISB是通过Rose 2架构来设计的因此它可以实现分布式的灵活部署它的启动分为三步首先是编写配置文件这些配置文件分别定义了摇操作接口机身接口数据采集服务和数据集构件服务是它们的运行模式和参数第二步是启动设备将摇操作设备和机身设备启动起来那么摇操作设备把摇操作数据发送给摇操作接口然后接口将其中的姿态数据提取出来发送给机器人机器人执行任务的同时把真实数据返回给机身接口第三步就是启动服务将这些接口和服务启动起来那么整个的数据就开始自动化的流动并且生成数据集对于仿真数据的生成的话也是一样的步骤我们可以直接通过Invidia的Omios平台来进行数据生成例如我们可以同样通过摇操作设备对接摇操作接口来将姿态数据发送给Invidia并且通过仿真接口去接收合成的数据然后通过数据生成服务和数据集构建服务来得到相应的聚生智能数据集整个流程是不需要编程的可以快速开展数据生产流程进于ISB的呢我们可以构建相应的数据飞轮来加速模型的迭代例如这里展示了在Invidia平台下的自动化数据集构建通过虚拟摇操作我们可以在Invidia的ESAC里面去快速的实现仿真数据的生成并且构建相应的数据集在这个数据集的基础上呢我们可以训练得到一个模仿学习的ACT模型这个ACT模型通过摇操作接口可以再次对接回ISACSIM平台并且呢进行预随计划的操作合成更多的大规模的数据然后通过仿真接口将数据发送回数据生产服务和数据集构建服务构建得到新的数据集形成一个数据飞轮不断的迭代加速模型的训练我们的这个数据飞轮呢经过实测在真实世界数据集构建上有36倍的加速在仿真环境数据集构建上有3.5倍的加速整个的数据飞轮呢有6倍的加速而我们的ISB的呢整体的设计呢就刚好针对了居生智能的数据瓶颈首先是应对成本和一动我们能够最大化软硬件结构开源削减软件成本第二呢应对数据孤导我们最广泛的兼容各类技术确保各种场景任务型号的数据的丰富度第三是应对评估空白我们会自动化进行数据集的构建并且给出一个数据集的性能潜力定性的评估方法这是因为我们的数据集是按基金塔结构划分的我们会按型号任务场景和执行来进行数据的划分这样的话呢根据我们之前的数据性能期望公式我们可以把关键参数提取出来它们的连程就代表了当前数据集的性能的潜力接下来是为大家展示一下isb的开源实例首先是第一个demo光罐摇操作的数据采集我们搭建了一个动补数据的采集场景这里展示的是光罐的动补系统动补摄像头这里是操作人员配戴好动补系统之后在系统里面的一个显示接下来呢是我们对接的大象机器人的这款机械P来进行一个实际的数据采集第二个demo是做虚拟药操作这是我们在invidia的isac-sim仿真环境中搭了一个仿真环境同样是控制刚刚的机械币来进行抓取的数据采集第三个demo这是基于Aloha的同购的腰操作进行数据的采集这是在真实数据采集的过程中做一个双手协同抓取的一个工作而这是把数据再导入到Modulka仿真环境中进行一个大规模的数据的合成用于模型的进一步的训练好的我今天演讲就到这里这里有一个我们的项目开源地址的一个分享欢迎大家跟我们一起交流学习谢谢大家谢谢大家��年エ 트롯 tipo这样