はい 早稲田大学の尾形です実際ロボット基盤モデルの展望と その応用可能性というタイトルで発表という講演をさせていただきます よろしくお願いいたしますまず簡単に私の自己紹介というのを させていただきますこの履歴を全て読むということはいたしませんがもともと早稲田大学で機械工学 いわゆるロボットをジュラルミンを削って作っていたというところからスタートして途中で農科学に行き そして京都大学で情報のほうをやって現在また早稲田に戻ってきた経歴のポイントとしては機械のハーベアと それから情報 いわゆるコンピュータサイエンスですねここを融合するようなところを ずっと意識してやってきましたここに本が 私の本がこういうふうに書いてありますけどこれ2017年 今から8年前に書いた誕生ですけどディープラーニング 今は生成AIと呼ぶべきなのかもしれませんがこういった新しいタイプのAIというのが ロボットを大きく変革するだろうということを書きましたこの当時はだいぶちょっと時期早生だった という感じもしないではないのですけれども今まさにこの分野において大きな変革が起こっている というふうに言えるというふうに思っていますこの例えば一つ有名なところが こういった歩行ロボットの研究たちです強化学習を利用して しかもシミュレーションの中で 学習ができるようになったそれをいわゆる生成AI系のモデルに入れることによって 非常にロバストな歩行が実現できていますこれはもともとオリジナルアイデアは この左下にある2021年のATHのアニマルの仕事が最初だったというふうに思いますこの研究が何がすごいかというと 今まで歩行の研究っていうのは非常に特殊であるとなかなか難しくて簡単にはできないよというふうに 思っていたわけですけれどもそれがこれをオープンにしてコモディティ化したみなが使えるようになったというのは 非常に大きいんですねそして それがいろんなところで再現されるようになりましたこの右下にあるのは 最近の例ですけれども 中国で作られている人型ロボットですこれもちょっと前だと これを制御 普通の制御でやっていたので本当に一部の人しか実現できないような 特別なスキルだったわけですけれどもこれが世界中でいろんなところで 再現できるようになったというのは非常に大きいんですねこれはもちろんダイレクトドライブモーターの 大きな進歩であるとかもあるんですけれどもまさにAIがロボットを使えるようになったという 一つの大きな事例だと思っていますただですね これ自体は 確かに素晴らしい進歩なんですけど歩行ロボットという点でいうと 特に日本の方はすぐに思い出されるのが足もであろうと思います今から20年前にはホンダが人型ロボットを作ったあれは歩くことができ 走ることができ 階段を登ることもできました実は歩行ロボットというだけであるならばもちろんあれはすごく特別な技術で 誰も真似できないというところがあったという点によってこの今の研究とはちょっと違うんですが しかしできてはいたんですよねでも実はその20年前の研究 アキューマノイド研究というのはなかなか実用化とか事業化することが 難しかったというのがありますそれはなぜかというと その時はあることはできたけど物を見ることはできないですし 聞くこともできないですしそして物体をハンドリングすることも できなかったからなんですところが今はAIがあるわけですねその典型的な事例が この模倣学習と呼ばれているものですいわゆる基盤モデルの考え方ですね自然言語であれば人間が集めてきた 人間が作られた文章この文章をAIが再現できるように学習をするとこのやり方は人間がロボットを操作をしてその操作した動作をロボットが 再現できるように学習するというわけですつまり基盤モデルの考え方も 言語だけではなくて映像でも環境音でもいろんなものに使えるんですがこれをロボットに利用しようとする 考え方が出てきたということですこの考え方自体はちょっと後で少しご紹介しますが私自身は十何年前からやっていた 方法ではあるんですけれどこれがこの1年になって 手法と そして集められるデータ量ですねこれが一気に進展をしたというふうに 言えると思いますこういったタスクというのは普通の人であればもちろんできる 仕事ではあるんですけれどでも今までのロボットであれば とてもできなかったそして人がロボットを操作して 動作を集めていくわけですのでその対象とするロボットは 人型であるほうが良いということとあとこれは人が教えているテスラの例であるとか オープンAIの例ですけれどもこういったものを使って 次々とデータを集めていくという競争が現在 世界中で行われているというわけですこれはですね 実は私が2017年に 国内の大きいプロジェクトに対して提案をしたときのスライドであります先ほどディープラーニングが ロボットを変えるという本を書いたときと同時期なんですけど このときからロボットが データドリブルになるんだということを非常に強く主張していましたつまり今のロボットというのは 常識としてはロボットのモデルがあり環境のモデルがあり そして認識のプロセスがあってプランニングのプロセスがあって というふうにやるのが今でも普通ではあるんですけど それがまさに自然言語の処理が文法解析やかかり受け解析から エンドとエンド つまりデータから直接動くようになったのと同じようにロボットもデータから 動くようになるんだよということをこの当時 8年前に申し上げていたわけです残念ながらこの提案は これもデータをいっぱい集めてきてサーバーで学習させるんだよっていうことを 言ってたんですがこの提案は残念ながら起こったんですけどでもまさに時代はそういう方向に進んでるなということを信じているということでありますこれからデータを集めていくということが 起こっていくでしょうただ 実はそのデータの集め方というのは あまり簡単なことではありませんターゲットとするタスクを教えるということは もちろん入るんですけど実はそれがきれいに集まっていく方法論って 何なんだろう実はターゲットのタスクだけではないものを 教えなければいけないんじゃなかろうかとかいうことも入ってきますよねまた 身体の動きはどこまで教えればいいのかロボット 人型と言ってましたけど 人の格好じゃないものも当然使えますしそういう意味では非常に難しいそれから人間の場合は そこまでたくさんのデータはいらないですね先ほどのこのロボットの場合 これ1万時間というデータを集めていますそれによっていろんな作業ができると言ってるわけですがおそらく服を畳むというのにだけであるならば そこまで人間は学習はしないですよねおそらくはプリミティブとなるデータを 徐々に積み重ねていくというそういった工夫が必要になるだろうというふうに 考えているわけですこういった視点を機能しているのが この発達ロボティクスという概念です英語で言うとDevelopment Robotics もしくはDevelopment Cognitive Robotics なんて言い方をしますがこの概念は もともとはディープラーニングが 生まれる以前というんでしょうかニューラルネットワークと呼ばれていた頃にニューラルネットワーク自体は 役に立たない技術とずっと思われてきたわけですがそれをロボットに利用するというときの 研究分野の一つですそれは何を考えるかというと そういう神経回路ニューラルネットワークと 人間のような身体を与えることによって僕らの発達や学習のプロセスを理解しようという そういう研究分野として日本からスタートしたものです今はヨーロッパで非常にアクティブに 研究をされていますこれはもう2018年ですねアセア大学でこういう学会を 私がチェネラルチェアでさせていただいたときの図ですけれどもまさにこういう発達ロボティクスのような 考え方というのが今 このデータズリブンのロボットの学習というときにも十分利用できるのではないかというふうに 考えていますその中の一つ重要な概念として例えばですけれども 自由エネルギー原理と言われているような脳神経科学の概念がありますとこの式自体は 実は機械学習でよく使われるエルボーを 逆にしただけの式なんですけど要はモデルの予測誤差 あとそのモデルが持っている潜在的なプライヤーと言われている 事前の信念分布で表現されることが多いですが そことの距離を表していますここはあまり難しい話は しないようにしたいんですけどこれが脳にも当てはまるんだと脳は実は世界に対する 予測器として働いている生成AIっていうのも 実は予測器なんですねある程度単語が来たときに 次の単語はどうなるんだろうとかある音楽が振る線があったら 次の音符はどうなるんだろうというのを人間のデータから予測できるように 模倣と言ってますけど予測できるように学習していくわけですそれがまさに脳のも同じようなことを してるんだとこの予測誤差を減らさなきゃいけないというときに確かに学習というのはあるんですがもう一つ重要なのが推論です今 これはずっと自分は言ってきたんですけど今 まさに大規模言語モデルでも 推論の重要性というのが改めて注目されるようになってきていますつまり学習も重要なんですがその答えを出す前に チェーンオーチンクのような考え方ですね中で推論を繰り返し行わせるもしくは外部の知識にアクセスさせることによってラグのような技術ですねこれによって精度を上げようこの概念が実はこのモデルの中には 自然に入っているものなんですねそういう意味では もちろん全く違うものの オリジンは全く違うんですけどでも かなり近い概念を指しています私たちはこういうロボットに対して こういうことをやるのは自然だよということをずっと言ってましてその一つ いつもこのスライドを 私のプレゼンの場合出すんですけれどこの予測学習という言い方今 模倣と呼ぶ方が自然なのかもしれませんが生成モデルを使ってロボットを動かすということを 約10年前にスタートさせていますここではですね人がロボットを操縦する先ほどテスラがやってたことと全く同じですが操縦してデータを集めていきますただ ここでは50パターンぐらいタオルを畳むという動作ですねこれもさっきGoogleの大きいやつが 服畳んでましたけど私たちのちょっと繰り返しですが 10年前です今見てる映像から 未来に変わるであろう予測映像を 生成AIの技術を使って リアルタイムに予測をしていきます映像を予測するということは実はロボットの身体のジョイントですね関節の情報というのを予測するということにも使いますこの関節は実際は人間が操縦することによって 作られてるわけですけどこれをロボットが目の前のタオルを見たときに 自分で予測できるようにしなさいというわけですね学習をしながら 実際にはこれ 学習させてない場所にタオルを置くんですけどもしくはちょっと違ったタオルを置いたりもするんですけど そうすると誤差が出ますねその予測誤差が小さくなるように中で推論 繰り返し入力をかけていってその予測映像であるとか もしくはロボットの動作自身を修正していくとそういうことを行っていくというモデルですこれを10年前にやっていろんなところで展示をしました10年前のネタなので 今はあまりしゃべるのも恥ずかしいのかもしれませんけど改めて服を畳むといったデモが 去年になってトップのビッグテックから出てきてるというのはなんか面白いなと思いながら見てるんですけどこれ 学生さんが自由にタオルを置いてやってますね50回しか教えてないというのがポイントになりますつまり予測をしながら 修正 しかも推論をしながら動くということをやりますので実際の学習にそれほど大きなコストをかけなくても動けるという そういう考え方ですこれ 日本の人工知能学会誌ですねの2年前になってしまいましたけれどもいくつか歴史をですね ディープラーニングの歴史を並べてくれていたんですけどそこに選んでもらったりもしたりとこれ 10年前ですから本 これもちろん未学習なんですが未学習のものでもこうやってこれタオルだと 頭の中で予測画像は実はタオルだと思ってるんですがこうやって畳むことができるようになりますそうですね 日本は産業用ロボットの国です先ほどのロボットの展示というのを国内 もしくは海外も持って行って複数の場所で展示をしていたところをいくつかの日本の産業用ロボットメーカーからこれは応用できるんじゃないかということを お話を伺うことができました例えば この汎用ロボットの中でも 国際ロボット展でやったものですがもしくは粉体計量や液体計量ですねこういったものも日本ならではなんですがこれ お薬だとか もしくは化粧品だとかを作る実験をするときにこういうプロセスが 必要になるんだそうですけれどもこういった応用が実際に我々のモデルを使って 行われていると一部 すごいプリミティブなモデルではあるんですがAI模倣学習ということで 商品化されたりもしていますその後 日立製作所とずっと継続した研究を 行っていましてこれももう3年前になってしまいましたが ドア開けを行うというものですドア開け 通り抜け このアーム 両腕の制御それからこの台車部分の制御 全てが模倣学習によって動いていますつまり台車を操縦するという フェーズの模倣学習それからドアを開けるという部分の模倣学習ドアを通り抜けるという部分の模倣学習を 組み合わせることによってこういった作業が非常に安定して 行われることができるとそれから今 よくエンドとエンドで動かすと 復帰動作というのがよく出てきますね失敗したと こういったものも 自然に出てくることになりますこれはサイエンスロボティクス サイエンスの島石ですけどそれの表紙に選んでいただいたものですこういう研究成果をずっと続けて 出してきていたわけなんですけれどもここから実は一つ国のプロジェクトに 採択されます先ほど8年前には起こったという話をしたんですけどムーンショットという 日本の内閣府が指導している 大きなプロジェクトがスタートしました目標1から現在10まであったのかなそれは生活支援やエネルギーだとか 医療系だとか いろんなプロジェクトがあるんですけどこの中の目標3というところで 私たちのAIロボットが実際に人間をサポートするんだよとそういうプロジェクトがスタートしていますムーンショットというのは もう気づきの方もいらっしゃると思いますがアプロプロジェクトですねケネディ大統領が1960年代終わりには 我々は人間の月に送るんだともうちょっと当時からいくと 古代妄想ではないかと思ったようなことを言って本当に実現してしまったという話ですけれどもまさにそれになぞらえて 人と本当に生活してサポートするロボットこれはずっとずっとみんなが夢見ていたわけですけれどこれを実現するんだよということを言ったわけですね今の時代もあっちこっちで人型ロボットが 実際にそういうデモをやり始めてはいるんですけれどこれは2020年にスタートしたプロジェクトですこの時に言ったのが ロボットはスマートフォン化するんだという話をしましたスマートフォン ご存知のとおり 元々は 一応電話ということになってますがその機能というのはカメラ機であり ゲームであり 地図になったり辞書になったりいろんなことができるいますねそれを非常にハードウェアとしては 最もシンプルに ボタンとか全部取っちゃって最もシンプルにしたデバイスに この機能を全て収めることによってもちろん専用機と性能を比べると 全部負けるんですけどしかしここに全部その価値が集まったことによってこの市場全部合わせたものよりも 圧倒的に大きい市場を手に入れることができたわけですロボットも今現在産業用ロボットがあり そして配膳ロボットがありアバターロボットがあり いろいろあるわけですけどこれが機能としてはマージしていくそれでこちらではiOSやAndroidのようなものが その機能の入れ替わりというのを実現したわけですがここにAIが入ることによって ロボットがいろんなことができる一台のロボットが本当にいろんなことができるんだという 未来を実現しようとしたわけですそれで我々が作ったロボットが こういうロボットですアイリックって名前をつけました AIドリブンロボット for Embrace and Careというんですが人型ロボットに一応していますこれ 今 人型ロボット こんだけブームなんですが これを作ったのは2020年なのでただですね 私は早稲田大学の人間ですけど 早稲田大学で学位を取ったんですが早稲田大学は世界で一番最初の人型ロボットを作った大学ですワボット1号というロボットが1973年に作られています要するに 先ほどの発達ロボティクスとかの文脈もそうですが人間の形をしたロボットというのが 学習をする 発達をするという当時 1970年代は工学的人間学という言い方をそれをリードされた加藤一郎先生は そういう名前で呼んでおられましたけれどもまさに人型ロボットを作るということによって 人間を知るんだよということがあったんですでも ここでのプロジェクトは それが実際に現実に本当に役に立つんだよというお話をしていますちょっと調理をするだとか いろんなタスクがあって それを全部ご紹介いろんなことできるんですけど 全部紹介するちょっとお時間はないのでこれ実際に去年のiCRAという インターネットシャルカンファレンスのロボティクス&オートメーションという世界で最も大きい そして権威のあるロボットのカンファレンスが 横浜で開かれました7000人ぐらい来たんですけど その横浜でライブでデモをした動画で見せるというのは もちろん動画でも見せれる ただデータを見せるよりはずっといいんですねそれから何でしょうか ちゃんと十分にカット割りをしないで 長い動画を撮るということをやっている場合それは偉いとは思うんですね 先ほどの例えば このパイゾロはすごくそれを意識して後半で非常に長い カット割りを全然入れないような実はカット割りをしてるというのは そこでロボットを再起動してる 再セットアップしてる可能性が極めて高いんですけどそれを疑われるのが嫌だから ちゃんとスピードを1倍速だよとか こういうのもすごく大事ですね実は何十倍速でしたとかあり得るんで そういう1倍速だよっていった上で動画をきちんと長く撮るっていうのは これ素晴らしいんですけどそれでも撮り直しがきるかもしれませんし 環境はいろいろとセッティングできると私たちがずっとこだわってるのは こういうのもそうですけれどライブで見せるということなんですね 目の前で動いて見せるこれもそうです GTC Conferenceで それこそND&GTC Conferenceで2018年にジェンスファンさんに 見ていただいたりもしましたライブで見せるという非常に強いこだわりを持っています これもそうです例えばちょっとライブの時の風景ですけど これはもうコタオルを取り込むとかいうこういうのも1つ前のロボットでは なかなか学習 データドリブンで動かすという発想でなければもともと対象にもならないような 難しいタスクなんですけどこういったものがデモデータが ある程度作れるようになりましたエンドとエンドで学習ができるようになりましたよということですねそれからこれもドア開けですね これもライブで見せるということに意味があるということを非常に強くこだわってまして 学習をさせていますと先ほどのこの大きいロボットのほうは こっちですねこっちは片手で20キロぐらい持ってるような 非常に強いロボットなんですけどそこまで必要じゃないだろうということで このロボットの場合は片手で2,3キロぐらいまで持ってるようにしてるんですけどそれで作業をいろいろさせようという もしくはデータを集めようということを考えてやっていますまさにデータを集めるということが 重要だと考えていましてこれを見せると今スタンフォードやグーグルで作ってる アロハとよく似てると言われちゃうんですが僕らアロハよりは数年前に作っているので 一応視聴はさせていただきたいんですけどこういうデータを集めると 人が創業することによってデータを集めていくんだよということこれはサイエンスでやってたことと 全く同じことなんですがサイエンスロボティクスの論文でやってたことと 同じものなんですけどそれを作っていると今 こういう形できれいに これ同じロボットでして 高さをこうやって変えられるんですけどこういう形でまとめまして これを広く皆さんに使ってもらえるようにしようというようなことを考えていますさて ロボットの基盤モデルということで ちょっとお話をまた少し続けていきたいんですが先ほどどんなデータをどう集めるのか 難しいんだという話をしました一つ いろんな考え方があるんですけど発達ロボティクスの視点から 少しお話をさせていただくならばこれ RTXっていうのはですねさっきのiCLA 昨年のロボットのトップカンファレンスで ベストペーパーを取ったGoogleだけじゃないんですけど 世界30カ所以上の機関が協力してデータベースをどんどん作りましたっていう ロボット基盤モデルを作ったっていうお話なんですね確かにすごいんですけど でもこれをじゃあうまく使えるかというとまだみんななかなかうまく使いこなせないつまりターゲットとなるタスクを つまり ただピックアンドプレイスとかとかプッシュだとかそういうことになってしまうんですねどうしても片手で集めることになりますのでできるタスクというのは限定的ですし やれることも限定的になるざるを得ないです全く違う視点でやったのがこういう研究で これも僕は好きなんですけどこれはですね タスクは設定しないんですね仮想空間に置いて このアームがあって そのアームを人が操縦できるんですけどここで好きなことをやってくれと お願いしますとそうすると人間というのは不思議なもので タスクは与えられてはいないんですけど全く無意味なことはしませんよねなので この中でフリーに動いてもらったデータをプレトレインの 事前学習のためのデータに使ってあげると良い性能が出ますよなんて そういう研究だったりしますもっと極端な例を出しますとこれはロボットではないんですが私 産業技術総合研究所というところの ポジションもあるんですがそこで一応強調者に最後ちょっと 入れてもらったものなんですけどCVPRというところにあった画像認識のための 事前学習用のデータを作るという研究なんですがこれが面白くて 現実世界じゃないものを 学習させるんですねフラクタル 気化学を使って ここはいろんな作り方があり得るとは思いますが非常に自然物にフラクタルというのは 関係するわけなんですけどそういったものを使って作った いろんな画像 物体の画像ですねこれを先に学習をしておくわけですねそうすることによって 高い認識精度が作ることができるつまり対象課題と全然異なるデータで 事前学習をするという発想ですこれ ロボットでできないんだろうか そうなんですね実は赤ちゃんというのは 生まれたばっかりの時というのはそのターゲットとなるタスクが あるわけではないですねもちろんおっぱい飲むとか そういうのは大事なんですけどでもほとんど手足は かなりランダムに動いています本当にランダムかというのは 議論はあるんですがいずれにしてもそれが非常に強い 指向性があるかどうかというのは分からない実はそういったものを バブリングっていうふうに呼んだりしますつまり赤ちゃんが自分の体のダイナミックと 環境のインタラクション相互作用を学習するというときに動かす このランダムな動作これを事前学習に使えるんじゃないかという 思想があるわけなんですね実を言うと本当に使えますこれは今後 我々は今 いろいろ研究も さらに進めているところなんですが2つほど我々がやってきた仕事を ご紹介しますとこの一番上 これは高橋くん 今 プレファードネットワークスプレファードエレメンツっていう 基盤モデルを作る会社にちょっと移動してますがプレファードネットワークスに行った 高橋くんがやってくれた仕事ですが柔軟関節ロボットをシミュレーションの中で想定してそれらパブリングを行うことで ターゲットタスクの学習がうまくいくよという仕事ですこれはドイツのミュンヘン工科大学との 共同研究として発表したものですそれから こちらはやはりランダムなモーションと ターゲットタスクというのを同時学習させるということでプレトレーニングというのとはちょっと違う 並列に学習させるっていう感じですねということを提案したモデルで 加瀬くんがやってくれたものです現在NVIDIAのほうにお伝えになっていますがこれも非常に優れた研究成果ということで彼はこの研究とはまた別のワールドモデルに 関係するような研究でNVIDIAと協調があったりしますいずれにしても AIというのは確かに独自の技術とはいうものの非常に人間に近いモデルなんだということなんですねそれが確実にそういったアイデアというものが僕の視点から見ると まさに人間の発達学習のアイデアというものが再利用できるようなものが いっぱいあるなというふうに思っていたりしますこれは去年の12月ですね 本当に年末に発表させていただいたものなんですが日本でもそういう基盤モデルを作っていく ロボット基盤モデルを作っていこうという動きがありますそれを私も参加させていただいている この産業技術総合研究所のプログラムでオープンしたものですACTと言われているトランスフォーマーベースの 法学習のモデルもしくはディフュージョンをベースにしたモデルもしくは我々がずっと発達ロボティクスの頃から やってきたようなリカレントネットをベースにしたようなモデルそういったものを試せるようにしていただいていますなおかつそれがデータセットであるとか ロボットモデルについても今もかなり 比較データについても 一部もう公開していると思いますけれども非常に皆さんに使ってもらおうということを 意識して作っているモデルですぜひですね ご興味があればダウンロードいただいて使っていただければいいなというようなことを 思っておりますこれだけではなくても まさに1万時間 集める必要があるかどうかというのはあるにしてもですね もう1研究室はもちろん 1研究所ということでももうないと思っていますデータを記録オープンにして 学習モデルもオープンにしてみんなで共同開発していくようなコミュニティというのも今後非常に重要になるんじゃないかなと思っていますもう1つですね ちょっと基盤モデルに関係するでしょうかトピックをお話しさせていただこうと思うんですけれども推論というものの重要さということを 改めて言いたいわけですね先ほどその言語モデルが推論というものの機能を足すことによって非常に優れた性能を出すようになったということを お話ししましたそれはもう学習と推論というのは 実は両輪というのは先ほどの能動的推論というか 自由エネルギーの観点からすると当然のことではあるんですけれど それがだんだん認められています例えばですけど 今 こういう物体を見たときに欲しいのは実はこの裏にある牛乳だったとしてますとでもこれ普通にパターン認識というんでしょうかランゲージモデルでもビジョンモデルでも 何でもいいんですが使ってみて認識すると その対象の中にこの牛乳が見えるということを期待するのは実はかなり難しいんですところがですね これを何とか自分で考えさせるということともう1つ重要なのが このモデルに質問してもらうということなんですねこの質問してもらうというのは 実は先ほどの能動的推論というのとよく似ています つまり世界に対して働きかけて自分の予測誤差を減らそうとする行為 先ほどは動作を変えてましたけどそうではなくて 言葉によって尋ねるということですすでにこういう研究としては 発表しているわけですけども自分の中で出てくる可能性はあるんだけれど それが実を言うとモデルの中での不確実性に対する評価 自分の今の判断に対する確信度のようなものが評価できますので それに基づいてこれでいいのかということを人間に対して 確認をしていくということができますそれを何度か繰り返す この場合ちょっと多いですけど4回もやってますけど これによって いや 実は僕はミルクが欲しいんだという情報を具体的に加えてあげることによって ミルクカートンというのがこれが答えじゃないですかと言えるようなわけですね冷蔵庫から卵を取ってくれと言っているときに いわゆるこの卵とですね実はここにエッグカートン 卵のパックがあるんですがどっちを取るかというのは 実は卵っていうだけだとこうなるところがですね 実は卵料理を作りたいんだと具体的にはスクランブルエッグとか オムレツですね オムレツを作りたいんだって言うとだんだんこう不安になってくるわけですね 卵1個で作れるんだっけとなってきて実は言うと こっちですねっていうことになっているわけです物の見方というのは実は言うと一つには決まりません例えば空き缶が置いてありますっていうときに それ空き缶って本当に空き缶であればゴミである可能性があるんですけど 実は言うと飲みかけの可能性もありますよねそうするとどっちだろうという コップっていうのがあったときにそのコップは水とか飲み物がどのくらい入っているかによってまだ飲めるのか飲みかけなのか洗い物なのかが決まってきます実は言うと文脈というのは画像一つでは決まってこないんですそこにはそれを扱う 見てる人の物体を見てるゴールがあるわけです空き缶とゴミ箱があって それはゴミ箱は必ず空き缶を入れると決めてしまうのは実は言うと時期 ちょっと気が早いかもしれないですねそのゴミ箱を片付けなきゃいけないっていうシーンあり得るわけですよそういう意味では ランゲージモデルはいろんな可能性を推定はしてくれるんだけど僕らと文脈を合わせるというときに 実は言うと対話が必要になってくる推論と対話ですねこれがまさに研究としては 非常に大事なポイントになるんじゃないかなと思ってますおお ちょっと難しい事例をご紹介します道具を使わせる今 ロボット基盤モデルでは 実は道具を使うって研究はあまりないんですよでも我々としては道具というのが非常に面白いターゲットとしてもうかなり昔からずっとやり続けていますちょっとご覧ください道具を持ってこういうような動作 これは模倣学習でやるんですが最終的には取れたらよかった 自分の手の届く範囲に持ってくるってわけですねところが こういうのをこうやったときに途中でやると えーって言われて もう一回取りに行くというエラーリカバリーと言われてるものですが こういう動作が自然に出てきますで ここから面白いんですけど こういうふうに動いているっていうことを見せてるんですがこの丸は何かというと これアテンション 注意ですこの注意というのは 実に言うと 私たちが教師信号を与えたわけではありませんこのロボットがこの模倣学習を実現するにあたってどこに目を向けなければいけないかということを 自分で身につけることができるようになるんですで さあ まずここで見ていただきたいのはその模倣学習で獲得した結果として このロボットは見なきゃいけないポイントが3つあるということに気づいています1つ目は自分の手です 当然と言えば当然ですねそしてもう1つ目がその対象 そして道具である道具を引き寄せて取るというのを繰り返してますからそういうことだろうということまで 気づいてくれるまではOKですと緑と赤の違いはですね 今現在注目すべきことと近い将来注目すべきことっていうのは 両方書いてあります予測から注意が出てきています注意っていうのはよく何だろう 画像の中で光ってるものがとかね強い色が出てるものがというのが 注意点だっていうことを言う人がいるんですがそれは完璧な間違いであって そうではなくてですね自分の行動が未来どうなっていくかという 予測の結果として注意というのは自然に生まれてくるんですだから黒いものであろうが 光ってないものであろうが 大事なときは大事ですしずっと光々と光っていようが いらないときはいらないんですそれは非常に大事なポイントでして ロボットならではの視点とも言えるかもしれませんが重要なポイントですさあ じゃあここでですね道具をつかんだ瞬間何が起こったかそうですね 手に向けていた注意が消えたというのが お分かりいただけるでしょうか実はこの手の中で自分がグリッパーと 持っていた1っていう数字がここに飛んでます分かりますかつまり手を防具を持った瞬間に 自分の手先は自分の手ではなくなったんですここになったんですこういうのをですね 道具身体化っていうふうに呼んでいます実際にマカグザル お猿さんの実験なんかでも道具を使うように訓練をした お猿さんというのは自分の手でつかんだときに 発火する脳の状態とそれが道具をつかんだ後に 道具の先が対象に触れたときの状態というのが同じになるということが知られています同じニューロンが発火するということが 知られているんですねもちろん道具が身体化したという以外の解釈も 実はいろいろできるんですけどただ 私たちのモデルはこのように 見事に手先の表現が道具の先中に行くわけですねここでもまさに多様な解釈が起こっている ということはお分かりいただけると思いますつまり 道具というのは物体では この瞬間なくなっているんですねシンボルグラウンド 記号設置問題なんて 言葉で表現されることもありますけれども僕らが見ている世界観というのは 決して安定しているものではないということです一つ画像が与えられた 一つ文章が与えられたということで世界の解釈は決まらない常にダイナミックに動きますし それは非常に難しいひと昔はフレーム問題と呼ばれていたものにも 強く関係すると思うんですけれどもこれは決して今のラージラングエッジモデルとか 基盤モデルができたこの現在においても 決して解決されてはいないんだということはここで強く主張しておきたいというふうに思いますそれを見る 例えばこのアテンションというのは 他のものが追ってあったりこういうのを無視できる能力でもありますあと 道具の種類を変えたりしても 実はできたりするんですねこの辺はぜひ この昔の我々の論文ですけれどもご興味があれば見ていただければというふうに思いますでは 徐々にここから少しまとめのほうに 入っていこうと思うんですが20何年前に人型ロボットのブームがあった日本ではもう人型ロボットを散々やり尽くしたという感が実はありますそして そのときに得た教訓は 人型ロボットの授業は決して成功しないという苦い体験でしたただ 今でも本当にそうなんでしょうかというのは 私はずっとまさにディープラーニングがロボットを変えるというのを 変えたときから思っていることですあのときのロボットは確かに優れた運動性能を示しました日本人だけしか持っていない 非常に特殊な制御技術によっていろんなことができたというのは それは確かなんですだけど あのときのロボットは見ることもできなかったし 聞くこともできなかったつまり AIがなかったんですよだから その人型ロボットがあっても すごくいいハードウェア あの当時からすごくいいハードウェア作ってたんですけどでも その性能には限界があったわけなんですつまり ロボットの発展を キーとなっていた境界条件が AI側にあった今はどうでしょうか画像を認識できる 人の声も非常に もしくは環境も うまく認識できたりします我々の研究では 触覚とかの処理も ディープラーニングによって非常にうまくできるということも分かっていますただ AIの人たちがすぐに手に入れられるロボットは一本のマニピュレーターだったりしますなので さっきのRTXのような研究ではピック&プレースかプッシュか 引っ張るぐらいしかできなかったわけですねつまり AIのほうでは マルチモーダルなデータつまり ビジョン 言語 音声 触覚 力覚そういったものを扱うための 一つの境界条件がロボット側にある 身体側にあるんですね今 まさにこの二つが融合しようとしています歴史的には非常に一度一度 もともとオリジンは同じところにAIロボットって創生機はすごくよく似た研究だったんですけど今 ものすごく離れてしまっているそれが今 だんだん融合しつつお互いがお互いを必要とするこれを僕は共進化というふうに5年ぐらい前から呼んでるんですけどお互いが つまり こういうロボットを作れるのでこういうAIがあれば こういうロボットを作ってもいいよねとかもしくは こんなロボットを使えるんであればこういうデータ集められるよねとかそういうことができるようになってきてるんですねまさに世界で今 こういうことが起こってるんだよなと思っていますもちろん それはやれるということとつまり何でしょうシーズ あとやってほしいニーズこれだけではダメなんですね 実に言うともう一つ この社会需要性つまり やってほしい やれますではなくてやっていいっていう ここも大事ですこれはもう今 AIでまさに起こっていることですけどもロボットの場合には 社会に与えるインパクトが より大きいですのでそういったことは気にしなければいけないと思っています最後のスライドですこの 強進化ってどうやって起こっていくんだろうか今 人型ロボットがあちこちで作られていますモーターの極めて優れた革新が起きましてロボットというのも 前の人型ロボットよりもずっと安く作れるようになりましたそれはそうで こういったものから方向が進んでいくという考え方もあろうかと思いますただ 個人的には こっち側のほうも相当有力だと考えていましてつまり もう皆さん目の前に配線ロボットがあり案内ロボットがあり 掃除ロボットがあり 警備ロボットがありますつまり 移動ロボットってもう汎用化したというか皆さんが使えるようなフェーズに入ったんですねそのところに少し腕がくっつくというだけでやれることは それをデータドリブンで動かした場合にはやれることが一気に増えていくだろうということが期待できますそういう意味では 今後 そっちのほうの方向性というのも私としては注目していきたいなと思っていますいずれのストーリーでも 今 ビジョンランゲージ画像と言葉っていうのは すごく注目されてるんですが実は重要になってくるのは力学と触覚ですこのロボット独特のデータですねこういったものがないとスキルというか 器用なことができないんですよねでも こういうことが まさにロボットが広く広がっていきみながらそこでデータを集めるという 未来が開かれますと非常にオープンな 大きな動作データセットロボット基盤モデルを作るための 動作データセットができるんじゃないかなというふうに期待していますということで まさにこの共振化ということを私の最後のメッセージとさせていただきまして私の講演 今日の講演を終わりにしたいと思いますどうもありがとうございましたありがとうございました