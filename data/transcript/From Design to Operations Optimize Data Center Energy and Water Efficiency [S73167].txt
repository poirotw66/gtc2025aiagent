 So please welcome me and joining four esteemed guests. They'll be talking about from design to operations, optimized data center energy and water efficiency. So Char is with NVIDIA. He is actually the director of product marketing who owns the data center GPU and AI training product marketing. And then, of course, there's Chaitan Kapoor. He leads product marketing, product management at CoreWeave where he helps CoreWeave build out a compelling and differentiated set of cloud services. Prior to CoreWeave, he was part of the compute services leadership team at AWS and managed the hardware accelerated compute business. And then, of course, we've got Martin Olsiv from Vertiv. And Martin is responsible for global product strategy and planning, driving roadmap interoperability of products to deliver complete solutions, investment scope, evaluation, and prioritization across their global business units. And he also leads the global engineering R&D business, planning, and strategy. Holy smokes, that's a lot. And he's been with them since 2017 and has held various leadership positions in engineering strategy and business planning. And then, of course, Mark Fenton has over 20 years' experience pioneering the use of computational fluid dynamics, CFD, in the data center design. They're excited about that. And operation industry. This specialized field has given Mark exposure to build, use, and provide cutting-edge technical software in an emerging and explosive market. As product engineering director for Cadence Reality Design Twin Platform, Mark is involved in strategic partnerships with data center vendors and consultants, as well as delivering software and services for global enterprises, hyperscale, co-location, and edge data center companies. Please help me in welcoming all of these esteemed guests today. And with that, I'll turn it over to Char. Thanks, Laura. And thanks to everyone for being here today. We're going to talk about energy efficiency and sustainability in data centers, a very important topic, as we now see a very rapid adoption in AI across every single industry. Industries from transportation adopting autonomous vehicles, all the way to internet companies setting up chatbots. We're seeing AI enter every single workflow in every business across all industrial fields. Now, all of these AIs are powered by AI factories. And as you saw Jensen say yesterday, in these AI factories, we have large language models that are generating tokens. Those tokens equate to revenue. And these AI factories that are generating all of these LLMs and these tokens represent a trillion-dollar data center opportunity for some of the companies that are represented here today. Now, that impact of that very large data center opportunity is being driven faster and faster at an exponential rate by growing demands for compute. Now, these compute demands are being driven by several scaling laws. We're all extremely familiar with pre-training scaling. That's how you teach a model the knowledge of the internet. And that has grown at a rate of 50 million X in the last five years. And we've seen predictable gains when it comes to increasing the number of parameters, increasing the size of a data set, and so on. And we've seen, as a result of that, you actually get a better performing model. Now, there are two additional scaling laws that are now taking shape in addition to pre-training scaling. Post-training is where models go to school, either in the form of distillation or where they learn how to reason or think. That pace is growing at an exponential rate as well. Trillions of tokens for each single model and at a rate of over 30 X of pre-training scaling. And lastly, there's test time scaling. This became really popular with DeepSeq. The advent of reasoning models is putting more and more pressure on the number of tokens that are generated by each of these AI factories. We see over 100 X increase in pre-training and test time scaling versus single-shot inference, how it's traditionally been done. So with all of this pressure to increase the amount of compute, we're seeing gradually, we're seeing steady increases in the amount of energy that data centers would require. So currently, data centers consume just over 200 terawatt hours per year. Roughly about 2% of global energy usage. And that rate is expected to increase to 5% by 2030. Now, some of these numbers were actually compiled before the advent of reasoning. So you can expect that there's going to be progressively more pressure on data centers in general with the amount of energy that they might consume. So how do we solve that problem? We have to do it by getting more and more energy efficient. Now, we've done a pretty good job so far. This particular chart shows you how energy efficient NVIDIA GPUs have gotten in the last just over a decade. Since we introduced Kepler in 2014, we've reduced the amount of energy that's required to generate a large language model token by over 200,000 times. Now, that's a pretty impressive stat. And with the latest addition of our Blackwell Ultra, we're obviously expecting more gains. And we'll see continued improvements with the advent of our next generation that you saw on the roadmap at yesterday's keynote. Now, how do we accomplish these types of gains? Because that's a pretty stupendous rate of energy efficiency improvement. It starts with a full stack approach. Everything from application frameworks, software, and libraries. We do a lot of work behind fusing kernels, implementing optimization algorithms, and driving towards reduced precision formats. By implementing those, we can reduce the amount of energy that's actually required. In addition, the hardware is extremely important. So the world went through a fundamental shift in the way it does computing, from CPUs, which process serially, to GPUs, which are highly paralyzed in the way they operate. And they take care of compute-intensive functions far more efficiently than CPUs ever could. And you can see in the chart on the right, it shows you a representation of how GPUs, while they do have a higher TDP, they operate so quickly and complete the task so efficiently that in aggregate, the amount of energy they consume is much less than CPUs. Now, one of the major advances that we've had in the Blackwell architecture that we introduced last year was the introduction of liquid cooling. By adding liquid cooling onto the entire rack, we're able to make an incredibly dense compute rack. And that represents 1.4 exaflops of compute. The introduction of closed-loop liquid cooling systems has really led to an improvement in the amount of energy efficiency that we see. We're able to recycle and reuse water, and as a result, the higher water efficiency, we see that it actually saves about 300 times more water than a comparable air-cooled hopper system. So with these improvements, I'll now turn it over to the next speaker. Please welcome Chapin Kapoor to the stage to talk about how his company is implementing these solutions. Great. Thank you. Thank you, Shar. Just a real quick re-intro from my side. So Chetan Kapoor, I am the chief product officer for CoreBeave. I've been with the company about nine months or so. And prior to that, I was at AWS. And at AWS, I ran the hardware accelerated business. So I'm very, very familiar with the entire accelerated computing space and how that has evolved over the last decade or so. Can I get this? Clicker real quick. Sorry. Thank you. Yeah. So I want to start off by introducing CoreBeave to many of you. Some of you might have seen us or being mentioned yesterday at the keynote. So we are a purpose-built cloud provider. So we are one of the new upcoming companies in this space. And our focus from the ground up has been all around just AI workloads. And I'm going to spend more time to kind of talk about what that means for us. But essentially, based on the value proposition we have, we are able to kind of bring latest and greatest NVIDIA hardware to the market before anybody else in the market. They are more performant because we are using the NVIDIA reference architecture. And ultimately, we help companies that are investing in AI get to market faster. So that's our core objective. Now, from a portfolio stack perspective, we actually build things from the ground up. We are deploying our own data centers. So we have about 32 data centers that are online already. A vast majority of them are going to be liquid-cooled, you know, starting now. And I'll spend more time about it. From a data center up, we have a collection of infrastructure offerings. So we have GPUs, obviously, a whole range of them from NVIDIA. We have CPU machines to kind of complement some of the GPU workloads. As you guys know, for these large clusters, networking is super important. So we actually deploy NVIDIA InfiniBand that is sharp-enabled, non-blocking, to actually make sure customers get the most out of their GPU fleet. And on the infrastructure side, lastly, we focus on storage. So we have got some custom storage solutions. One of my favorites is a new capability called Chaos, CoreView AI object storage service that we GA'd just this week. And that provides like an S3 interface, but the performance of a distributed file storage system. So like, makes it really, really easy for customers to get the most out of their infrastructure. From the stack-up perspective, we support, you know, VPCs, bare-metal access to these machines. And all of this compute and infrastructure is made available using our managed Kubernetes offering called CKS. And ultimately, we also have some high-level services at the top that makes it easier for customers to kind of consume all this great infrastructure from us. Lastly, I want to focus on the column on the right, which is also a pretty big differentiator for CoreView, which is a suite of capabilities we call as mission control. And essentially, these are capabilities that help us get the best quality hardware in production in the hands of our customers and maintain that at a really high-quality rate. We are phenomenal when it comes to observability. So if you want to pinpoint down to a particular link in the actual network and figure out what's going on there, we give you that traceability and observability to help customers, you know, detect any issues and kind of move on. So from a data center footprint perspective, like I mentioned, we are about 30 data centers across America, and we are also expanding in Europe. We have talked about this publicly a few weeks ago. Our footprint of GPUs is north of 250,000 GPUs. So we are quite hitting the scale. And, you know, again, this is one of the reasons why I was kind of excited about this presentation because, you know, we know what it takes to actually run, you know, GPUs at scale and we're bumping up against the scale of a traditional hyperscaler that is out there, right? So let's talk about specifically the data center aspect of what we're doing there, right? So I broke it down into like four key sections and essentially it starts by liquid cooling efficiency. So right, Char talked about it a bit. What we are seeing is there is a direct 30% savings when it comes to like power efficiency when we actually switch over from air cooled to actually liquid cooled. So what that means is in reality, if you look at H100s or H200s, both of these platforms actually support air cooled and liquid cooled solutions, right? So we chose to go down the path of actually deploying our H100, a big portion of our H100 and H200 fleet via liquid and that itself translates to, you know, notable savings. As a quick anecdote, like, you know, if a H100 server is about like 10 kilowatts of power, we actually see that server drawing about seven kilowatts of power if it's liquid cooled, right? Because again, you don't have those fans, you don't have that extra circuitry around kind of making, contributing to that, right? So that directly drives to the bottom line. We're able to kind of deploy more critical IT infrastructure in our data center with the same power envelope that we're getting. There's also really cool stuff on the transformer-less design that we're doing in our data centers. So typically, the voltage that is coming off, you know, generators or mains or, you know, UPSs is 400 volts. And in many cases, you actually have to step it down from 400 to 200 so that you can actually power your racks, right? So as you can imagine, you know, there's going to be energy loss that happens in that conversion. It's typically around 7% to 8%. And then there's also heat involved because these transformers get really, really hot, right? So they're also contributing to your heat budget, right? So with what we're doing now is deploying our next generation racks that have an input of 400 volts itself, right? So we totally take out the transformer and actually claw back, you know, that efficiency loss and then, again, use that to actually more densely pack our data centers. And a similar thing on the water efficiency side, Char hinted about this. So, you know, instead of evaporative cooling, we actually use closed-loop cooling that, again, just helps us kind of minimize wasted water and also helps us actually drive better efficiency and utilization across our fleet. So, again, you know, I want to hand it off to the next presenter, but ultimately, you know, what I want you guys to take away is that, you know, yes, these GPUs are really, really performant, you know, but, you know, there's a lot that goes into kind of running them at the most optimal temperature from a total TCO perspective and, you know, we have a lot of experience with this and NVIDIA has been a great partner kind of collaborating with us on the actual design of the servers and the racks and including a design partner such as Dell to kind of make it happen. So, again, appreciate your time. I'm going to hand it over to Martin. Thank you. I appreciate it. And thanks for having me here today. Thank you, sure, for the invitation here. I'm Martin Olson and I'm with Vertiv. I've been with Vertiv for about seven years now. Been in the data center industry for well over 20 years and the data industry, in this case, here being the critical physical layer part of it, so power and cooling and as we heard Jensen talk about yesterday, just this astounding growth and compute and the correlating power that comes with it. It's just absolutely astounding and just to echo the introduction, holy smokes. I mean, that's just a lot to suddenly deal with over a short amount of time here because I remember we were at about 20 kilowatts per rack not long ago. It was probably just last year and we had sort of inched ourselves up from, you know, five, six, seven kilowatts per rack to these, you know, 20 kilowatt we felt that was, you know, that was really something. And then suddenly it's 40, 80, 130 kilowatts is the latest one with Blackwell. And now we can finally talk about where we think it's going, which is, you know, certainly beyond that, 200 or so is probably the next jump. And we heard Jensen talk about 600 kilowatts in a rack, whatever that rack looks like. So I'll talk a little bit about what we're looking at here from Vertiv. So from a critical infrastructure standpoint, one of the biggest challenges is really the disparity or disparate systems that we have and the way we go about designing them and deploying them. It poses a real problem, particularly when you think of this load that's now no longer just 20 kilowatts in a rack, but 130 and potentially now 600 kilowatts per rack really makes a difference. Power availability is probably the biggest challenge right now. Most people recognize that. Just the provisioning of megawatts of power is really, really challenging. I think PG&E, just here in San Jose, they have a demand of about three and a half gigawatts by 2028, which is sort of Miami city-size power demand and that's in addition to what's already here. So it's a very real problem and the way to solve it is obviously we've got to be real conscious about the efficiency of these systems because efficiency really, really matters. There was a period of time where we had sort of all in aggregate our peers within the industry really been focused on efficiency. There was much more that we could sort of eke out, but now that you look at it, even a half a percent efficiency on this really makes a huge difference when you're running a gigawatt data center. The other piece is permitting and being able to deploy this here, particularly reciprocating engines, generators, backup power, and with the introduction of renewable energy sources, battery energy storage systems with it, requires a different look at the data center and how you run it, right? It really needs to be integrated with the grid. The other piece is asynchronous densification here. Compute has, as we've seen, scaled tremendously, really grown in terms of footprint, and the power and cooling really needs to grow with it from a densification standpoint. Right now, that sort of stays stagnant, if not doubling in size from an MEP standpoint. That's a real challenge unless you look at the whole powertrain and the thermal chain together. As you look at the accelerated deployments, we need to speed that up by a factor of three or four in order to keep up with the requirements that are out there. It was pretty interesting because the type of deployment, the type of customers we're working with now have just a completely different perspective to what we're used to. I had dinner with a customer here the other night, and it was very clear that this idea of a unit of compute was sort of the critical piece in their mind. It wasn't that they were deploying some compute or GPUs, and then there was power and cooling infrastructure. The whole thing was a unit of compute. It took a little while before we got connected on the same wavelength on that one to where simple things like milestone billing. It helps when you've got the right frame of reference to where they're talking about this sliver of compute that needs to be up, whereas I'm thinking about the factor acceptance test in the legacy power and cooling world. It's just a difference in mindset of that. Another interesting one where the speed of deployment was things you don't think of that tends to slow some of this down, transportation. The long pole in the tent, though, for transportation was these racks are full of very expensive gear, thanks to NVIDIA, and there was just problems getting insurance on being able to pack a whole container full of them. You had limitations on what an insurance company would cover, and that suddenly staggered out all the shipments for that reason. When you're talking $2.5 million per rack or whatever the growing price is, that's significant. Growing variation in scale. It's really difficult to look forward to the next generation, just right now, the way it is. So being able to adapt to that and make sure that you can upgrade quickly, refresh rapidly, because you will have to do it. And you've got to think of it in terms of, I've got to be able to pull out a rack and potentially put a new one in with new compute in it, and I've got to keep the rest of them 9 tenths of the data center running. That's the requirements here because the stakes are that high. I think the last one here, dynamic workloads, is a real challenge and opportunity. The challenge is obviously in that the AI workload is highly dynamic in the way it operates, very different from what we're used to with your general compute. It's a little bit more of a mass that slowly grows over time or a little bit more stable. The AI workload, it's very dynamic when it reads and writes to memory. And for those reasons, it also presents some opportunities from an efficiency standpoint because you've really got to design your power and cooling infrastructure to closely track that load. It's almost like, think of it as a Formula One pit crew. You've got to be collecting that telemetry quickly, right? Rapidly take all this data and process it and figure out when that car zips in for a pit stop. You've got to be ready to fill it up, new tires, but you've got to do not too much. You're wasting it. Not too little, you're going to lose the race. Same thing here when you're talking these levels of power. So really important to be thinking of it from the way we think of it is we call powertrain grid to chip. So from the utility entrance through switch gear, power distribution, UPS, battery energy storage, overhead busway, all the way to the chip itself, and then we think of it as the thermal chain chip to heat reuse, right? So that's taking the heat off of the chips themselves and ejecting that heat either through heat rejection means or heat reuse. That whole chain right there, it's really important to think of that as one because any misalignments in size and capacity resiliency throughout it compounds the entire system here and it presents itself to some very significant challenges. So what we talk about all the time is this unit of compute, right? So power, cooling, and compute as just one. And that's essentially what our customers are asking for as well, right? So we spent the last five years or so working with the NVIDIA team, data center engineering team, on coming up with optimized solutions for this. We're working with them from a power, cooling, and control standpoints very closely and have been for the past few years here. And the output of that is reference designs that are fully optimized for, in this case here, this is a seven megawatt designed for GB200 NBL72. We have the next designs coming out for GB300, for Rubin. We haven't quite figured it all out for 600 kilowatts per rack, but we're close. But that's what we do every day in and out. And I just want to stress again, it really takes a different mentality and perspective on designing this here to be able to drive this kind of efficiency here. So you're talking 20% energy efficiency improvement, 30% less space, deploying this in half the time of what we're doing today on legacy traditional type sites. Drives overall 25% total cost of ownership as a function of that. And then on top of that, as I mentioned about the dynamic loads, right, that predictive analysis, being able to capture that telemetry and turn that into intelligence and information and knowing what to do about it so you can closely track that load really drives some significant energy efficiencies there. So I really encourage you guys to come say hi to us as well. We're on the floor, and I just want to thank NVIDIA for the partnership so far. And with that, I'm going to hand it over to Mark. Thank you very much. All right. It's like a relay. It's like 4x100. Okay. Well, nice to see you at the end of the last session. It's a bit of an honor. I'll move onward. See you later about it. Hello, Cadence. Right. I just want to... Let's imagine you've walked into GTC. You've got your swag bag, and in the swag bag is your very own crystal ball. You can ask questions, and it's going to give you answers. What kind of questions are you going to ask? And I kind of... A lot of people go, like, probably the lottery numbers would be a good one. Or who's going to win the Super Bowl? Or when are my kids going to leave home? But all jokes aside, thinking about data centers, the same kind of process. What kind of questions could you ask? What do you care about? What keeps you up at night? What do you worry about? And when we have those conversations, we get some great answers on great questions, depending on who you're talking to. If you're talking to the CFO, they're worried about CapEx and OpEx and return on investment. Should I go to Colo? Should I build my own data centers? Should I go to the cloud? You talk to IT, they're thinking about availability, thinking about capacity, thinking about projects. We talk to facilities and they're worried about risk, worried about downtime. So there's a lot of different questions depending on where you sit within the data center space. Now, I can't tell you who's going to win the Super Bowl or what are the lottery numbers going to be. But the good news is there is a crystal ball for data centers and it's in the form of digital twins. And at Cadence, we offer that foresight that if you want to know how your data center is going to change and respond, you ask the digital twin. So let's get into it. Before we do, I just wanted to, and this is actually similar to what Martin was talking about and we've seen a lot of this already, but the questions that people are asking are changing and the pressure is mounting on us as an industry. And it kind of comes down to these three different demands that we're seeing. The first one, the resources. So this slide is, thanks to Jensen, out of date by one day. RACs, which were 120kw now, we have to start planning for 600kw. So that is just a magnificent change, but terrifying at the same time. How do we house that safely? So that's the questions that people are asking us. Can you help us manage that and operate that? And at the same time, the capacity demands are growing. So it's getting harder to actually find capacity. Building is getting more difficult. The numbers about sort of construction in co-location and how much is pre-leased is unbelievable. Some regions, 80, 90% pre-leased before the data centre is even completed. So it's getting harder to even get into that co-location space as well, even though there's lots of build. And then finally, legislation is coming for us. The governments are, at least at this stage, making us report more about energy and water use and our resources. And in some kind of cases, there have been moratoriums where governments have said we're actually not going to build anymore because we're running out of power. So these three sets of demands are really putting pressure on the data centre industry, and that's what's forming the questions that people want to ask. Okay. So let's talk about this digital twin. So here at Cadence, we have a reality digital twin platform, and that can help you with those questions and answers. Now, a digital twin is a digital representation of your physical data centre. And so it behaves exactly the same way. It lives, it breathes, it will respond to change. And so that's where you can ask those questions that are keeping you up at night, and it will give you those answers. There are lots to the digital twin, but generally, we talk about it being a multi-physics environment, so it can talk about air, it can talk about liquid, it can handle the different types of coolant. So whichever kind of cooling and power infrastructure you're looking at, we can handle that within the digital twin. It runs at multi-scale, so we can actually now do, in one model, all the way from the chip, all the way out to the chiller, connecting the entire system. So we can do system analysis at a really incredible scale. But as the digital twins get kind of more and more complicated and more and more sizeable, we need to make sure that they stay nimble. So that's where the GPU acceleration is coming in, doing lots of great work with NVIDIA to make sure that the digital twins can give you those answers really, really quickly. And then we do talk about digital twins generally being connected to the live sites, but actually they can start life right at the beginning of the design. And so you can have a conceptual design twin, which then will mimic how your design is going to behave. And then as you move through to operations, we can then start to connect it to live telemetry or any other information that you might be running and pull that into the digital twin so it stays up to date. And as they get more and more sophisticated, we then can embed AI onto that to give you more insight. And I'll talk a little bit more about what we've been developing at Cadence on the AI front as well. OK, so let's dig into it. It's kind of interesting because AI is driving these huge kilowatt changes and we have to then manage that. But it's also nice that we can then help with the problem by embedding AI into the software itself. So the digital twins are getting smarter. And we're really excited. A lot of this has been in development and we're going to be bringing this to market very soon. But it's within the Cadence Reality DC digital twin. We're looking at a lot of AI assistance. So how can we help our users get these answers more quickly but also automate a lot of those questions and answers? So we're looking at performance where we can identify issues. We can give you recommendations and then offer those recommendations. Even run a simulation to tell you what those recommendations will look like. So we can automatically kind of tell you what's going to happen. We're also building out the chat. So you can actually ask those questions in natural language. Get the information back. And that's where it's connected to the agentic AI. So we can actually get the agents to run tools for you, offer those recommendations. So really some amazing technology built around AI coming. One of the bits of development that's very exciting is the capacity part where we can use surrogate models of the data center to train. And then we can just give you pretty much instant simulation results. So what does it look like if I make a change to this data center? Maybe you put some power in a different place, move racks around, and give you almost nil real-time inferenced results. So tell you what it's going to look like. Okay. And we're launching this at GTC. I'm really excited. This is part of the collaboration with NVIDIA. It's the Cadence Reality DC experience, and it's powered with the NVIDIA Omniverse. So I'm just going to play this video, but just to give you a sense of what these digital twins can look like. So this is actually running in the web. It's a real high-fidelity model, so super realistic. I've got to say the NVIDIA guys and the Omniverse team have done an amazing job, because sometimes it looks like the real thing, and you kind of forget where you are. So some pretty amazing technology. Now, under the bonnet, this is all run through USD, so that's an open USD file format, and that's a really nice way to collaborate against different tools that run through at USD. And we've actually started, as part of the Alliance of OpenUSD, an interest group that's just looking at digital twins and USD files and how we can come up with a standard. So if anyone's sort of remotely working within USD and is interested, please come and talk to us about joining that Alliance Open Group, because it's open to members. And I think, oh, it's just going to keep running, which is great. Can I just leave that on loop? No, I think that's it for me, actually. That's done. So thanks very much for your attention. If you're sitting there and you're going, I've got some questions, I'd love answers. Come and talk to us about digital twins, either at the Cadence booth or come and check us out on cadence.com. And I'm being pointed at furiously. Oh, okay. Thank you very much. Thank you. Okay, so if you do have questions, because this is the last session of the day, you can feel free to come up to the mic and ask there. And any four of our wonderful panelists can answer questions for you. So the question I have is, what kind of UPS battery density and capacity and space allocation you think we need to prepare as you're trying to power multi-megawatt type of uptime, like between as you switch from one to the other? You could come here and talk. Mike on the way. Let's see if I understand the question. So it's a density, battery density? Yes, for the 15 minutes. Okay. And the concern is what? Generator to switch over to generator. Okay. Yeah, I mean, certainly a lot being done just from purely lithium-ion batteries and optimizing from a battery or runtime standpoint, right? And that three to four minutes is really what we've designed most of them for. And that tend to cover most of it. In fact, a lot of them, some recent ones we've done, there's been no generator on it because it's been primarily training operations which wouldn't necessarily require a generator. But I'd say lithium-ion batteries is the best option to go with right now, both from a, again, speed to market and complexity standpoint. So hopefully that'll answer your question. Perhaps. I'll come find you. Hey, so you said that Jensen mentioned that there is a new demand with the Rubin Ultra AI of 600 kilowatts per rack. What is the current average consumption right now in a data center? The current average per rack? Yeah. For an AI factory, or AI data center. But imagine what we're seeing is somewhere in the 70 to 80 kilowatts. Okay. So it's 10Xing? Yeah, of course. And probably peak right now is somewhere in the 130 kilowatts. Yeah, just to add to that, right? So GB200 is in the 120 kilowatts range, right? H100s and H200s, you can actually deploy them in a four-server per rack configuration or eight-server per rack configuration. So that'll give you, like, a 40 kilowatt or 80 kilowatt. And then prior generation GPUs, like A100s, again, you would actually max it out by having six, seven, or eight servers in there. So at least over the last two years or so, what I've seen in the industry from, like, a data center for deployment perspective has been in the 40 kilowatt range, going up to 80, and now going up to 120, and then, you know, there's a roadmap ahead for that. Got it. Thank you. Thank you. Hi, guys. Great presentation, first of all. So I would love to understand how are you powering these data centers? If you are power limited, if you're going to rely on utilities to provide power, it's probably going to be a while before they build the infrastructure and transmission infrastructure to power your data centers. Are you seeing a lot of on-site generation? I guess nuclear is there, but it's probably further out, five, six years, to even get permissions, permits for those? What are you doing today in the next two to three years? The industry is getting pretty creative, right? So, like, there's definitely focus on, like, maximizing, you know, what data center space and power people have already access to. So if you look at, like, cloud traditional hyperscalers, right? So they have access to space and power, but a lot of times it is actually dispersed and distributed across, like, a geo and things like that. So for large-scale distributor training, you need concentrated power. You need tens of megawatts going up to hundreds of megawatts, right? So there are a whole bunch of creative things, on-site generation, definitely folks are considering that with gas turbines. Retrofitting of, like, existing sites, going in there and kind of cleaning things up and, you know, routing power from multiple sites into a single location, you know, and taking, you know, historically, you know, industrial complexes and going in there that do have access to power. They're not using it and kind of turning it over. So the industry is doing really, really creative stuff and trying to kind of maximize. At least in the short term, yes, it is going to be bound to, like, what is available and what is, like, being able to kind of turn over. Yeah. So. Any thoughts from you, Martin? Yeah. I'll just echo part of this here. I don't think we've seen any sort of 100% on-prem power generation yet. People still trying to figure out what to do, particularly with refresh of older sites. We're seeing some moves towards fuel cell hydrogen, things like that. I'll say that what we probably see more of is grid interaction. So being able to interact with the grid kind of as a shock absorber of some sort, right? You may have access to renewable energy, may have access to battery energy storage systems, even fuel cells will provide that for you to at least take care of some of the peaks in it. But talking modular, small modular reactors, I think it's always 10 years out when you ask the bureaucrats, right? The technology is pretty solid, but it's just about getting them sider, right? Thank you. Hey, a question for Chetan. So obviously CoreWeave has put some stakes in the ground, I'll say, with choices on networking, on cooling, and kind of what that roadmap looks like. As we're moving into a paradigm where inference workloads are going to take a higher share, I mean, obviously, both are growing, but a higher share might go to inference. How are you thinking about that roadmap and how does that change in that new paradigm? Yeah, so the way we look at inference, at least from a cloud service provider perspective, we have to be careful about what type of inference we're talking about, right? So there's going to be lots of use cases where LLMs are actually going to run on device. They're going to run on edge, right? So if you're sub-10 billion parameter size model, the consumer industry, the professional industry, will get to a point where you'll see those LLMs running locally, right? One of the workloads, there are two types of workloads that will continue to lean in towards cloud deployments, which is like the cutting-edge models that are trillion parameter size. And for those type of models, a lot of the core characteristics that you need on the training side apply directly, right? Where you're not talking about a single GPU, you're talking about a cluster of GPUs that are hosting that model itself, and you're trying to get the maximum performance throughput cost per token down to whatever extent possible, right? So some of the core infrastructure capabilities also apply in that case. We've also built some IP around helping customers better load balance the same cluster across training and inference, right? So if you have, let's say, access to 10,000 GPUs and you want to use the vast majority of them to actually support your daytime peak inference usage, you can actually do that. And when that drops, you can actually fill that in with actually training, experimentation, or other kind of jobs, right? So at least that's the current view of the world, you know, with reasoning as we were talking about. The time it takes for that inference to complete is also extended. Like I recall supporting, you know, in my prior role, Alexa kind of ramp up on their deployments, and those interactions are very short, right? So they're, you know, 10 seconds or so and kind of done. But with reasoning, if they're running, you know, for in some cases for minutes, if not tens of minutes, right, it starts to look more of a batch workload that needs to continue to exist, right? So, you know, it's getting to a point where from a core infrastructure perspective, there are going to be tons of similarities. There are going to be some uniqueness. There will be some applications that are going to be sensitive to latencies, compute capacity, location is going to be important. But fundamentally, you know, cost per token, you know, performance, time to market, all of that is still going to apply. So, thank you. Sorry, I'll be quick. I know that we're close to time. This is a question for Mark on the digital twin kind of infrastructure. So, obviously, with all this live telemetry and monitoring for cooling and power management will require a lot of sensors, will require a lot of kind of these devices in the data center. How will you leverage partners, say, with Vertiv or other power and cooling providers to make sure that data is real-time up-to-date and what kind of design integrations do you need to have with partners in order to make that happen? Yes, good. Great question. So, the software is basically open to connecting to other devices and then pulling information. So, we have that capability to make sure that the digital twin, when it's operational, is fully connected and just continuously syncing. So, it's not just about monitoring and telemetry. It might be, you might be using asset registry or other DSIM tools and making changes in other places and we want to be able to sync all of that data autonomously, real-time, sync it in and then make sure the digital twin is reflective of reality. So, that's kind of how we'll work generally with anyone in the ecosystem and to make sure that we're completely agnostic in terms of whatever tools that you're using from whatever vendors and just make sure that the digital twin can talk to those devices. So, yeah. Got it. Thank you. Hi, guys. Thank you. Very interesting question to Martin and Chitain. Can you comment a little bit more about the distribution or the ecosystem of water cooling? How much of it is open loop evaporative and how much of it is closed loop and like the various considerations for each one? Like for the evaporative, like do we need to build them near water sources or for the closed loop are we considering other types of coolants other than water? So, I'll be honest. Like I don't have a specific data point on that. Like vast majority of core weave sites are going to use closed loop, you know. But again, it varies from industry to industry. There's a lot of time to market pressure also. So, in some cases, vendors or providers might actually choose to use existing cooling technologies and kind of retrofit them only from a space and power perspective. So, the exact split of how that looks like, open loop, closed loop, unfortunately, I just, I don't have that data. Okay. Thank you. Hi, thank you. This is Shenxiong. I have a question for Chaitan and Martin. So, we know that with the AI workload that is ramping up so fast and we are seeing the clusters getting bigger and bigger, we also learned that the fluctuation of the workload is getting so high. Like a 100 megawatt data center can drop to 25 megawatt in terms of milliseconds. So, what is, there are some technologies in the world like supercapacitors, but what is your like vision ahead? Like, how are people going to take care of this problem? Because these problems are not only affecting whatever is downstream, they're also affecting upstream and then further affecting everything that is connected on the switch or on the transformers. Yeah. I can talk about it from, like, the usage patterns that we are seeing from customers that are doing this. So, CoreWeave today, we, for the most part, support large AI labs and enterprises running really, really large clusters. We have some clusters, tens of thousands of GPUs and things like that. So, again, the pattern that we have seen is in some of the alternative use cases, customers will have a separate training cluster, separate inference cluster. The inference cluster doesn't see great utilization, right? So, from our side, like I mentioned earlier, we have allowed customers the ability to mix and match. And let me get into some of the specifics around that. So, like, inference workloads are typically containerized workloads. They prefer, like, a Kubernetes environment so that they can scale up and down. Training, on the other hand, they are batch type of workloads where Slurm is a really good interface for people to actually submit training jobs or experimentation jobs, right? So, on our platform, we have let, we have given the customers the ability to mix and match both of these workloads on the same physical cluster, right? So, what happens is if the inference is peaky and it's maxing out the utilization, in the downtime, all the cached and batch workloads, you know, start to kick in and start to occupy the usage, right? So, we're seeing really, really high utilization across our fleet because, you know, it's a steady state usage with a mix of training, inference, experimentation, fine-tuning workloads and that really helps kind of drive up utilization. And the other factor to consider is, like, cost economics, right? So, if your cluster is only seeing 40, 50% utilization, that's borderline criminal, right? Because you're spending tens of millions if not hundreds of millions of dollars on your clusters and if you see 40, 50% utilization just because it's a separate cluster set aside for inference, it's not going to work. Your CFO is going to hound you down and figure out, you know, how to kind of maximize it. Okay, we're getting the signal we need to wrap up, but, yeah. Oh, I apologize. Thank you. I don't want to ignore your two questions, but if you guys, we can actually take them right outside and we're happy to have them answer more. Thank you guys so much. Thank you. Abbott Oh, thank you. Thank you very much.