 Let me start by introducing a bit about DP Technology. We are an AI for science company based in China. And what we have been doing is to integrate AI, physics-based modeling, and high-performance computing to solve scientific problems and challenges. And one significant achievement that we have made was that in 2022, we used the largest supercomputer at that moment with 27,000 NVIDIA V100 GPUs and simulated over 100 million atoms on Summit and achieved the top performance. And also was awarded the ACM Pornamel Prize. And after that, what we have been achieving is that on one hand, we are continuing this journey integrating AI and physics to solve challenges in problems like drug discovery and materials discovery. And on the other hand, we have witnessed the progress of AGI. So the integration of AI and physics-based modeling and high-performance computing and AGI, larger language models, has made a larger kind of possibility possible. So on this journey, the very first achievement, let me just do it briefly, is the potential molecular dynamics for crossing-scale simulations. And what it did is to go from quantum mechanics solutions to atomic systems and to train a neural network, taking atomic positions and chemical species as input and output energy and forces. And after training based on quantum mechanics, what it can achieve is like several order of magnitude acceleration compared with quantum mechanics solvers. And also, there's a lot of long list of optimization techniques that we did on NVIDIA GPUs. And if you're very interested, welcome to just have a look at the ACM Pornamel Prize paper. And also that marks the beginning of my journey with NVIDIA GPU. And after that, what we have been doing now is that, let's say if we take the AI for science, like progress, and break it into two stages, the first stage has been very focused topic, that is to use deep neural networks to approximate high dimensional functions. And we use it in two ways. One is to use AI and deep neural networks to accelerate computing to better solve physical equations. And deep potential is a very representative example. And another one is that if you have a very clean and large data set and a very clear metric or loss function to be optimized against, then you will have a very good problem to be solved. And alpha-fold 2, which maps the protein sequences to protein three-dimensional structures, is like a model for that purpose. And definitely that's two kind of like early successes. You have like clear data set where you have a very clear rule to be solved. And as we accumulate more and more data, it has been made possible to develop just like large language models to develop like large models for AI for science. In particular, the microscopic models. And along this series, there are three representative ones. One is the DPA series, which is used to learn all these data sets generated by quantum mechanics, mapping atomic chemical species and atomic positions to energy and forces and other properties. And in particular, what we are doing is that we map this kind of relationships and also different kind of labels together. So we developed a multi-task training schemes. And right now actually, we just last year, we released DPA2, which is now covering like more than 70 elements and can be fine tuned to a new system for like accurate PES and MD simulations. And also, on the other hand, we developed this Unimow, a unified framework for a local representation learning. And using Unimow, we have made it possible to better transfer to like quantitative structure, affinity relationship modeling. That is a regression task and also generation task and also searching tasks. So that's Unimow. And also, UniRNA is a large model for RNA sequences. So by using this kind of large models for AI for science, overall, they enable transfer learning and out of distribution predictions. So let me just briefly introduce these kind of models, DPA2. So what's unique or what's interesting in this feature is that going from input to a unified descriptor, that's a shared part of the DPA2 model. And then for different kinds of tasks. So since the community has accumulated a lot of like data from different kinds of labeling techniques, so what we have alloys and molecules and one could use different software or different methods to generate energy and forces. But since they share the same inputs, they can be put together and to train this model and to train and to obtain a unified descriptor. And the mapping to different systems will be connected by a fitting network. And by adopting this multi-task pretraining scheme, what we have achieved is that to go from the beginning, the base model, actually for any kind of downstream tasks, one could achieve better transferability. And also, if one go from like this pretrained model to any specific task, on one hand, what one could do is that just like here. So for any different system, it can be fine tuned and then directly do the simulation. But definitely what one could also do is like to distill the model, the larger model to smaller ones for accelerated like simulations. And for all these tasks, what's different from previous paradigm is that previously we optimized two kinds of tasks or three on GPUs. One is quantum mechanics calculation. Another one is the model training, but a very small model training and also simulation. And this model trained on small system sizes could be adopted and used for very large system simulations. So this inference part can be very large, even though the training part are like used for very small systems. And that's very unique for like microdynamics model training. And what's interesting is that here, the model part, since we are trying to assimilate all this information from different kind of like data sets from different domains, actually the multitask training is now consuming more and more GPUs just like large language models. But like right now, we are using hundreds of GPUs to do the largest set of training. And so the pretraining fine tuning paradigm is now happening on atomic world and atomic models. So we call this large atomic models. So just like large language models are for languages and bits. Large atomic models are for those systems that require a model better modeling from atomic scales. And then Unimo is a model like just like his name is a model going from three dimensional molecular structures that's generated from physics models. And by performing pretraining tasks, unsupervised pretraining tasks like denoiding, one could obtain a unique representation. And then actually this model has been used for various different direct modeling of like relationship between the structures and the properties. And also actually this is under a very small effort. What we could achieve is that by combining like Unimo and the docking capability, one could achieve very good score. And this is a result compared with the uniform, the half of three result. So overall, this model is now becoming like the very popular choice. If one want to obtain like a representation for a descriptor for molecules. So that's Unimo and UniRNA is more like a sequence model, just like large language models. And just like all these pretrained models after pretraining, it could achieve like very top performances on various different applications. Okay. So that's the AI for science large models. They share similar features with other large models, just like IOMs or multi-model cases. But what's different is that they have their own so-called mortality and they have different kind of input and output. Definitely UniRNA, these sequence models are very similar to language models. Okay. So that's for the model part and what we did previously. And right now, what we have observed is that right now for the general topic AI for science is no longer like AI enabled tools for accelerating discovery or accelerating simulations. But more broadly, AI is right now revolutionize the whole paradigm of how we do research. If we look at recent progresses like Google co-scientists were doing deep research. So all these progress are based on AI better capabilities of AI models to read news or all these materials on the websites. And what we have observed is that now AI is unifying three things. Literature mining or AI reading and AI computing and AI experiments. So along this line, what I want to show you is our recent effort on these reductions and also like what are those interesting computing problems emerged from this process. Okay. So let me just introduce to you our product for research and education. That's called Borum. So Borum is a product like the first page. Let me just simply show you. Maybe let me see if I can do it or not. Let me try to show you directly on this webpage. So on the homepage, it's just like a searching tool and one can just use natural languages to do the searching. Let me just use one. And also there are recommendations for papers or like authors and such and such. So if let's say some, let me just try to ask recent progress of deep Mb. And also you could just choose different kinds of models. But if you want to use those better ones, you need to just pay some small amount of like credits. But basically what one could do is like some recent progress of deep Mb. And then it would just not only answer questions based on network website information, but actually behind that is all this different, all this like abundant literatures. And so behind Borum, actually what we have done is that we actually did a large amount of works to do literature mining. And also like for different literatures, if one go into more details, for example, if one is interested in this specific paper, what one could do is also to just talk to this paper and it will just give you some results. And if we download paper and also put it in your library and you could see like there are different categories and scholars definitely. If you search, let me search myself and there are definitely a lot of like names. And this is my first one is me, but there are like 50 results. And what's interesting is that if we look at my profile, you could talk, even talk to me. Okay. So for example, like there's a chat chat bot and Pinfone's recent work on like, for science, let's see. And it will give you answers. And let me just show you more. If you like have have some literature. Sorry, I forgot to put some data inside. And you could also talk to the paper that you downloaded and read in details. Okay. So basically, and this is sorry for the Chinese part. But basically this is about the literature platform. And also if you look at the details and of all this other part, you could just do computing. And for example, here is a DPA based on the DPA2 pre trained model. So this is a tutorial and you could just divide through your own image. And this is connected so you can just run the code directly. And this is connected to the cloud. And we're using an media GPU is here to perform the simulations. And basically this is how we make AI reading and AI computing together on the same platform. And okay, so given this as some background knowledge, let me introduce some basic tools that we need to, to, to, to develop this kind of capabilities. The unique smart is a multi-model analysis and research transformer. Unlike other multi-model like large language models. What's unique here is that you are not only like required to read all these words and the texts, but also different scientific model modalities. For example, the molecules and Markoach formula like here, and also like chemical reactions and charts and tables specifically for scientific. And one need better mining of all these things to develop a better capability. So that's requires like a streamlining the analyte annotation process and also train some small models and such and such. And on the very basic part of it is so-called uni parser, which is used to extract multi-model information from scientific literature. So as you can see, so there's already such a large amount of papers and patents. So 160 million papers and those 190 million patents. So for each paper, what one, one need to do is like, not just directly throw all this information to a model, but one need to just do this partial part. So converting to captions and converting to Markdown and those are commenting equations to latex and converting to the molecules to smiles. And then one need to convert these chemical reaction information to graphs and those are the cover literature to text. And we're converting the tables to latex. So this is what is required to achieve in the first place. And to achieve this, so it's so that you can imagine that for each paper, it's just it's fine to do this. But for a large amount of literature work with such a like different level of complexities. So it's a bit harder. So then what we need to do is like we develop a lot of like different kind of parsers or tools, more parser to do these molecules and molecule reaction recognitions. And those are tabular recognitions and chart to table and such and such. After develop all these things, one need to just do all these kind of integrations like optimization of model deployment, lightweight models used and also quantization deployment. And also one need to just do modular design with slow coupling and parallel computing to simultaneously just deal with one paper with all these kind of capabilities combined. And also definitely high concurrency requests processing and also during high concurrency requests, the time for communication and computation overlaps. So basically what we achieved is that after all these kind of optimizations, we achieved top performance performance in terms of like average speed and also like the accuracy of parsing all these different different kind of like cases. So this is just the first tool that one need to use. And imagine there have been like like tens of thousands of papers coming to the world every day. And also like even for those open access ones, there's a lot. So if we want to develop a tool like for all users, for all these researchers to read papers, let's just imagine like each researcher read one paper per day, then it could be like millions or even more like times of like parsing jobs that one need to just do. So this is the beginning of like transferring the paper to a PDF or to a model. So actually, so there are so many like inferences required. And as I, so recently I think so the optimization of this tool is very important because it's just so closely related to the cost that we provide the serving. And it's even like the cost is like this part is even more consuming, more like compute compared of more like, deeper and also more compute intensive tasks, simply because like this is used by everyone implicitly without recognizing that they're using it. So this is UniParser and then after AI reading so then there's AI computing and then for AI computing let me just introduce to you this ermid one-stop computational platform for preclinical drug discovery because there's so many different kind of computes and functions on ermid and which serve different kind of computational patterns and to go from a molecule or a protein or target to like a hit and then to a highly optimized molecule. What one could do is that at different stages from the scratcher of a protein to the pockets to screening and to lead optimization and to property predictions so there there's just a lot of like different kind of computes combined so actually on top of this in the first place what we are trying to do is now to go from all these patterns using UniParser and then our multi-model UniSmart to extract for each target all these molecules related to it and also their functions and their properties and that's is now wrapped into a database that we are going to release soon and going from the target then one will have the tool to predict the structure just like AlphaFold. UniFold is our own version of AlphaFold and it's a model inference and also definitely there are MSAs that play a that is searching intensive and then going from structure to screening so there's one more part that if you want to study the movement the dynamics of this protein you need record dynamics but record dynamics the time scale required is so large that one needs enhanced sampling and reinforced dynamics is the technique that combines deep neural networks to accelerate and the the free energy profile and the energy landscape of the system so that one could just accelerate the simulation so so this is then highly optimized on GPUs and then screening is for which once we identify the pocket we just quickly screen like hundreds of millions of molecules and this is the part that I will go into details later and this is definitely like very small jobs but hundreds of millions of small computing jobs just like those on cloud and then after identifying leads when one do leads optimization UniFDP is a tool that is very computationally intensive and also compute the optimization is definitely very specific for this kind of physics model based simulations and then UniqSAT so this is part property predictions ADMET properties are hardly predicted by physics models but basically based on the data that we can acquire we could just better leverage their the information inside by using large models like UniMod so based on UniqSAT so basically you can see that at different stages of this pipeline computing infrastructure is always important but at different stages the compute computational pattern is very different okay so then let me just introduce you to the point okay so then let me just introduce you UniDoc and and and and here I have to say that I'm I feel very grateful to the team from Nvidia that we work together to make this UniDoc a very powerful solution to like this high throughput screening of the drug industry so what's achieved here is that there are several steps to go from single molecule parallel search what we do is that we optimize confirmation count and iterations and use parallel spreads to enhance efficiency and then what one do is that we do multi-molecule parallel search so what we do here is that since each docking the task is just a small task so when we combine hundreds of like molecules docking tasks together and dock them simultaneously and then one needs to leverage better leverage memory allocation for faster computation and then the optimization part is that here what one need to do is that we we need to minimize communication cost and optimize memory and task scheduling so putting them together so it's it's a balance between the throughput and the kind of global search and local minimizations but with a like good combination of these considerations one achieved very top of performances so compared with like all these other schemes things so what what what what we could achieve right now is that um one just do this kind of like docking um for like one can do docking routinely for like a hundred million molecules um just a decent amount of gpus that's what we can achieve right now okay so then here what we we we have achieved is that right now we we are also integrated into the generative ai virtual screening um tool sets on nvidia and also right now uh we we are actually working hard to make a better version and we are very close i think we'll be releasing this um a new version by the end of march this month and it will be better optimized for like a drug optimization program in particular it will be highly optimized for the next next several steps like uni fep okay then comes fep so computing wise fep is a very very different computational task compared with uni doc or uni doc or uni mo so remember uni mo is like more like a more large large model and it has a clear stage training and pre-training and fine tuning for downstream tasks and those then uni doc is like a high throughput job a uni fep is very computational uh noly intensive so we actually optimized a lot um all these details detailed implementations on gpus and competition of the interactions between different atoms and molecules and also what's interesting is that we also customized the force field by to by fine tuning it with the large atomic model gpa2 and with all these optimizations it's now performing on par with like a lot of different uh a lot of like different tasks it's it's performance uh i think it's doing a very good job okay just this is an example and after developing the production maps and one can do this simulation and collect results so the key advantages include the performance on par with fpp and what's also interesting is that by integrating this with unimow one can like largely increase the the the the the the coverage of of of of of of molecules that one can achieve and also there's no limitation on tokens and also several other advantages so then by doing all these uh optimizations on the performance on gpus and also sampling techniques and force fields overall right now i think uh uni fep is becoming a more and more competitive uh solution uh on drug development development industry so we also like um compare we also benchmarked a scheme on a lot of like different targets and i think now we are we already like released uh like the largest benchmark uh we are about to release it like um very soon um um and it's like a benchmark against more than 100 targets okay and here's an example of dealing with like this complex complicated gpcr systems and achieving good results okay so combining this physics-based tools like unidoc or uni fep and the model but more like data-driven tools like unimow one could do of some even more crazy things like generating data here a lot of data and fine-tuning the model and um developing the closed loop so this is um and we what we achieved includes that we achieve best ai docking model before effort three okay okay so that's about like our practices from ai reading to the latest part of ai computing and there are several ongoing programs so i do think that this year we will observe some gp3 moments on the ai for science large models so dp3 is amounted so right now we already have like a fairly good model that is ranked the second on the matbench discovery and what i also want to comment on this benchmark is that actually so remember we are um developing this multi-task scheme and we we still keep this model of physics one in the sense that it has the energy continuous and also symmetry preserving and also it has the forces as the negative gradient of the energy against the input atomic positions and so it's a physics model and it can be used to perform record dynamics simulations and also it could be used to directly do a mapping to like experimental uh properties but there are several other models for example eqv2 developed by meta so basically uh it's more like a purely data-driven model without like this free physical physical property guarantee so it's it can hardly be used to perform simulations and basically definitely i think there are there's still a large number of very active explorations along this line but overall what we observe is that the gp3 moment um like in the field of ai for science is coming and also what's interesting is that previously i think our starting point is the largest computer computer with the most the um like the largest number of gpus 27 000 so we are very much focused on compute and then as since the capability of large models is increasing dramatically and now ai reading combining the capability of the base model and the uni parser and uni smart and all these scientific modalities we are now having very good ai reading tools but ultimately for a lot of tasks like materials and drug discovery whether you can make it whether you can make and also after testing a very good candidate for drugs for a lot of data is making the most significant difference so the real bottom knock is to go from compute or to for designing to making and testing and if the experimental process can be made better automatic it's just like in embedded intelligence that we we we focus on like lab experiments so if this part can be made better then what we can imagine is that we we have a better close loop of this optimization process that will be like combination of this dry lab and wet lab and integrating this part what one will achieve is this kind of new infrastructures new computing systems that combine different kind of computational patterns and also new experiments systems that makes all these making and testing processes automated and intelligent and this will be something that we are look very much looking forward and we'll be optimizing all these different computing patterns arise from this process okay and also by realizing all these kind of factors we have launched the so-called open lab initiative and we have like 10 very top researchers practitioners on our scientific adversary group list and so so basically what what the key factors are the data and feedbacks are the data and feedbacks in terms of scale quality and rates so in terms of this what what what one observe is that the computing part is something that we we can achieve the largest amount of feedbacks and we can generate data in the like controllable more more or less controllable manner but and that's something achieved fairly achieved as of now and right now and right now what's harder is that to integrate like for example characterization data from experiments and also literature data so this year is a year for like all literature data and those experimental data that is like generated easily and then ultimately what one could imagine is some so kind of such kind of such kind of like general purpose capability for generative design and also for generative design and manufacturing so that's something that we are trying to achieve and by combining all this part literature computing and the experiments the all this paradigm of scientific research will be changed changed okay so with this i would like to thank you for listening to my induction of our recent progress