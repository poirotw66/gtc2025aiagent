 Now it's my pleasure to introduce to you Nick Becker who is a senior technical product manager on the Rapids team at NVIDIA Where his efforts are focused on building the GPU accelerated data science ecosystem Prior to NVIDIA Nick worked at Enigma technologies a data science startup and before Enigma He conducted economics research and forecasting at the Federal Reserve Board of Governors the Central Bank of the United States Thanks well as well said my name is Nick I lead product management at NVIDIA for our accelerated data science libraries and I'm excited to talk with you all today about Rapids and our accelerated data science ecosystem in 2025 Going over really what's been going on over the past six to twelve months and what's work in progress right now That we think is particularly compelling and important for you all to be aware of We'll do this as a broad survey Dipping into different pieces of the ecosystem and then we'll leave time for Q&A at the end So as a general overview we'll cover a few different things but really in two broad categories First we'll do an introduction to kind of set the stage for what we're talking about today Then we'll cover library developments across three different categories starting with data frames in sequel shifting to machine learning and graph analytics or advanced analytics and Then moving on thank you moving on to the world of vector search and large language models After that we'll shift gears again and go toward not just library developments but developments in the broader developer experience So things like tools and platforms and ways you can tap into this technology Then we'll end with a summary So just to kind of kick things off I think it's useful to level set and why we're talking about accelerated computing for data science When we look around we talk to companies in every industry really one thing becomes clear The world needs higher states and it's driven really by three things The first is that data sizes and data sets that people are using are exploding both in size and dimensionality and That comes not just with the existing world of data. We're processing but new types of data with new complexities that they bring and that we have to handle At the same time as this explosion in data size and complexity We're also seeing the need for faster performance sometimes even real-time performance So think about use cases like fraud detection and risk management in the world of financial services and credit card transactions When I swipe my credit card, I want to be confident that it is going to work smoothly But my bank Also wants to be confident that it's me swiping that credit card or using it online Every transaction that's going through the system needs to make sure it's being utilized to build the best fraud detection models but those models need to be used quickly with all of that data to make sure that It's going to be done as soon as I swipe my credit card through the system. I don't want to wait for that to be done We see it in every industry from fraud as we just talked about to recommendation systems where you've got consumer internet companies with potentially hundreds of millions or billions of users and millions or hundred millions billions of pieces of content Just the cross product of those two things is too much for standard CPU architectures to handle and in every industry We see this to be the case whether you're doing scientific computing Classic enterprise computing sensor data in the field the need to go faster is only growing We need a new platform beyond what we've already had with CPU systems and Rapids is a foundational ecosystem of open source libraries Designed to accelerate data science end to end to give you the opportunity to tap into accelerated computing for all of those different use cases We just went through It's an ecosystem of open source libraries for things like data loading and transformation or ETL model train analytics as well as vector search and deployment optimizations It's built on core foundational libraries bringing GPU acceleration into the open source data science and data engineering ecosystem and it's backed by Nvidia enterprise video AI enterprise excuse me so that you know that well of course you can tap into the open source capabilities if you need that Enterprise grade software. It's there for you through Nvidia AI enterprise Now Rapids is designed to work end-to-end, but it's also disaggregated you can pick and choose for your use case Maybe you and your company are all in a CUDAF the CUDA data frame library that powers data loading and structured data processing Is integrated into the rapids accelerator for Apache spark Maybe you're actually really focused on graph neural networks. That's okay, too Coo graph the accelerated graph analytics library Helps power PyTorch geometric for scalable graph neural networks Now this ecosystem is Part of a broader ecosystem Rapids is not alone It fits into the broader data science ecosystem like this Rapids in this case we're focusing on just three libraries It's a suite of GPU and see excuse me GPU accelerated Python and C++ libraries built on top of CUDA It's part of the CUDAX ecosystem and you can use it just like that you can use CUDAF for speed of light optimized GPU data frames or QML for accelerated machine learning But you can also use these libraries through the tools you might already be using in The open source data science world today We wrap these up into zero code change required experiences for Apache spark so that you can keep using your Apache spark workflows and Tap into Nvidia accelerated hardware libraries like pandas and polars and more GPU accelerated workflows are available with zero code change change so with that I'm gonna shift and walk through some of the Developments in this ecosystem over the past six to twelve months and show some what's excuse me show some of what's work in progress right now To start we'll dig into data frames and sequel Starting from this the world of single machine problems then going to the world of scale-out problems And when you start with single machines and data frames and data science There is no better place to start than pandas pandas is the most popular library for data frame analytics in the Python world with millions of users It is the first choice for so many data scientists and It is now fully possible to GPU accelerate your pandas workflows with zero code change required If you're using a notebook you simply load an extension at the top if you're using scripts and look at the command line you just flag it to Python when you want to run your script and you can get acceleration of up to 50x using GPU acceleration built on top of QDF that foundational library that you don't need to think about you can just use pandas and tap into it Of course, this is something we've been working on for a while and it's now generally available Now this ecosystem is a lot more than one library. It's a whole powerful set of libraries that make the Python data or py data ecosystem great One of those libraries more recently that's become popular is polars We announced in the fall that we had GPU accelerated polars and it's now available in open beta Giving you the opportunity if you like polars to process hundreds of millions of records in just seconds getting up to 13x performance gains and speed ups on video GPUs and again zero code change required you can Trigger the GPU execution on a per query basis or per operation basis or you can configure it globally at the top of your workflow Once you do it everything is the same and This whole ecosystem whether it's pandas or polars or spark We know that When you want to go to GPUs you want to get the best performance you can But you also want to make sure that there are safety guard rails in In that workflow so maybe if one operation or the way you're doing something isn't fully supported It doesn't just throw an error so it's gracefully falling back to the CPU for any query with polars or pandas or even apache spark Where the GPU couldn't quite be used in the way you wanted to so you get a truly seamless experience Now this is the single GPU world and I mentioned that CPU fallback One of the things that folks especially when you're they're using maybe smaller GPUs run into is that memory can be a challenge One of the things that we've done in the past few months is make it possible to scale beyond the GPU memory limit Even on these single GPU workflows by using something called unified memory This is available on all modern Nvidia GPUs But it's a capability that allows you to Locate more memory than you have just in the GPU by using the whole system as As if you had more memory than your GPU has This is pretty powerful because it means that you as a user can go much larger than you need to do Which matters in data science where workflows that you know might process 5 10 20 gigabytes of data Might actually have to spike memory up to four times as much depending on what you're doing think about a join where you're combining data sets together Unified memory is something that we've been working on for a while as a company and it's also something that's configurable So over the past few months we have been optimizing unified memory for data processing and now as a result you can do things like this Well, unfortunately, I can't play the video But it was maybe there's an internet issue going on But what you would see in this video is that in a workflow for using pandas in the CPU versus pandas in the GPU Even though you're processing eight gigabytes of data on a 16 gigabyte GPU and needing to spike memory up to 20 or 30 gigabytes You can still get a 30x speed up with your same pandas code even though even though you're running on a small GPU Oh looks like the internet's back now Now this capability is available by default for accelerated pandas and polars In the same theme shifting a little bit more to the world of scaling out Unified memory was really powerful when we have these pandas and polars workflows, but sometimes we have to do more than that that. When we're scaling out with tools like for distributed data frame analytics in Python, we often need to do I.O. in the cloud. And I.O. in the cloud can be challenging. Remote storage is often more difficult to get better performance out of than local storage. But it's so helpful and so convenient. Well, over the past three to six months, we have been optimizing remote I.O., I.O. from remote storage, things like Amazon S3 and those kinds of remote storage platforms for distributed data frame analytics in the cloud. And as a result, now these benchmarks are using our April release, our nightly packages that will be available in April. With QDF and Dask, compared to the status quo, you can now get four times faster I.O., reading data, from Amazon S3 than you could in the past. And this is fully configurable. You can see on the left that up to 4x speed ups on one workflow. On the right, you can see that there are multiple different knobs you can tune, whether that's the number of threads you're using or the task size. And I won't go into all the details of what that means, but at a high level, every workflow is different. The way you'd optimize something when you have to read 10 gigabytes of data or 10 terabytes of data might be very different. And so we give you the freedom to do that, but of course we choose what we think are sane defaults. In the same theme, if you're doing distributed data frames in ETL and Python, you might also be using Apache Spark. The NVIDIA Rapids Accelerator for Apache Spark is designed to be a zero code change accelerator for your Apache Spark workflows today that you might be using CPUs for. It really does three things. One, it gives you a chance to speed up your Apache Spark workflows using NVIDIA infrastructure, whether you're on the cloud or on-prem. Two, it gives you, through those speed ups, the opportunity to save money. Data processing at scale, particularly in the cloud, is often focused on performance per dollar, trying to get through all of our data pipelines efficiently and cheaply. By bringing those Spark workflows to accelerated computing, you can lower costs and reduce, if you're on-prem, power consumption, cooling, and your broader carbon footprint. And third, with those speed ups, you can get quicker time to value. There's just the fact that productivity, performance, and cost are all possible with Apache Spark through an accelerated infrastructure. Now, what does that look like in practice? In practice, we can see here some benchmarks from what we call the NVIDIA Decision Support Benchmark, running at a three terabyte scale. This is a benchmark of about 100 different SQL queries that are derived from an industry standard benchmark called TPCDS, but not explicitly comparable to it. And these 100 queries on CPU Spark take about six times as long. You can get a 6x speed up just by the same Apache Spark workflows to accelerated infrastructure. And that translates to up to an 80% cost saving. And that means you can do more with your existing infrastructure. You can bring more pipelines into your production workflows and just save money and use it elsewhere in your business. I'll have a little bit more to say about Apache Spark when we talk about new developer experiences later in this talk. So shifting away from the world of data frames and SQL to machine learning and graph analytics. Before talking about what we're doing with accelerated computing here, I want to actually focus on the world of CPU computing. There's incredible innovation happening across the space of AI. We saw much of it today in the keynote. In the world of tabular data, Scikit-learn, the workhorse machine learning library we've been using for the past decade, is still the number one most popular library. It actually has, based on surveys, more than a million and a half users today, which is incredible. It's got millions of monthly downloads. It's actually growing at a clip of almost 50% year over year. And it's because tabular data is everywhere. Whether you're doing classification, regression, clustering, all sorts of different things, Scikit-learn is a fantastic default choice for data scientists across the world. But in the past few years, it's been, oh, excuse me, complemented by new tools and new techniques. In particular, techniques like UMAP for dimensionality reduction and HDBScan for clustering have not just become new techniques for doing these tasks, but libraries themselves to complement Scikit-learn. So now you have tools like UMAP, which is a library for a very specific type of dimensionality reduction that's been shown to be very effective at projecting high-dimensional data into low-dimensional space while preserving a lot of that structure. And HDBScan, a new density-based clustering technique that's become very popular for these kinds of workflows as well. These tools complement Scikit-learn. And together, all of these tools ultimately need to be used not just once. Whether you're doing classification or clustering or dimensionality reduction, you're running these algorithms many times. Maybe you have a model that you're trying to optimize for the best quality metrics, like accuracy or precision. You have to train many iterations of that model. Same thing with UMAP. Understanding what's the best projection of your data in low-dimensional space. Unfortunately, going back to where we started, this doesn't work when you have a lot of data. You end up being bottlenecked. Pure performance and, of course, on cost. We need higher performance data science, and we need higher performance for machine learning. QML provides that higher performance. It's designed to be that tool to give you accelerated computing for machine learning. And today, we're very happy to announce that QML now accelerates this world of machine learning with zero code change required. You don't need to worry about GPUs or CPUs. Write your code for any specific hardware, specific libraries. You can keep using your favorite Python ecosystem tools, specifically Scikit-learn, UMAP, and HDBSCAN. And with zero code change, you can now accelerate your workflows with NVIDIA infrastructure. Now, just like with QDF Pandas, you can do this in a notebook by loading the extension. You can do it at the command line by using a Python module flag. And you can tap into speedups ranging from 5 to 200x, depending on the algorithm. And because we implement this in a way that is designary for imports in mind, it's not just your code that gets accelerated. Like this broader ecosystem, any third-party library that you use that might call down to these Scikit-learn or UMAP or HDBSCAN function calls will also be accelerated without you needing to do anything about it. And of course, for unsupported operations, there's graceful CPU fallback. So let's take a look at what this looks like. We're going to give you probably 3ses ofoles, Sascha, and Mus' hull to 7, especially in the system we're in this ok. The capital pm released in the wykizs, to 10 implanted models and so on to the wingteaus that can be carried out. And of course, we're going to be able to change the processes of using a functional component in the setup of today-pan So let's take a look laser-player, let's look at this, ever formula-fed is a trickplus arched tool. So let's take a look. We're going to be able to change the tests that allow us at research! Take your info for each step to see that everything you have konst handy for passing, and you feel free doing business first. We're going to get a chance for邊 Step 3 asода, trying to go through and get the educational software function. So we're super excited about this. It's now available today in open beta. You can go install this on your own systems. You can also go get it by default in Google Colab. It's now available for you out of the box. So you can start accelerating your machine learning workflows with no code change required. Now, one thing I want to mention here is what it means to be no code change. In the world of ML, we are creating artifacts, models. These models now can be deployed on CPU or GPU infrastructure, regardless of how you train them. And we're super excited to see what everyone does with this. And it doesn't end there. We just talked about Python. But this zero code change capability is also now available for Apache Spark ML-based workflows. So we've been kind of going back and forth in this talk between the PyData world and the Spark world. We want you to use the world that's most effective for you. We accelerate all of the ecosystem. And so you can now access these zero code change requirements, excuse me, zero code change required UXs for Spark ML, and get these big speedups. And so if you're using Apache Spark today for machine learning, you can simply Spark Rapid Submit, and away you go. Now, shifting to a very specific type of machine learning, tree models, one of the most popular libraries for tree models in the world of Python is XGBoost. It's a gradient-boosted decision tree library that is often the first stop for anyone who's doing classification, regression, or quantile-based learning, or anything like that for complex problems. For many years now, we've been working closely with the open source community to enable XGBoost to run smoothly on GPUs for up to 10x or more speedups with no functional code change. Just configure XGBoost to use CUDA. Well, XGBoost 3.0 is coming out shortly. And XGBoost 3 is a big change. In particular, it's focused on pushing the scaling limit for gradient-boosted tree models. For many, many use cases, things that fit into memory work well. If you've got medium-sized data sets, you can run XGBoost or any algorithms, and as long as it fits in memory, it works well. But for the largest problems, that becomes a challenge, because scaling out works well but can be communication-bound. If you have many nodes in the cloud, things can become expensive. If you're on-prem, you have to manage that infrastructure. When we talk to companies across the board, people want to know. And so in XGBoost 3, we have fully redesigned what's called the external memory interface that makes larger-than-memory data sets now something you can train XGBoost models on. Now, this is a big change, and it relies on a whole bunch of work the open-source community has done over the past six months. And we've, alongside of that, optimized it for coherent memory systems like Grace Hopper and Grace Blackwell. These systems have super-fast CPU-GPU interconnect through things like Envy Link Chip-to-Chip that allow us to buffer data back and forth so quickly that we can actually train out-of-core XGBoost. So we're super excited about this. One of the things that it enables, among many, is that a single Grace Hopper system, a single Grace plus Hopper GPU, can now handle up to one terabyte or larger data sets. So when you make that investment in a Grace Hopper or a Grace Blackwell system, you know that you can scale XGBoost to the largest possible data sets. Now, the release candidate for XGBoost 3 is now available. We're going through with the community on final testing, and the official release should be coming shortly. In the same world of tree models, the other side of that coin, you're training the model. You have to deploy the model. Across the board, in every industry, we see people optimizing their tree models for deployment using something we've been developing for many years called the Forest Inference Library, or fill. Fill provides accelerated inference for tree models, namely XGBoost, LightGBM, Scikit-learn, Random Forest, things like that. And it's a fantastic tool. But as part of all the work we've been doing on tree models, we have optimized fill from the ground up, bringing new features and much faster performance, up to 4x in some cases, and a median performance gain with this new Forest Inference Library of up to 40% more, on app, excuse me, a median performance of 40% more compared to the current fill. That brings us to a median performance gain over CPUs of about 28x. Now these performance gains are great, but there's also new features you can tap into. In particular, there's a new CPU execution mode that lets you take your models that you want to optimize for CPUs and GPUs, and fully optimize your pipelines on CPUs for testing or lightweight deployments, and then switch smoothly to GPUs for optimal performance. Maybe you want to maximize throughput or minimize latency. No problem. The new fill is designed to make that seamless. It also comes with an auto-optimization capability, specifically for folks who need to find the right tradeoff between latency and throughput. No more manual hyperparameter tuning. You can just call the optimize method on your model and be confident it's going to be optimized down for you. For explainability, particularly in regulated industries, there are two new features that are something we're very excited to share with folks. The first is the ability to find out exactly what nodes correspond to different parts of a tree within a broader ecosystem, excuse me, a broader umbrella of many different trees in a model. And the second is the ability to analyze which trees contributed to the output of the model most, which parts of the tree mattered. This is all going to be available in our new forest inference library. You can actually access this experimentally in our nightly packages, but it's going to be available in the stable releases coming soon. Shifting away from tree models, out of the world of ML into graphs, we've been working for a long time with the community on accelerated graph analytics through NetworkX, the Python ecosystem's most popular library for graph analytics. About three or six months ago, in the past few months, accelerated computing for NetworkX, built on top of Coup Graph, became generally available. That means you can go access this in your favorite package manager through pip, conda, containers. It's available. You can learn more in the NetworkX documentation, and it's ready for production. It accelerates NetworkX algorithms up to 600x, of course, depending on the algorithm. And again, it has graceful fallback to CPU if one of the algorithms you need isn't supported. You can tap into this anywhere you're doing your graph analytics today. And it's built on that foundation. Hopefully that's a theme coming through today. Core foundational libraries and rapids that you can use on their own, but make it accessible to you through your favorite tools, like NetworkX. Like NetworkX, like Pandas, like Spark, like Scikit-learn, as of today. And that KooGraph foundation doesn't just do graph analytics for traditional graph analytics workloads. It also now supercharges PyTorch Geometric for optimized and scalable graph neural network workflows. Using KooGraph-powered PyTorch Geometric, you can now get up to 3x speedups for end-to-end training workflows at very large scale for the most significant graphs. Now it's built on top of KooGraph, core algorithms for the supercharging of PyTorch Geometric, as well as a companion library that we have called WholeGraph. And together what these do is make it possible to run very efficient graph sampling algorithms and manage giant graph features that we have to randomly walk around every time we want to go through an iteration of our training loop. Now these huge graphs require more than GPU memory for the largest scale to do it efficiently. And so we can actually do that. We can spread these graphs across the whole system, the CPU and the GPU system. Whole graph. And doing that in a way that's optimized for Grace Hopper and Grace Blackwell. Again, the super tight coherent memory with NVLink chip-to-chip interconnect makes it possible to do this efficiently. And the result is that end-to-end you can see huge speedups versus PyTorch Geometric on GPUs today. This benchmark of the 3x is on about 1,024 GPUs. And even for smaller and medium problems, on a single node, compared to x86 Hopper, being able to use the coherent memory of Grace Hopper or Grace Blackwell can give an order of magnitude speedups as well. This is fully compatible with PyTorch Geometric. We work closely with the PyG ecosystem to make sure this is smooth. And you can get this today with pre-built containers on NGC. So shifting to our third category, vector search and LLM workflows. We've talked a lot about data science outside of the context of LLMs and RAG. But I actually don't think it's right to separate these. Accelerated data science, data processing, data engineering. This is critical to successful operationalization of AI for large language models. On both sides of the house, whether you're doing the training and development, or if you're trying to deploy them alongside a broader system with things like RAG. When you're developing LLMs, you need to make sure that the data you're feeding into that model is high quality, it's been pre-processed to make sure that your model is not learning the wrong patterns. Because ultimately, the data in determines what's going to come out. And once you're using that model, when you're doing RAG or retrieval augmented generation, you need to bring additional information to that model from across your business. And that information is something that needs to be fresh. And that means you have to keep updating it all the time. You don't get to just take a snapshot once and say, oh, it's no problem. The 3,000 documents we're creating every day, we just won't include that. That's not going to work for anyone. We have to constantly be iterating and updating. And accelerated data science and vector search is designed to make that problem better, too. So starting with the model side, Nemo Curator, part of our Nemo ecosystem, is a scalable, configurable toolkit for processing data to help you train large language models and AI models. Curator is specifically designed to make sure that you can build the highest quality training data sets with as little effort as possible. It gives you the ability to tap into all this great work we've been talking about over the past 25 or so minutes or 30 minutes in a smooth, simple interface so that you can build the highest quality model you can, taking advantage of advanced techniques like semantic deduplication, model-based filtering, fuzzy deduplication, and so many other different techniques, all accelerated, which gives you faster results. You can get a faster iteration cycle, your teams can experiment more, build higher quality models. But it's also cheaper when you're operationalizing AI. And Nemo Curator should be the first stop for anyone who's thinking they want to curate and preprocess data for AI models. On the other side of the house is KUVS, KUDA Accelerated Vector Search. NVIDIA's KUVS library accelerates vector search index building and the actual search itself. It's not a vector database. You can see below we've got, we work with the ecosystem of partners. KUVS sits underneath your favorite vector database. And it allows you to tap into the best performance for building your indexes for your vector database. The best performance for minimizing latency or maximizing throughput at a given latency. And like the other things we've been talking about, it's flexible. CPU or GPU. You can interoperate between your graph-based algorithms on your CPU and your GPU smoothly to fit your deployment paradigm. Again, we are accelerating vector search, accelerating vector databases through our favorite, excuse me, through your favorite vector database partners. So if you love Milvis, that's great. If you are in the Weviate ecosystem, if you're using OpenSearch, KUVS accelerates the whole ecosystem. So you can use the tool that makes sense for you. So those are some of the key updates around accelerated computing for data science libraries. I'm going to shift for a few minutes and talk about new experiences that you can have and new tools and platforms. The first is documentation. Something that we don't always think about until we need it. When we do, we really want it to be good. We have put together comprehensive guides and deployment documentation for getting started with accelerated data science wherever you're doing your work. Whether you're on your local machine, your laptop, your workstation, you're in the cloud, or maybe you're on an HPC supercomputing center. The deployment documentation available on the RAPIDS website is the first place you should go to quickly get up and running and find out how to fit accelerated computing into your workflow. Now, it's not just deployment documentation. There's workflow examples, there's getting started tutorials, and everything in between. This is, in my opinion, the best kept secret of accelerated data science, the documentation out there. And we've done a huge amount of work over the past six months to make this great. We're really excited to see what people do with it. Shifting for a moment. I mentioned earlier when we were talking about Apache Spark that I was going to share a new developer experience for Apache Spark. We are excited to announce today, and it was actually announced just about an hour ago today, something that we're calling Project Aether. It is a new tool to automatically qualify, evaluate, validate, and optimize Apache Spark workflows for GPU acceleration. Now, Apache Spark workflows are not often things people just have one of. It turns out many organizations have thousands of Apache Spark workflows. And, you know, we can't put them all on GPUs at once. We have to make sure we're using our time well. Well, Project Aether is designed to do this all automatically. It automates all of the various steps that companies previously have been doing manually, such as figuring out which of their workflows are the best fit to start with. Where are they going to save the most money? How do we tune them? How do we optimize them? Make sure that we are doing this in a staged production rollout. Aether gives that to you through an automated process. Starting with a Spark event log, you can send it off to the Aether and have it qualify your workflow, run it on a GPU cluster that runs in a sandboxed environment, so you know that you're not too quickly going to production. In that sandboxed environment, we use to do some test runs, get the output, compare the results, figure out how we should tune it, and then we do the tuning. You can optimize this process without needing to have yourself or your teams be the one to take every little step and evaluate. Aether gives you the opportunity to automate this process of qualifying, validating, optimizing, and deploying Spark on GPUs for performance and cost savings. We're very excited about Aether. If you're interested, please see me after. It was announced today. And we're looking for folks who are interested in early access. Shifting away from the Spark ecosystem back to the higher level. Many people use JupyterLab. JupyterLab is probably one of the most popular development environments for folks in the data science world. Envy dashboard has been redesigned over the past year to make it so that you can get the most out of your JupyterLab experience when you're using GPUs. It gives you all sorts of metrics about GPU computing and utilization. So you can see when you're running your ML model using scikit-learn on a GPU or running PyTorch, are you using your hardware effectively? Can you get a sense of what's going on? And it gives you the opportunity to do this right from your notebook. You can install this today. We recommend it for everyone who wants to make sure they're using their GPU hardware. We also, in the same realm of combining data science and AI, NVIDIA also provides unified deep learning and data science containers on NGC. Now, these containers are not just your normal container. They're actually optimized for the tip of the spear performance and CUDA capabilities every time you get the new container. So if you or your organization are looking for the absolute most performance you can get, these are the perfect containers for you. Another platform I want to highlight is Google Colab. There's a talk on, I think, later this week that you'll see Paige Bailey from Google talking about this. But we are very excited to work closely with Google Colab to bring accelerated data science into the world of their Gemini Coding Assistant. For folks who are not familiar with Google Colab, it is a hosted notebook platform that Google provides with access to CPU and GPUs. It's a fantastic tool with millions of monthly active users. And you can see in the top right that I'm using a T4 GPU. Now, I know I'm using a GPU. Shouldn't my coding assistant know I'm using a GPU? Why wouldn't my coding assistant be able to know that I'm using a GPU and recommend and generate GPU accelerated code? It should be smart enough to do that. We're very excited to work with Google to make this a reality for Google Colab. We're also working with Snowflake. Accelerated data science in Snowflake Notebooks is now a fantastic experience. Snowflake Notebooks, built on what's called the container runtime for machine learning, is now in public preview. And it's a fantastic place to start with your data science workflows in Snowflake. So to conclude, we've covered a lot. We started by walking through the fact that across every industry, we need higher performance for data science. It's not just about AI. AI and data science go hand in hand. And we need higher performance. And Rapids fits into this ecosystem to bring you that higher performance however you want it. Whether that's through GPU-specific optimized libraries or GPU-optimized libraries underneath your favorite tools. And it's also not even just about Rapids. It's an entire ecosystem that's embracing accelerated computing. There are hundreds of integrations for computing libraries in the general purpose domains, application-specific libraries, and deployment tools. These hundreds of integrations are worked on by hundreds of GitHub contributors with millions of downloads, bringing you accelerated computing for your data science workflows wherever you're doing your work. Thank you all. Hopefully you've learned something today. Happy to take questions. And we'll go from here. Thank you. Thank you.