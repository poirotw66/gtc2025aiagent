 Hello everybody. Welcome to our presentation session for NVIDIA GTC Conference. My name is Martina and I'm from Jaskawa Europe. And my name is Benjamin and I'm from the German Research Center for Artificial Intelligence. In our today's session we would like to show you how we dealt with industrial robot and how we did easy programming and how we did the robot low-code deployment. So just quickly through our agenda, we will talk about motivation for this work, why we chose this topic, then we will show you a system architecture that we have used, and then in the next step we will show you how the AI training was done by Isaac Sim of the products that are in front of the robot. We will shortly talk about the concept and encapsulation of skills and then we will jump directly to behavior trees in industrial robotics. At the end we will show you three videos as a demonstration and then we will wrap up our talk in a conclusion and further work. Okay, so the motivation for our work was actually the industry industry. So we have a lot of work that we need. Recently we have noticed that there is a much higher need for customized production. In-game comes the high mix and low batch size manufacturing. And of course with this comes also the fact that the workers' qualification is not really sufficient or the human workforce is decreasing. So actually this is shown on the chart on the right side. The chart is split into three areas, light blue, dark blue and red. And it actually shows the population in Europe. So on the vertical axis is the amount of people and on the horizontal is their age. So there are three groups. It's the light blue from 0 to 20. So this means young people not working, the dark blue is the working population and the red is the people in the retirement. And this chart actually shows that in 20 years, everything will shift to the right. And in this big peak, this dark blue peak will move to the red area and the light blue zone will move to the dark blue. This means that we will have total drop down in the available workforce in Europe in some 10 to 20 years. So in this case, we also see that there is coming new markets that are previously not automating only because it was too expensive to automating them or it was not possible due to technology and technical deployment. But manufacturers actually require that also newly developed technologies and new approaches would be also converted and become for industrial manufacturing possible. They would like to have also faster onboarding processes and easy to use and easy to program automation, maybe with large language models and so on. So there we see the shift towards the cognitive and adaptive automation. Yeah. Thank you, Martina. Now I want to talk mainly about our work cell we use for demonstration and testing our ideas. The work cell mimics a hand working place equipped with a collaborative robot. For our tests and our implementation, we use a workstation which is mounted to the cell using an NVIDIA GeForce RTX 4090, where our i6 simulation and the gesture recognition will run as well as we have a ROS2 interface to the robot. The central piece of this work cell is the Motoma Next NHC12 robot. It's a collaborative robot equipped with a Jetson Auron inside. It has a six axis manipulator with four sensors on each axis, a payload of 12 kilograms and it has an integrated body camera. And yeah, it performs the motion operations and part planning inside. Additionally, we use a Siemens PLC and the industrial edge to control the entire work cell. And for vision system, we also use a StereoLabs Z2i RGBD camera, which is mounted on the top right corner of the work cell. So now what's what's behind all of this hardware? The architecture of our system mainly focused on executing production plans and the other hardware is that we have one central controller for the cell, which can be controlled by an orchestration system or an operator. And it calls so-called composite skills using OPC UA. And this station controller controls the PLC and the robot and sends tasks or so-called skills, which we will explain later. To each device, the robot itself synchronizes then with ISACSIM using ROS2. It just published the the joint states of the robot to ISACSIM. And now for a deeper dive into the robot, I will hand over to Martina. Thanks, Benny. Thanks, Benny. So as one of the components, as Benny already said, is the robot from Generation Motomannext. This robot is completely brand new in Yaskawa and it is already in the portfolio. One line is industrial line from 4 kilo to 35 kilo. And the second line is collaborative line from 12 and 38 kilo payload. Actually, these are new manipulators with the new motors. They have high speed. They have also incorporated the high absolute accuracy, which minimizes the deviation between digital twin and the reality. And they also offer additional tools for offline programming, like a robot simulator studio or the development kit for the programmers to test their applications on their actually desks in the office. For the easy to use, it's incorporated also icon based robot programming on the tablet. And the reason why we actually introduced the new robot is that we would like to keep all the facts that the robot has, which is the endurance, strength and accuracy, which robot is very good in and the human is very bad. And on the other hand, human is great in adaptivity, dexterity and reasoning tasks while the robot is failing. So we would like to bring all these three components of the robot to a much higher level. And actually how we do that, this actually explains this slide with the architecture of the robot controller. So inside of the robot controller, it has two parts. One part is the RCU. This is the robot controller unit. And the second one is ACU. This is autonomous control unit. These two components talk together through the RPC client server interface and typical tasks that the robot normally does, which means storing variables, the register data, making motion activities and so on. This is performed by the robot control unit. And on the autonomous control unit, we can deploy users applications that can incorporate any AI tasks. So why? It's because the ACU has a GPU from NVIDIA Chatsun Orion generation. And therefore, we are able to run any parallel tasks, maybe for classification algorithms of the images, image processing and so on. So actually to make the programmer's life easier, Yaskawa have developed services that run on the ACU. There are several of those. There are these orange ones. So I will go just quickly through them. It's a robot control service allowing you to touch the robot. This means send and read variable or maybe start the job, move the robot to a position. Then we have a path planning service. It actually makes the path planning. So it allows you to send from a robot from the point A to point B. And the robot itself will calculate the environment map that's around him and avoid any obstacles that might be in his way. Then we have machine vision service. This is actually a very easy interface for making easy vision applications. It's a GUI where it guides you through the process. So it has calibration in there, hand-eye calibration also. And you can create your own chop flow for classical image processing. Another service is a force control service. This allows any force sensitive applications like maybe polishing and deboring. The AI service offers the classification methods for image processing. And then ROS2 application, the micros agent service allows you to run ROS on the Edge device. So then we have also user applications and services that are actually deployed as a Docker containers on the ACU. So this is how a programmer could access it. So he can use the APIs that are offered by the services, incorporate them in his application, in his program, and then deploy it as a Docker. So this is how a programmer can use the software. Our platform is open and it supports the main programming languages. As an example, Python or C++. And therefore, the customer is able to make applications directly on the Edge device. Switch back to Benny, please. Okay. Thanks, Martina. Now, after looking into the hardware and our demonstrator we used for this implementation, I want to talk now about the use case, what we actually did and implemented. And our process flow starts in the beginning with creating synthetic data for our object detection models. This is crucial because usually in the industry you don't have the time to create manually labeled real-world photos of all of your products. That would not be feasible for the industrial application. So we generated synthetic data based on the CAD models, which are usually very early available to us in iSEC-SIM. For this, we placed our demonstrator in a laser scan of our entire lab and placed the objects on the demonstrator. All done in iSEC-SIM. Then our idea is to hand over the part to the worker. To do so, we implemented a recognition for the gestures of the human. So you can give the robot a start command that it just not starts randomly, handing you over parts, even if the worker might not be ready. To do that, we used Google's MediaPipe API, which has some implementations which helped us getting that running pretty fast. So we have a built-up. And we have a built-up. So we can see how the robot moves, how the robot behaves in iSEC-SIM. And this is done by the ROS synchronization I showed earlier. Now, finally, to hand over the part, we have to recognize the hand position of the worker, which is again done with MediaPipe, where we detect the 3D pose of the hand. And having the position detected, we are finally able to execute our task. How we created the synthetic data. iSEC-SIM has pretty good features creating already labeled images, which I'm showing here. So we randomly just place our objects, which are small 3D printed parts of the truck, on our module and created many images. And the big advantage is that we have ground truth information right away. So we can use that instead of manually labeling the images ourselves. Now, in the beginning, I talked about skills and that our demonstrators orchestrated by calling skills. But what's the idea behind it? We really start at the bottom of the module, where we create so-called motion primitives. Like move to a certain position, follow a path, close the gripper, open the gripper. Those are, in our idea, the most atomic, the most specific skills. And the entire concept of skills builds up on aggregation and encapsulation. So we want to reuse these once implemented functions, like moving to a certain position or controlling the gripper, in the next level of abstraction. So, for example, a skill like pick or place or weld can utilize these already implemented functions, like move to a certain position. And we can go even more abstract, like pick and place, where we just have to extract our poses. For example, we know, okay, our pick position will be in a predefined location. And we just have to give the skill pick one position and the skill place the next position. And we don't have to worry about path planning, controlling the gripper, because we want, while having these two kinds of information, just that the robot does its task without worrying about all of the stuff we already implemented. And finally, the most abstract skill in our implementation is the handover or gesture control, where we send the extracted positions to the skills below and include the external information we receive from the camera, like the hand position and the gesture. And then we can just forward to the skills which are below these, the information, and we don't have to worry about the movement of the robot at all, just giving the positions we got from the camera. Now, we want to discuss more deeply what needs a skill to be executed. For this, me and my colleagues did some research and have already publications. A skill consists of three main building blocks in our vision of skills, which is the feasibility check, the precondition check, and the execution. I want to start explaining what feasibility check means. Before I give a robot a task, I want to make sure, especially in changing environments, that the robot is able to do that task. For example, that the position is reachable, that the robot can move there, and many other tasks. And I want to know, okay, how long, roughly, will that take? Because that's really crucial in industrial applications. The feasibility check will also be called or could be called already in planning phase because it's not directly linked to the execution. The precondition check you will check, is it now possible to execute the skill? For example, is the safety trigger, do we have a knee stop active where we have to make sure that everything's okay before? And if that's also executed successfully, we can start the execution. During the execution, we have four main components. The monitoring information, there could be information like distance left when I want to move the robot. So everything which is relevant during the execution itself. The parameter set where I set all of the information, like the pick position, the placed position, if it's relevant, the maximum movement speed, for example. And final result is everything I get after the skill was executed. For example, the elapsed time, the consumed energy, and all of these informations I gather in final result data. And to know what the current state is, we included a state machine, which consists mainly for optionally a fifth state, ready, running, completed, and halted. Yeah, I think the states are kind of self-explanatory. But the state machine gives us the opportunity to also use the skills in other skills, because I can always wait. Okay, for example, I call the gripper open or gripper close. Is it completed? Or do we have an issue? Maybe the emergency stop is pressed, so I end up in a halted state. The skills have another advantage. I can orchestrate them easily from a production plan, but can also call them by natural language, because LLMs are able to parameterize a skill, because pick and place is also near natural language, but a PLC program, for example, or a robot job in a specific language is much more difficult for the LLM to create. And the other advantages, an operator can also control it much more easily, because if I have a skill in front of me where it's just, okay, move to a certain position, or move to ideally a named target, this can be easily done by an operator. And how we orchestrate the skills more deeply, will now tell you, Martina. Martina. So then jump into the paradise, pick, place, and give skill. So this is just one part of it. We just want to show it on this simple example. So we chose the give skill to give you an explanation how the skills are actually nested. So, as Benny explained, there are also some atomic skills or skill primitive that interact with the hardware itself. So this means trigger the camera, move the robot, open the gripper, close the gripper, and so on. So these are here under the atomic skills in each of the pick boxes on top of that. So, we have parameters. So a parameter in the give skill is a pick pose and object name. So object name, in our case, can be anything that you have pre-teach, pre-taught in your project. So, in our case, we have a red cabin, a chassis, and wheels for demonstration purposes. And the parameter in the pick pose may come from the camera as an example, but it can be also given by some other skills. So it really depends and it's nested together. And then at the end, I have also a stay because I also want to know for error handling if I already picked something that I don't go and pick again. So in that case, I would crash and that's not really wanted. So this is just in short how the deployment looks in our example. And what is the connection between skills and the behavior tree as we had originally in our agenda? This I will explain in this slide. So we have used large language models to have a possibility that operator can talk to a robot and actually give it a command. We are using at the moment speech to text by Google Cloud. So this is not yet deployed on the edge. This is still to be done. But this transcription from or the classification and recognition of the sentence is done by Word Transformer that is fine-tuned with our specific commands and labels for action and for a parameter. So action in our case would be give big place. And the parameter is what? So it's a red cabin or it's a wheel or it's a chassis. So in this example on the right side actually shows the entered command, give the blue semi-trailer to the worker. So with the bird model we would extract action would be give and the blue semi-trailer would be a parameter. So we have actually also possibility to make give what, where. So you can actually enlarge it to many more cases and make it much more interactive. That's the beauty part of the behavior trees. And this is the reason why we chose them. They are very modular and scalable. So this means we can create subtrees and then combine them together as we want, according to a logic, because they are also given in a hierarchical representation. And they are very easy to read. The behavior tree is ticked from the top to the bottom and executed also from the left to the right. So in this case, you can actually create the complete behavior of the whole structure or a whole application. Or you can just make a small behavior tree for some subsections of it. This is why it's very flexible. It's also possible to enhance it with any AI because the behavior tree is usually in a library in the C++ or in Python. And therefore, you can program your own nodes and interact there with any other devices or models or technology that you like to have. They are also creatable based on natural language. We will also show that part. So we actually generate the behavior tree based on the sentence given to the robot. And they are easy to debug and test because it's very easy to look through and make a log where exactly you are in your tree and which part is currently executed. And last but not least, very important part is it's very good for error handling incorporation. So you can put multiple scenarios and then avoid any troubles in your production. Well, and now the connection with skills. So actually, in our case, the skills are the nodes of the behavior tree. So in this particular example, you can see the blue nodes, servo on, pick, place and servo off. So it is quite explanatory. It's actually a give skill, which turns the servos of the robot so that robot can start moving. Then it's the pick command. Then it's the place command. And the last one is turning the servos off when we are ready. So actually, it can be nested as you like. So you can take your skills and put your own behavior together. So if I may, I would go a little bit deeply into the implementations of large language models and the behavior tree. I have this small short pseudo code in there. So besides of importing the libraries, then I am calling a very important part, which are the services APIs for the robot controller and for the interaction between RCU and ACU. Then I define the behavior tree nodes. So basically, these are the pieces of code where I currently call skills that were shown by Benny in the previous slides. And then in the next section, I'm actually loading the model, which was originally pre-trained by BERT transformer. And I load the tokenizer. And then I have the sequence for retrieving the action and parameters from the sentence given to the robot. So I have the code that I have. So I have the idea of the behavior tree. That's actually how I build up the tree based on the action and on the state which I'm currently in. So again, the state is to avoid picking and then picking again, which would come to a disaster at the end. So I'm going to go to the next section. And the last part of the pseudo code shows the main loop where I am activating the communication with the RCU through the gRPC connection. Then I'm getting or obtaining the action and object labels from the verb. And after that, I'm taking the behavior tree itself. And this is running as a Docker container. This is one Docker. It's deployed on the ACU. And in the next slide, I will show you how. This would be a video. Thank you. So in this video, I have used the simulator. So you can see that I gave it a command, pick the wheel. The bird recognized pick wheel. The action and attribute and performed. Then I have another action given to him, lift the chassis. But he cannot do that because pick chassis cannot be executed. He already holds the object. So then I give him another command, put down the wheel, which is recognized as a place wheel. And then robot performs it. So this is the small show how this is running. The picture on the right side actually shows or the camera image is from the robot simulator. This is the offline tool that I talked about before. You can here easily simulate the complete ACU application and have it running on the simulator. It's a digital twin. And this text lock, which I have, this is the lock directly from the ACU. So I can also use it for having information that's currently happening at the robot. Thank you. So now I put it back to Benny. Now I have, after we've seen the simulation of Martina in the Yaskawa simulator, we will see a video of the simulation in iSEC Sim, what we did. You can see there the CAT model, which is exactly the same. We used also for training of the synthetic data. After playing the video, you will see me starting a skill which picks up a part and place it in a storage slot. And you can see on the right bottom corner, the image of our stereo labs RGBD camera. And above that, the same simulated image. So we used similar angles also for the image creation to improve the recognition. And the task is completely executed synchronized. So the same motion in the, in iSEC Sim as it is on the, on the reader robot. Yeah. After the robot executes the skill successfully, it moves in the home position. And we can run this entire, entire application also accessible for the worker on its teach pendant, where I started in the, initially the, the skill. In the last video, I want to show you today, we show the actual handover task. Right in the beginning, you see, I give the thumbs, thumbs up, gesture that the robot knows. Okay. Now I can turn my servers on. I can start. The robot grabs the part and hands it over to me. Okay. On the top left corner of the video, you can see the skeleton detection where my hand is detected. And we use the, the middle position of our hand in here as the target position for the robot. The robot moves, moves to my hand and place it there. Yeah. That's, uh, what I wanted to show you today from the, from the implementation part. Um, now I want to give over to, uh, Martina for the final part. Thank you, Benny. So we are basically through our presentation. I will now a little bit summarize in the conclusion, but we have explained. So we have shown you the Isaac sim that offers the virtual training environment for object detection and the classification. So in this case, we can, um, use it for faster and cheaper deployment in case that we would bring a new products, uh, to our production process. Then also in Isaac sim, you could have seen the human posture and hand position in the depths that can be obtained and recognized. This actually tends towards the easy to use, easy to teach, uh, deployment approach, um, originally, uh, stated as a big problem. And then, uh, in the middle part, we have shown you a possible way how to orchestrate, uh, the complete system and the manufacturing process according to the production plan. So this is, um, I actually idea of the factory of the future. And, uh, uh, in this case, uh, it goes, uh, towards the manufacturing on request when we can have a really big, uh, flexibility and, um, we can utilize the machines, uh, then automatically based on the planned orchestration system. In this, um, um, um, parts, uh, where the atomic skills, skills and composite skills were shown, uh, we targeted, uh, the robust industrial approach and usability according to typical manufacturing standards. So we have shown that the skills are OPC way based protocol, uh, and they are very easy to basically deploy to any device that it's connected to your system in case it fits it. So pick, uh, can be used, uh, either for a Gantry system or for the robot. Uh, and the last part showed the orchestration of some other tasks that might be given by operator using the large language models, the port model transformer, and, uh, behavior treatment. Uh, and the behavior trees as another part of orchestration. Um, and this was, uh, more towards the easy teaching and easy deployment approaching industry. Uh, our work is not over yet. Uh, I have mentioned that we have, uh, maybe some parts that are not quite finished, uh, that would be maybe training, uh, more gestures in Isaac sim to give a little bit more flexibility for the operator for easy teaching and easy teaching. Um, easy operation of the robot. Um, then we also have speech to text recognition done, uh, at the moment on the workstation. That would be also another task to deploy it as a Docker container on the ACU directly. Um, so this is currently in process. And, uh, for sure, what is the big request from manufacturing and from industry is to have a really good error handling. Because in case that something goes wrong, we don't want to bring the robot from very complicated, uh, situations. And we want to be, uh, system as robust as, as possible. So thank you very much for your attention. And, uh, if you want, you can see the robot life, uh, at the Hannover Messe at the end of the March, uh, and April. And, uh, secondly, you can also come to automatic affair, um, in Munich. And we are also open for any collaboration, uh, or comments from your side, uh, maybe by email or this contact us by LinkedIn and, uh, so on. Thank you again for your attention. It was a pleasure to present for you. Thank you. Thank you. Thank you.