 decorumpak Kai chevette cut wear b Thank you. Hello everyone, thank you for coming to my presentation. I am Elior Serif, a data scientist at TotalEnergies, and today I will present how we integrate artificial intelligence to accelerate the modeling, the optimization and management of reservoirs in the subsurface, a key challenge in the energy transition. The energy transition requires optimized management of underground systems, whether for storing CO2 or exploiting reservoirs. However, traditional methods such as reservoir simulators based on partial differential equations require massive calculations. So these costs limits our ability to explore various scenarios or respond quickly to changes. So our goal here is to make these processes faster through AI. The flow in poorest media are non-linear and chaotic dynamic systems. For example, a small change in permeability can radically alter the spread of CO2. Traditional simulators, while accurate, are resource intensive. One meter resolution grid for a 100 km2 reservoir generates millions of calculations. So machine learning and data science offers an alternative by directly learning the relationship between inputs and outputs without explicitly solving all the equations. And this opens the door to tailored models for each geological context. Three factors are converging today. Climate urgency, carbon neutrality commitments by 2050 require rapid solutions for CCS and CO2 emission reduction. High availability, sensors and historical simulation provide terabytes of data to train models. And advances in AI allow us to have access to real-time prediction with comparable accuracy to traditional simulators. And what we want to do is to enable rapid high-resolution flow prediction with precision matching traditional simulation methods. So in the first part, we will focus on sequential Fourier neural operators to model CO2 storage. And in the second part, we will address sequential physics-informed neural operators and variational convolutional autoencoder for history matching. In this chapter, we focus on CO2 storage. To prevent the injected CO2 from resurfacing, we need to predict its migration over decades. For this, we will use a sequential Fourier neural operator to predict gas saturation and pressure over a 30-year period. Let's start with a quick recap of how a traditional simulator works. So a traditional simulator works in three steps. First we have the input with the reservoir conditions, geological model, rock properties and injection plan. Then we have the numerical simulation. We solve the Darcy's equation and Mars conservation on a fine grid, often using finite difference methods. And the output, we have the pressure distribution, fluid saturation and production rate. So this process is accurate but slow if we want to reproduce this simulation thousands of times. Our approach replaces the simulator with an AI model. So geologists and reservoir engineers generate hundreds of or even thousands of scenarios with varying parameters. And then a neural networks learns to directly map input to output. And then the model is tested on scenarios not seen during training to ensure its robustness. So our inference time will be much lower than the simulation time of a traditional simulator. To train our model, we use the following dataset. So first we have field variables like permeability, porosity, reservoir thickness and perforation location. And we have also scalar variables like injection rates, initial pressure and temperature. And this radial model represents a typical central injection well for offshore CCS project. You can find this dataset via this link. And I would like to thank Gege Wen from Imperial College London for making this dataset open source. As outputs, we have 24 3D snapshots for spatial distribution of CO2 saturation and pressure over 30 years. This slide presents the structure of a Fourier neural operator FNO, a neural network architecture designed to efficiently solve parametric partial differential equations. So the goal is to convert the input function F in physical space into its representation in frequency space. The Fourier transform decomposes F into its constituent frequency scale. This allows for simpler global operations such as convolutions in the spectral domain. So we first convert our inputs, for example, a permeability field into frequency. Then we apply filter R, layered by the neural network equivalent to a green function in spectral space. And then we have the inverse transform. We reconstruct the physical solution for our case, the pressure and saturation. W here acts as a non-linear projection layer in physical space. And the Fourier layer is summed with W before passing through the activation function. And one of the key advantages of the FNO is to capture multi-scale interaction without explicitly solving the equation. We will train two different models, one for pressure and one for saturation. But we will use two different losses. So the pressure in the reservoir follows an elliptic equation dependent on global conditions such as domain boundaries. For this reason, the loss function is designed to focus on point-wise squared errors which provides a direct comparison between predicted and true values. Saturation equations are of the hyperbolic type. They describe dynamic phenomena similar to waves with information propagating along specific passes. Saturation is influenced by transport and advection processes. To take into account these differences, we add this term here to limit errors on our wave front where our saturation front and gradient will be much higher. The model was developed using the NVIDIA Modulus framework and the training was conducted on 2000 examples over 100 epochs using GPU H100. The inference time is approximately 1000 times faster than the traditional simulator and we achieve very high accuracy on the test set for pressure on the left and saturation on the right. In these two graphs, we can see the R2 score over time and we are able to maintain a very high accuracy for the 20 form time steps. Let's compare the pressure predicted by the sequential FNO is modulus with the reference simulation in the middle and the errors are shown at the bottom. Here we have our CO2 injection well and the propagation occurs radially. What we can see that our model reproduced the result obtained with the traditional simulator quite faithfully. Here we have the same thing for the saturation. Our errors mainly come from the saturation front and we have tried to minimize them as much as possible with the loss function I mentioned earlier. Looking at the scale here, you can see that the errors remain very low. Let's move on to the part about history matching now. So I will use a sequential physics informed neural operator, which I will refer to as PINOT from now on, and a variational convolutional autoencoder, which I will refer to as VCAE from now on, to solve an inverse problem. What we aim to do with our PINOT is to predict the pressure and oil saturation of our reservoir based on reservoir parameters such as permeability and porosity. The obtained pressure and saturation allow us to determine the oil and water production at our wells. This constitutes our forward problem. What we want now to address is the inverse problem, meaning finding the geological parameters of a reservoir based on historical production data. In this part, we aim to quantify uncertainties. And to do so, we will use a PINOT along with an ensemble method. For our PINOT, I will start by briefly introducing the equation governing our problem. We use the black oil model, which is a simplified reservoir simulation model. And to model this flow, the principles of mass conservation are applied to each fluid. For our case, it's a two-phase flow, so it's for water and oil. And the flow of each fluid in the poorest medium is described by a modified version, a simplified version of the Tarsi law. So, in this study, an aerobic reservoir is used. This reservoir is represented as a three-dimensional grid consisting of 32,000 cells. The reservoir is composed of two types of rocks, a metphase with a lower permeability 20 mD and a much more permeable sand phase . The simulation includes six wells. Four are oil production wells and two water injection wells. Each well operates at a constant bottom oil pressure. 330 bars for the injection wells and 310 bars for the production wells. The reservoir is modeled with no flow boundary conditions. The main objective of this study is to predict the evolution of pressure and water saturation in the reservoir over a given time period. The simulation covers a total of 51 time steps, each representing a 30-day interval. The model uses a relatively small dataset of 600 cases for training, which poses a challenge for modeling complex reservoir. However, this study also aims to test the capabilities of the physics-informed neural operator model in capturing the complex interaction between these parameters with a limited amount of data. The sequential nature of the Pinot model is particularly advantageous for large-scale reservoir simulation, as it significantly reduces memory requirements by breaking down computation into manageable steps. These make it feasible to handle complex reservoir problems, even with limited computational resources. The primary goal of this model is to predict the condition at time t, based on the available data at time t-1. Specifically, our approach aims to predict the pressure and saturation at time t using pressure and saturation at time t-1. So we compute the oil and water flux and then prepare our data in a time-layered format. And the input variables for our Pinot model are the permeability, porosity, the total flow rates, the water flow rates, pressure, saturation, the time step duration, and the time step t. So to predict pressure and saturation at time t, we use this input variable from time t-1. And this is the foundation of our sequential approach. For the training, the process operates in five main steps. So first, the model estimates the future pressure and saturation values based on the inputs. Then, we compute the physical loss. So physical constraints are imposed using the Black Oil model equations and Darcy law. So here we have the equation coming from the Darcy law and here for the pressure and here from the Black Oil model for the saturation. Then, we calculate the supervised loss. These loss compare the model's prediction to real data to ensure consistency with observation. The total loss function is constructed, integrating weights to balance physical and supervised losses. More weight is assigned to the data as learning from the data provides the most valuable information. And at the end, we have the model parameter update. The parameters are adjusted using gradient descent to minimize the total loss. The parameters are adjusted for the data as we use. The parameters are adjusted for the data. The parameters are adjusted for the data. The parameters are adjusted for the data. During the testing phase, only the initial conditions of pressure at time t equals 0 and water saturation at time t equals 0 are needed to start the prediction. As they will then be performed sequentially with each prediction at time t using the predicted values at time t minus 1 as the new input. We also have a flow rate update. The new flow rate q at time t are calculated using the Pisman equation incorporating the predicted pressure and saturation values. This ensures that the flow rates remain consistent with the physics of fluid flow in Poulus media. And all this process is applied over our 51 time steps. We developed the physics inform neural operator using the NVIDIA Modulus framework. And for the training, we use an H100 GPU. And for the dataset, we used 600 examples for the training. And the comparative R2 score show that the physics inform neural operator model outperforms the Fourier neural operator FNO model in predicting pressure and water saturation. Here in red, we have the results for the Pino model and in green for the FNO model. And we have the same for the water saturation here. In both cases, Pino provides superior prediction accuracy, demonstrating the advantage of integrating physical knowledge into the neural operators. And these results confirm that Pino is more effective than FNO in capturing the complex dynamics of the simulated system, offering a more reliable prediction for both variables. However, we observe that the training time for the Pino is four times longer than for the FNO. Therefore, it is essential to find a balance between improving accuracy and optimizing training time. And we also see that the FNO model is more effective than FNO. Here we have an animated plot comparing our developed model using Modulus on the left, the true saturation in the middle, and the error between the two on the right. And what we can see is that these results for the pressure are quite good. Here we have the same thing for the saturation. And the biggest errors are located on the flow front. So to minimize these errors, we use the same loss as I described earlier. And maybe I will leave the animation on for a few more seconds. For our problem, we aim to reconstruct a permeability map from oil and water production data. But this problem is ill-posed. What I mean by ill-posed is that the system is under-determined. There are more unknown parameters in our reservoir, 32,000 cells, than available data, which consists of our historical production data. Then the number of measurements is insufficient. And the constraints on the permeability map are too weak to eliminate all possible solutions. And we can have also instability. Numerical errors in the forward model can be amplified, especially if our Pinot model is approximated. The Variational Convolutional Autoencoder, VCAE, is a powerful tool for dimensionality reduction and probabilistic modeling of complex data. In the context of inverse problems, the VCAE enables compact representation of high dimensionality, for example, permeability map, in a reduced latent space. And this facilitates efficient sampling and reconstruction while incorporating prior knowledge through its learned latent distribution. So, with this VCAE, we aim to generate new permeability maps that are not present in our dataset, by decoding a random vector from our latent space. Our latent space has significantly fewer parameters than the number of cells in our reservoir, allowing us to better manage uncertainties and move toward a well-posed problem, while the number of unknown parameters needed to describe the reservoir is significantly reduced. The VCAE consists of three fundamental steps. We have first an encoder. It extracts a latent representation from the input data. And then we have a re-parameterization. It generates a latent variable Z, using the mean and variance produced by the encoder. And at the end, we have a decoder. It reconstructs the original data from the latent space Z. So, this model is particularly well suited for data generation tasks and dimensionality reduction, while incorporating probabilistic constraints. So, with the VCAE, we can generate new reservoirs that are not present in our current dataset, by taking a random vector in the latent space and then just decoding it. So, for solving this inverse problem, I use the adaptive regularized ensemble Kalman inversion, a recall algorithm developed by Clement Etienneam from NVIDIA. If you need more detail on this algorithm, I recommend reading the reference paper provided here. But to keep it simple, we have our sequential PINOT model and our real production data, which serve as our target. So, for the initialization, we generate an ensemble of 100 permeability maps with the VCAE that we have previously seen. Then, we use these reservoirs as input for our sequential PINOT model to predict pressure and saturation fields for this reservoir. And finally, we use the PINOT model to obtain the oil and water production rates over time for each wells. Then, the residual between the measured and simulated data are computed for each member of the initial ensemble, along with their mean and variance. So, the residual is also our cost function. Here, gamma represents the covariance matrix of errors associated with the target Y. And during residual calculation, gamma is used to just normalize the difference between the measured and simulated data. And this ensures that the errors are compared on a consistent scale. Then, regularization is applied to adjust the residual based on the error and variance of the prediction. And then, we have covariance matrix calculation. So, the first one is to measure the variability of prediction among the ensemble members. And the second one is to establish a link between the latent space where our parameter map is represented and the physical prediction space. And it is used to better align latent variables with observed data. Then, the permeability field is encoded into latent space ZN using the convolutional autoencoder VCAE that we have previously seen. And then, the latent variables are dated using residuals, covariance and Gaussian noise. And the update variables are decoded to generate refined permeability fields. And by doing this, we update our initial ensemble. After repeating this process 30 times, so for 30 iterations, we can see that our cost function here has converged. And our final ensemble has successfully converged towards the optimal permeability map. And also, we have in the middle our initial ensemble. The goal of our ensemble method is to make the initial ensemble, shown here in grey, converge towards our target, here in black, which represents the oil production at our four production wells. And in green, we have our final ensemble, meaning our initial ensemble after 30 iterations. And we run the history matching algorithm only on the 25 times step of our simulation. The rest was used for forecasting to see if we still converge towards our target. And we can see here the separation for the four wells. Here we have the same thing, but this time for water production at our four production wells. On this slide, I present a possible permeability map reconstruction from our final ensemble and compare it with the true permeability map used to generate the real production data. We can see that we have managed to reconstruct the geology of our reservoir fairly accurately. But we can see that some parts are sometimes missing, like this one, and this one, and this one. This is due to the fact that we did not have wells in those areas, making it difficult to reconstruct them without information. And our four wells are located on these channels, and we are able to reconstruct them. With this workflow, so Pinot plus Arekai algorithm is an initial ensemble generated using a VCAE, we can significantly accelerate the execution speed of the process. With an initial ensemble of 100, it takes 40 minutes for 30 iterations on H100 GPU. And with an initial ensemble of 300, it takes 134 minutes for comparison using a traditional workflow on a conventional reservoir simulator. A reservoir engineer would take approximately 8 hours with an initial ensemble of 100 for this specific reservoir and size. So, to conclude, this workflow is designed for scenarios where it is necessary to sample the entire posterior density to quantify inverse uncertainty. It is important to keep in mind that AI surrogate models are meant to accelerate some tasks, such as history matching, as we saw today, but also others like well placement optimization. We achieve faster inference times and reduce computational cost during inference. However, training the forward model still requires significant computational resources, especially if we want to scale up to real fields with millions of cells. As a result, parallelizing training across multiple GPUs and using computing clusters is essential. And traditional simulators will remain the reference for high fidelity modeling. The goal is not to replace reservoir engineers, but rather to provide complementary solutions with alternative approaches. So, thank you so much for attending my presentation. I would like to express my gratitude to Total Energies and especially my manager Daniel Busby, for his invaluable guidance and support throughout this project. I also want to thank the NVIDIA team for their help in preparing this presentation, and in particular, Clement H&M, who introduced me to Modulus and provided technical guidance and support in building the entire workflow, from the PNO to solving the inverse problem. So I really want to thank him. And if you have any questions, feel free to ask in the chat. I will be happy to answer. And you can also reach out to me by mail or connect with me on LinkedIn. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.