 Hello everyone and welcome to AI meets Robotics, European Startup Showcase. I'm Naomi Betts, Senior Account Manager, Premutecturing and Industry at NVIDIA. I'm thrilled to be your moderator for this exciting session, where we'll explore how five innovative European startups are transforming industries by merging robotics, simulation and AI. Today, we'll dive into the real-world applications of NVIDIA platforms in robotics, including iSACSIM, Omniverse and Jetson, which are empowering innovators to accelerate development and deliver impactful solutions. Let's meet our incredible lineup of speakers who will take us on this journey. First up, we have David Niedermeyer, CTO and co-founder, and Wolfgang Pointer, Team Lead Future Technologies at Agilux. They will showcase how their autonomous mobile robots are redefining logistics and manufacturing with AI-driven precision and adaptability. Next, we'll hear from David Rieger, CEO and founder of NeuroRobotics, who will share how their intelligent robotic solutions are enhancing human-robot collaboration in industrial environments. Following that, Avilash Kumar, Team Lead of the Perception and Slam Team at Node Robotics, will introduce us to their advancements in perception-based AI, and discuss how they're leveraging NVIDIA iSAC preceptor to improve navigation and decision-making in autonomous systems. Then, we'll welcome Laurence Wellhausen, co-founder and CTO Software at River, who will reveal how their intelligent four-legged robots are revolutionizing last-mile delivery. He'll also discuss how simulation has accelerated their deployment process. Finally, Stefan Hotz, Chief Product Officer, and Lars Beyer, Staff Software Engineer from Vandlebots, will demonstrate how their new product, Nova, seamlessly integrates with NVIDIA Omniverse and iSAC Sim, transforming the programming and operation of industrial robots in virtual simulations and real-world environments. You're in for an inspiring hour filled with innovation, insights and actionable ideas. Let's get started with David and Wolfgang from Agilogs. Thank you, Naomi. It's a pleasure for me to introduce Agilogs as a pioneer of artificial intelligence-driven logistics automation. I will give an overview of what we have achieved so far by utilizing AI, whereas Wolfgang will give an outlook about what's coming next. So what is our vision? Our vision is actually very simple and we're still working on it since day one. So the idea behind Agilogs is to make it as easy as possible for the user to automate the logistics process. Basically to lower the entry barrier for starting with process automation and making process adaption at runtime as easy as changing, for example, the settings on your phone. The demand for this solution comes from the fact that production environment nowadays have a tremendous dynamic in their operations. The times when such environments were operated without adaptions for years or even decades are basically over. And in order to meet these new circumstances, we invented our X-WROM technology. That is our AI-driven operating system for autonomous mobile robots, which enables infrastructure-less robotics. And this is basically the core of our technology stack. So in 2017, we launched the Agilogs One, an autonomous mobile robot designed to move pallet-like load carriers from A to B. A compelling example of our AI-driven robots can be observed at this BMW production facility you can see here in this video. Where nearly 30 Agilogs One vehicles operate in a coordinated swarm. Each unit processes environmental data in real time, optimizing transport routes, avoiding traffic jams, and ensures uninterrupted materials supply. So the system continuously learns and refines its decision-making strategies, leading to enhanced efficiency and scalability. So our products can be operated in almost any environment where a forklift truck or manual pallet check is able to operate. With over 1,800 Agilogs robots deployed worldwide, we are trusted by industry leaders such as BMW, Daimler, or Continental within the automotive industry, or Miele, PepsiCo, and Siemens to name just a few examples in all kinds of industries of our more than 300 customers who trust in our technology and made us to a profitable company since day one. But what is the X-WROM technology about and how we utilize AI to create this capability? So the X-WROM technology is a decentralized infrastructure-free approach that enables fleets of Agilogs vehicles to operate without any adaption to your existing infrastructure. It's basically plug and play robotics. It's a self-service, easy to use product with open interfaces. So all you need is Wi-Fi in the proper floor or proper floor conditions. So it's important to mention that unlike traditional automated guided vehicles, the legacy technology, an Agilogs swarm operates without predefined routes or even a centralized control system. So everything you need is on the vehicle like your vacuum cleaning robot in your living room. This gives you on the one hand the ability to start process automation in less than an hour and allows you on the other hand to respond immediately to the needs of your process, such as increasing or reducing fleet size depending on your capacity utilization of your production plant, for example. But in the difference in the difference between the two of your vacuum cleaner, our robots are collaborative. With their AI-powered capabilities, they're capable to solve missions together in the most efficient way. They collaborate on perception to identify the environment where they operate. They collaborate on dynamic path adaption and path planning to ensure the best route from source to target. They collaborate on executing their missions to ensure to use the best performance according to the current fleet capabilities. They can solve even challenging situations like deadlocks that can be triggered through peripheral systems like a conveyor belt. All these features are based on real-time self-learning and decision-making and are already shipped with our products since 2017. So we continuously work to improve the features and capabilities of our products and Wolfgang will give you now an insight into the future capabilities of our technology stack and product portfolio. Thank you, David. The diversity of application narratives and the high demand for customizable solutions represent the growing challenge in our field. Nevertheless, we are confident that recent technological advances will enable us to deploy new generations of AMRs that have even better performance and are easier to use. AI, particularly deep learning, will expand our robots on-board perception capabilities, allowing them to get a better understanding of their environment. Not only will this improve their performance, it will also be a key component when it comes to enhanced human-robot interaction. Advanced reasoning will also allow us to automatically identify optimization potential for fleets, layouts, and even all logistics applications based on the data collected by our robot fleets. AI can also provide functions beyond material handling, allowing the AMR to cover more tasks that people will not be able or willing to do in the future. Now I would like to show you some of our latest concepts for deep learning based on-board perception. Here you can see how real-time image segmentation can be used to distinguish between drivable areas and obstacles. This is crucial for AMR operation in industrial environments since small objects and uneven terrain still pose a challenge for conventional approaches. The same foundation model can be used to segment and track relevant objects such as load carriers in real time regardless of their shape, size, or appearance. Considering the vast number of different load carriers that can be found in intro logistics, it becomes clear that such a method can significantly increase the potential for customized object recognition. Once such a detector is deployed onto a robot, it enables new functions like the one shown in this video. The so-called dynamic station handling utilizes a six degrees of freedom post estimation model developed by AIT, one of our research partners. This allows the ad hoc handling of goods without the need for manual station definition, making it more flexible than any other solution currently on the market. The second big driver for efficient and scalable AMR development is simulation. Highly detailed models of robots, assets, and environments in combination with various interfaces allow us to transfer many resource intensive tasks from the real to the virtual world. Ultimately saving costs, improving quality, and reducing cycle times. Within our development process, we've already utilized advanced robotics simulation for rapid prototyping, automated testing, and synthetic data generation. Now we are aiming for a seamless pipeline that leads to real world deployment. Robotic simulation will also play a very important role when it comes to digital twins of customer applications. I would like to show you now what is already possible when a powerful simulation platform is combined with a sophisticated AMR software stack. By using NVIDIA Omniverse and iSEXIM, we have been able to model all our robots in openUSD format, incorporating all onboard sensors like LiDAR and cameras. Not only do they look indistinguishable from their real world counterparts, the NVIDIA RTX technology also generates realistic sensor data needed for onboard perception in real time. Here you can see our Agilox 1 operating in a simulated vector environment. The sensor data generated in iSEXIM is processed directly by the Agilox core software. This allows the vehicle to localize, maneuver, and handle goods just like in reality. In the bottom right corner, you can see the Agilox user interface showing the static environment, obstacles detected via LiDAR sensors, and the planned route of the vehicle. This integration of NVIDIA iSEXIM and the Agilox AMR solution represents a groundbreaking innovation for us. It enables us to provide our robotic software, which normally runs entirely on the robot, with sensor data from any imaginable environment. Since the software is agnostic to the input, all the features and USBs of the systems are preserved during simulation. The concept is also scalable, allowing us to run multiple fully simulated robots in a shared environment. Both robotic simulation and AI are key technologies in the deployment of our next generation AMRs. We want to use them efficiently to ensure that we can continue to offer the easiest AMR solution in the market. So now I would like to hand over to David from Norea Robotics. Hello everyone and thank you also for letting me speak here on this event and thanks to NVIDIA. I think I would love to talk a little bit about our collaboration and also what Nura actually does today. So at Nura, there's everything about cognitive robots, robots which can hear, see, feel and also think. And basically, a little bit more information about Nura. We are today also, let's say, a size of 300 people, 350 plus people by end of this year, more than 500, growing rapidly, having already revenues. We just did last year about 10x in revenue by simply scaling and bringing a lot of robots, more than thousands of robots into the field. I think special about Nura also, it's that we enable companies. So we see ourselves more as an enabler. It means we are working also today out of the top 10 robot companies with four of them. And this also leaded to, I think, one of the amazing order books we have, more than 1 billion other books. So what we do, we do full stack robotics. So it means like from sensor, hardware, software and the platform, which is combining all of them. And in almost every of these parts, we're using actually new NVIDIA technologies. So if you're looking at artificial intelligence today, you will see AI gets more and more capabilities. AI is enabled to draw the best pictures. If you want to let it look like in Van Gogh, it will look like in Van Gogh, you will actually see AI used, I think we all know, like ChatGPT, like in all kinds of writings, all kinds of languages, you will see AI in music, creating music, and also more and more also creating new things and doing a lot of engineering works. But this is actually something where I always question also myself, is that what actually not that something what we humans would like to do? I mean, we love to do exactly this, or if we would have enough money on our bank account, having enough time, we'll probably do one of these things. And this is also why I think we have to think about how can we enable AI and bring it to the next level to do things we don't like to do, like taking the garbage out, filling our dishwashers, doing chores, which I think most of us hate, and at least having the choice to do them or not. And this is pretty much leading us to the point where we say, okay, AI needs a body, we need to embody AI to actually let AI do things we don't like to do, to give us more time to do things we like to do. And if you're looking at the robotic space today, you will actually see how is robots, how are robots used today? So basically, if you're looking at the major, like the majority of the robots are used like as a simple device for really, like just repeatable tasks with very high quantities. Why? Because simply, it's still very difficult to enable them. So it means today, we see a robot, just a piece of metal with some motors, let's say, and that's it. And to enable them, we actually put a lot of periphery around the robots. And then we need a very smart humans to somehow combine all of them and program them. And this is what we don't have on this planet a lot. So this is also why we did not see robotics really expanding a lot. And we did not see a robotics market like really in the boom, simply because it was limited by us humans, because we simply like have never had like a fully autonomous robot. So our basic idea at Neura was doing exactly this, what like something like a human means, bringing this cognitive abilities like hearing, seeing and feeling into robot arms, or devices like robots, and then combining them with the brain like neural networks on different levels. And this is how to enable them. And the first device we ever did was Myra was one of our products, which is the first cognitive robot on planets. It means a robot arm which having all the sensors inside and very strong CPU and GPU to actually enable a platform. What I mean by that is very easy to compare with something which already existing, everyone has in the pocket, it's like a smartphone arm. Like the big change in the smartphone industries and what Steve Jobs did, basically, like enable the smartphone by first, like creating a hardware which is actually capable to have an operating system, which is speaking to all kinds, let's say all kinds of sensors, and then enable the world by building a platform like a developer environment, and also an app store to make a multi-purpose platform out of it. Basically, he did not limit the platform by himself, but actually enabled the world to do so. And that's basically what we did in the robotic space. We created a hardware with an architecture to distribute the compute on the right side to enable actually an operating system, and let's say an architecture for artificial intelligence to enable also a platform, so where we have also the same like a developer environment and also an app store and a simulation world to enable the world. Product wise, I can we started like with simple cobwebs, made them cognitive, it means bringing like all the sensors like hearing, seeing and feeling inside and feel of touch inside, and then bring them on the mobile platforms to simply give them a little bit more flexibility, made some household devices, and you see like there's a lot of different kind of shapes up to a humanoid robot. And this is basically our great idea. We do see that the biggest advantage like our from bringing AI, let's say together with the hardware, is also today not just everywhere, just humanoid, but actually it could have a lot of different shapes because of the cost, because of different kind of reasons and different kind of industries. So basically to make it here also, let's say, to give here a little bit clearer idea, like we what we do is actually we enable the device itself, and what we also work on is not just the device, but actually also the environment. What I mean by that is like, first of all, we have a very smart robot, but also I think what is even more needed is actually that we even have a smart environment. And this is also what we enable through our platform, your reverse, and also combining that with a sensor and also an architecture of sense of the compute to enable it to have robots, let's say, also enabling our, let's say environments like hospitals or airports or all kind of different kind of industries. And what I mean by that is like simulating, this is what we are right now working on, like for example, a hospital, bringing sensors inside and simulating fully and then closing the loop by actually a human by giving them information or a robot, which is actually fully autonomous working this environment. So this is pretty much what we do within NVIDIA. We work on different kind of levels, like first on the hardware level. Yes, we do have a strong GPUs also of NVIDIA and every of our robots to actually do one of the biggest processing parts before actually sending data into our clouds or making bigger, let's say, machine learning tasks. And this is actually where we use AGX and RTX for it. We do also use the ISACSIM and ISACLab or the ISACSIM Studio to simply train our platforms there. And then also like having different kind of level up to bridging the new reverse and omniverse by simply new reverse is basically focusing on, let's say, usability of the robots interacting with the humans or the physical world, but at the same time also closing the gap with omniverse, with the simulation and also, let's say, the real world. So we are very soon releasing also one of our new products we call Aura AI, which is basically combining all of them. And I think this is also one of the key technologies Neura is basically also working on. And this is also why we have this many different shapes. So to give you a real example, what we do with that is already an older video, but I still love it because simply you see here a robot arm, which has all the sensors integrated like hearing, seeing and feeling. She will never actually touch the teach pendant to try to program something, but she simply talks to the robot in a natural way. It tells the robot, hey, look at it and just repeat what I did or build the same one. And you see also this robot is a robot, which is understanding the task and simply operating exactly on that task fully autonomously without actually having anyone else involved. But simply closing the loop between the simulation world and also the real world. I have also different other examples like doing the same thing just on a level where we have a humanoid, where we simply always say there's also just two arms on, let's say, a more unstable platform. And what you see here is basically also the same task again, not using any, let's say, programming or any other skills, but actually just the skills every human on this planet has like interacting with a robot in a very natural way by telling the robot what to do. And then simply, let's say, let the robot fully autonomously let do like whatever it is. So you still see like basically the robot does it autonomously, but in the same time, it's pretty much also more, let's say, still fighting a little bit more with itself, like with the stability, let's say like that on itself. But with the newest generation, which we will bring out in June this year, we will actually bring a new platform, which will exactly solve all this. It's a sensor lags we had to basically close the loop even better from a simulation world to the real world. There's also a video we launched also together with NVIDIA, like where we simply show also exactly like using the different levels of hardware, software and simulation platforms with NVIDIA and combining the new universe platform. Also, let's say with the Omniverse or connecting, sorry, with the NVIDIA platform. And this is basically what we do. Thank you for your precious time and thank you for letting me speak here. And now I will give over to Abhilash and thank you for your precious time. Thank you, David, for the interesting presentation. Hi, my name is Abhilash. And today I'll be presenting how we tackle the problem of detecting novel objects in industrial environments for mobile robots. Quickly introducing who we are and what we do as a company. We are a software startup that was founded in 2020 and are based in Stuttgart, Germany. We offer software modules for mobile robot manufacturers that reduce the friction of bringing their robots to market. Our offerings include components for various tasks such as reliable localization and mapping, advanced navigation and path planning, and flexible feed management and routing. Our NodeOS platform allows us to provide flexible software solutions which are agnostic to hardware and sensor configuration. This has enabled us to support 30 plus mobile robot types with various sensors and driving configurations. We are currently running in 1000 plus robots in production and have driven more than 1 million kilometers so far. If we have a look at what a day in the life of our deployment looks like, the majority of our customers saw the subset of interlogistics use cases, which means they transport goods from point A to point B in factories, warehouses, hospitals, etc. Given the variety of use cases and robot configurations, we come across a lot of custom objects and scenarios that need special handling. This typically means tuning a bunch of parameters to get the desired behavior for each robot and customer. A common theme here we see is that a lot of problems arise from the lack of accurate understanding of the environment the robot is in. This is especially the case given the fact that 2D laser scanners are still the primary source of sensors used to perceive the environment. This is mainly due to their ease of use, measurement accuracy, and safety certification. Processing data from cameras still needs more specialized knowledge and requires higher compute, which these systems traditionally do not have. This means that majority of the current deployments out there are missing out on rich semantic information that cameras can potentially provide. Over the past years, we have seen deep learning become the go-to technique for computer vision tasks. Deep learning architectures are becoming more efficient and cost of computers going down thanks to platforms like Nvidia Stetson. Common approaches used at the moment still require a lot of label data, which is harder to come across in our case due to the lack of standardization of objects in industry. On top of that, customers typically also do not prefer making their data from the environment public due to security reasons. All these factors lead to the need of having training and deployment pipeline, which scales with a variety of customers and use cases. Our approach has three key components. The first step is an easy data collection step, which enables our customers to simply follow through. We train our models based on this data to provide them with their own custom model. Step two is automated labeling and training so that we are able to scale easily with the growing number of use cases. Step two is to deploy the optimized model, which runs efficiently on limited resource we have on the edge devices. The first step is to generate a 3D mesh object of the interested obstacle or object of interest. This can be done easily using mobile apps such as AR code object capture on iPhone or Polycam on Android. We additionally capture ROSPAC data of the object in different backgrounds and load configurations. This sets up the inputs for our training process, which are RDPD images from the robot's camera and a 3D mesh object. We then use the captured data and 3D model to generate labeled images using NVIDIA's Foundation Pose. Foundation Pose allows us to track objects across image sequences. Performing this across multiple image sequences provides us hundreds of well labeled images without needing the additional effort of manual data annotation. During training, we use image augmentation techniques to further generate more data. We use classical techniques of precise dropping, changing lighting and noise conditions. Additionally, for making the pipeline more data efficient, we use methods to mix multiple images such as mosaic, cut mix, copy paste, etc. With more customers integrating on-board GPUs and cameras into their robots, we can now deploy our trained models directly on the edge, leveraging NVIDIA Jetson for efficient inference. This advancement allows us to incorporate additional sensor modalities into our Node-OS stack, enabling our deep learning models to enhance autonomous navigation and operate more intelligently within our customer environments. Here is an example of one of our use cases based on a real customer issue we had. Since robots are introduced to already existing workflows in factories and warehouses, the operating environment is not designed keeping the specific limitations of the robot hardware in mind. This leads to edge cases sometimes where we come across novel obstacles which are unaccounted for when designing the robots. As you can see here, the robot's 2D safety scanner only sees the forks of the manual forklift if it is raised at a particular height. Since the robot relies on scans primarily for collision avoidance, this leads to collisions. Using the previously described pipeline, we were able to detect the forks using cameras and add the filtered point cloud as an additional information source for collision avoidance. Here's a quick review of our pipeline again. In phase 1, we collect minimal data for training. In phase 2, we auto label the data using NVIDIA's foundation pose. In step 3, we train and tune the model as per customer's requirements. And in step 4, we finally deploy the model on edge and use the detection results in our nodeOS stack using software adapters. We see further potential in using this technology downstream for tasks such as semantic SLAM, where we can continuously update the detected objects positions in our live mapping module to help users visualize where specific objects of interest are in the environment. Another use case we see is advanced path planning instead of treating objects as static obstacles for a given timestamp. Correlating detections across frames can help predict objects movements to make local path planning smarter. These objects can be other robots, manually operated vehicles like tugger trains, forklifts, or even human beings. If you would like to know more about our work at Node, I would encourage you to reach out to us directly or follow us on LinkedIn. Now I would like to hand over to Lorenz so that he can talk more about their interesting work at RIVER. Thank you, Abilash. Yeah, hi everyone. I'm Lorenz. I'm a co-founder and CTOF software at RIVER. And we build robots with generalized urban autonomy to make robots work in the real world. That means outside, outdoors, where it gets really tricky. And our first use case that we tackle is last mile delivery. Our wheel-legged mobile manipulator is able to overcome challenges that were previously not solvable by other types of robots and is really ready to revolutionize the last mile delivery space. Why is this a big deal? There's generally a global workforce crisis, especially in physical labor. It's really hard for industry leaders to scale their operations sufficiently because these are really strenuous, mundane, repetitive jobs that people don't really want to do if they have the choice not to. So it is really difficult to scale operations, especially in the logistics space. And industry leaders are really looking for a way to scale these operations with robots because they are able to work any day in any environmental conditions. And these use cases have gigantic demand for mobile robots to operate outside in the real world. As I mentioned, last mile logistics is a very important use case here because it really is. It takes a person to go all the way to the door and deliver a package. And this is very strenuous. These people work very long hours. They walk tens of kilometers every day, which might be nice in summer when it's nice out, but mostly it's not. And it can be rainy. It can be dark. It can be cold. So really robots could alleviate a lot of strenuous work here. And this has previously not been possible because it requires bringing packages and parcels directly to customers' doorsteps. And the previous generation of delivery robots were typically just wheeled platforms that actually couldn't go the last 10 meters to the actual doorstep because typically you encounter steps and stairs that you cannot overcome. So usually you only saw these robots applied in applications for food delivery because people are home to grab their food because they wanted to have it hot. But for package delivery and grocery delivery, oftentimes these are unattended and actually required to drop the package at the door. So why is now the right time for last mile delivery? I already spoke about the industry demand, which has been there for a long time, but it gets bigger by the day because e-commerce is a growing industry and we all get more and more packages at home. But then that requires obviously the logistics to scale accordingly. So the demand has always been there, but the demand was not ready to be fulfilled because the robots were just not good enough. And this is changing now, A, because hardware and sensors are getting more and more inexpensive and therefore affordable, and they're also getting better. But what's really more crucial is that physical AI is now really able to perform all these tasks. So we believe that there's an iPhone moment coming where we see robots really going out there into the real world with widespread penetration into a future where you do really see robots out there all the time, just like you know these days see e-scooters driving through the city. Why is this now possible? In the past, this is kind of how your general or your a normal robotics stack looked like. You have individual components, all of these server serve very narrow focus, but they need to be meticulously engineered to be very robust. The interfaces between these components need to be really robust. All of this needs a lot of heuristics and a lot of compute to make this actually run in real time on real hardware. And then oftentimes it doesn't even really work because the complexity of the task is so big. And this is where five years ago reinforcement really really learning really stepped in to revolutionize at the time locomotion. So that was the first time where anyone but Boston dynamics was really able to do robust locomotion with legged robots. And this revolutionized the space by basically merging two of these previous components that were very expensive at runtime into one single neural network, which was really inexpensive to run in real time. And it also provided provided a really robust locomotion because you can train these networks in simulation. You can randomize environment to obtain really robust performance. And this is what we continue doing at River today. So we have seen that more and more parts of the robotic stack are being eaten, so to say, by neural networks to achieve end to end learning where you can really optimize the entire task end to end through data. So really, the only thing is left is the robot itself and then some kind of large scale navigation, Google Maps style. We provide general guidance, but then the actual task autonomy is all performed by neural networks, which can, which is really scalable. It's really robust and can run on inexpensive hardware in real time. Specifically, how we achieve this is through the use of simulation and reinforcement learning, as already mentioned. So the key observable spot that this is NVIDIA's ISAC SIM, where we train our robots. And then once we train our locomotion policies, in this case, they transfer SIM to real directly to the hardware without any fine tuning. And this is not limited to only locomotion. You can also train navigation in these environments. You can overcome stairs. And again, all of this transfers SIM to real without any problem. So you can directly deploy new tasks. This also works for previously unapproachable problems like this recovery task, which is very contact rich and was classically very hard to solve. And more acrobatics motions like these standing up on two legs. I mentioned that simulation and reinforcement learning are the core components for this. Here we leverage NVIDIA's ISAC SIM for simulation and then NVIDIA's ISAC Lab for the reinforcement learning component. But that's only part of the equation. What's also really important is real world data. We've seen this with foundation models. The availability of data is really key to achieve cutting edge performance. And this is why the why the core of reinforcement learning is so important. You can already achieve really, really good autonomy performance in the real world. You can then deploy your robot in the real world, collect data to tackle all the edge cases that were previously not solveable with simulated data. When it comes to using fine grade manipulation, more usage of image data, then you can feed that data back, improve your performance, gather even more data. And therefore, you can get a really powerful data flywheel to really continuously improve performance of robots to make sure that you really have autonomous performance at scale in the real world. So this is what this then looks like when you actually deploy it on real hardware. So we have autonomous path navigation, meaning we can just provide a reference path. The robot will follow it autonomously. It will avoid obstacles if there are some in the way. It can at the same time still overcome really challenging terrain like steps and stairs. And this is all emerging behavior. None of these motions were hand scripted. And I mentioned this before, even if the robot, which happens very rarely, does have a failure, we are able to recover through reinforcement learning, continue the task, which really allows us to solve edge cases, which are usually the hardest part of the robotics system. We can overcome really different terrains, really natural terrains as well. So they are here. You have really complicated contact dynamics. It's muddy. It's slippery. You get your wheel stuck in vegetation. And all of this is solvable through a simple real transfer. Now, this was just a list of skills, but how does this actually look like when you apply it to the use case of last mile delivery? So here we have our wheeled legged mobile manipulator. It is being loaded with a package by a human delivery driver. In this case, it is provided the location of the delivery. It will then navigate to the doorstep, is able to precisely place the package where it's supposed to be. Then very important, actually, for last mile delivery, it can take a picture of the delivery for proof of delivery with a camera that it's integrated into the mobile manipulator. The customer will receive a notification. Your package is here. And then the robot will return to the car, enter the van, and the delivery driver is able to go to the next stop. And this is how we foresee the future of last mile delivery, really fully autonomous robot delivering your packages. And with that, I'll reach the end of my presentation. Thank you all for your attention. And I'll turn it over to Stefan and Lars from Wanderdots. Stefan Wanderdots. Thank you, Lawrence, for the very kind introduction. We're happy to show you the latest and greatest today on our product, Bundlebots Nova, with a focus on your collaboration with Ipedia. But first, real quick, who are we? At Bundlebots, we stand for a new era of automation. We think that a new era is needed so that automation with robots can become accessible for every developer and end users on the shop floors of this world. And so that the industry can grow significantly faster than it does today. So what is holding back industrial robotics today? Why is it not possible for more developers to work with robots? And why is the industry not growing faster? The main problem that we see is complexity, high entry barriers, and a lack of flexibility. Let's face it. Robotics and automation development is stuck in the past. Outdated and fragmented tech stacks, flunky developer user experiences, and a too large, sting to real gap slow developers down. Even simple automation projects or adjustments to production lines require rare expertise and are held back by subload knowledge and access it. That kills momentum and innovation. The entry barrier is simply too high. That's why automation, from our point of view, is not scaling faster. It's time for a shift, a faster, easier, and more reliable way to plan, build, and operate robotics. That's why we build our product and why we're partnering with NVIDIA to redefine how developers interact with robotics. We bought robotics. The new era of automation starts today and it starts with us. But before we dive into more product details, let me give you a brief overview of what we have prepared. I'm going to wrap up my introduction shortly with a dive into the product, Vani Watsnova, and the extension that we've built into NVIDIA Omniverse. Then Lars will take over to show you how two of our customers are leveraging the product and the connection to Omniverse already to date. And we will also give you a preview of what's coming next in our development efforts. Nova is a fully-fledged, agnostic robot operating system that serves as the foundation for advanced automation. It serves as a basis to standardize all automation styles and robots in productive setups onto one software operating system. One software system, one programming environment, one user interface and deployment infrastructure for all automation lines and robots with already over 300 integrated and supported robot models today and counting as we add more to the system on an ongoing basis. It comes fully equipped with a developer platform that enables the operation of virtual and physical robots individually or in parallel. It provides a system infrastructure to integrate peripheral devices, tools, cameras, services for advanced and individual automation needs. It enables cloud-based management of the system and also fast on-prem deployment when and where that is needed. It provides in-depth documentation, REST APIs and SDKs for Python TypeScript and tools and examples, UI components for building robot programs specific to your use case needs. And this is key. And this is key. It also comes with our NVIDIA Omniverse connector for working with virtual robots and leveraging iSAC-SYN. The integration of Bundleboot's Nova with NVIDIA Omniverse specifically allows you to use the described features of physical and virtual robots in exactly the same way, relying on one unicide controller, software system and developer environment. This lays the foundation for a significant closure of the same real gap experienced in other offline platforms. And with that, a lot becomes possible as our users and customers are experiencing today. And to tell you more about that, I can do virtualize. Thank you, Stefan. Thank you, Stefan. And with Bundleboot's Nova, users can seamlessly work with multiple robots, as you can see, from multiple robot brands without being locked into one single manufacturer ecosystem. Our scripting language, BundleScript, which you can see here, simplifies the complex robot programming, making it more intuitive and adaptable. Here you can see robots from four different manufacturers running within a single unified program, all simulated in NVIDIA iSAC-SYN. By validating the entire interaction in iSAC-SYN, we ensured seamless coordination before transferring the program directly to the physical cell without any adjustments. To showcase this, I prepared a side-by-side comparison. Without the labels, you would barely notice the difference between the real world and the virtual simulation, providing how precise and reliable our approach is. Manually programming robot paths can be time-consuming and error-prone, especially when working directly on the physical cell. Manually programming robot paths is what we did before. That's how we operated in the past. But with the integration of NVIDIA iSAC-SYN into our workflow, we can now ensure that every path is collision-free, efficient and optimized before deployment. In this clip, you'll see how this works using palletizing applications. As an example, first, we create a virtual scene within iSAC-SYN. Then we connect the palletizing app in iSAC-SYN. Next, the user defines the palletizing pattern. And then we can just start. And by leveraging iSAC-SYN, we can simulate collision detection and validate real-world physics, giving both us and our customers the confidence needed for seamless automation. This use case was also developed in collaboration with our customers, CoboWorks, who rely on VandalBots Nova to operate their palletizers on their customers. And here you can as well see that we trained everything in the virtual scene and transferred it to the physical world with ease and without any adjustments necessary. Thank you. Traditional assisted path planning is no longer enough to meet today's automation challenges. In complex environments with heterogeneous workpieces, intelligent tools like perception sensors and cameras are essential for precise and adaptive automation. With smart path planning in VandalBots Nova, AI-assisted algorithms automatically generate optimized trajectories, reducing cycle times and improving overall efficiency. In this scenario, you see the sending process fully virtualized before being commissioned in the physical world. NVIDIA iSAC-SYN plays a crucial role by enabling real-time workpiece scanning, trajectory planning and testing multiple variations in a virtual space, ensuring reachability and performance before execution. Our intuitive app guides the user through selection of surfaces and edges of the sending of the scanned workpieces from their collision-free trajectory is planned, followed by a dry run in the virtual environment. Once satisfied, the customer can then seamlessly transfer the program to the physical cell, as you can see here, where it runs identically to the virtual simulation. A great example is also our customer, Blox, who operates a sending cell with a ceiling-mounted Ano-Robot. During the commissioning, we used iSAC-SYN to determine the optimal mounting position, ensuring the robot could reach all sides of the workpiece efficiently. If we had done this in the physical cell, it would have taken weeks. Due to the cycle of unmounting, repositioning and remounting, with iSAC-SYN, we virtually optimized the setup, saving significant time, effort and cost for us and our customer. Bundleboards Nova, as a scalable and open platform, seamlessly integrates with iSAC-Lab for advanced simulation tasks like reinforcement learning. By spawning multiple virtual cells, each connected to its own robot controller running on Nova, we can generate tests and refine trajectories efficiently. This approach enables continuous learning and optimization, driving higher process efficiency, improved performance and faster deployment in real-world applications. With that, I hand over back to Stefan. Thank you very much, Lars. And to sum it up with an architectural overview of Nova, we provide you with an agnostic robot operating system that comes with a versatile set of developer tools that allow you to build robot programs and applications and give you the flexibility and ease you need in your automation applications. And with our NVIDIA Omniverse extension, you can build virtual and physical robots and cells in exactly the same way. And those examples of our customers leveraging bundleboards Nova that you've seen in the videos that Lars shared are just two out of many. We've seen many interesting projects realized with Volkswagen, for example, leveraging our tool set for simulating advanced processes that were not achievable with their current tool set. And Scheffler leveraging Nova and the connector to employ reinforcement learning on simulated cells can get to a optimal setup of a new automation cell much faster than they would have been able to do in the past. And if you think that it's interesting, we would love to talk to you. Please do get in touch with us at GTC or via our homepage at finalbots.com. We currently handle the many requests for access to Nova and would be very happy to consider you for our selective early access program. So you can experience the power of finalbots Nova in combination with NVIDIA Omniverse yourself and join us in a new era of automation. And with that, I hand back to Naomi and say thank you to all of you for your interest and thanks to AVV for hosting us today. What an incredible session. A huge thank you to all our speakers for sharing their groundbreaking work. Let's quickly recap the highlights. From Agilox, David and Wolfgang showed us how autonomous mobile robots are revolutionizing manufacturing logistics with precision AI. David from Neuro Robotics demonstrated the power of human-robot collaboration both in industrial automation and everyday situations. Avilesh from Node Robotics highlighted the role of perception-based AI in enhancing navigation and decision-making for autonomous systems. Laurence from River showcased how intelligent four-legged robots are transforming last-mile delivery and how simulation has been key to accelerating their deployment. And Stefan and Lars from Wanderbots amazed us with Nova's integration into NVIDIA Omniverse and Isaac Sim, making industrial robot programming more accessible than ever. If you're inspired by today's session, be sure to check out our other robotics session throughout the week of GTC. And don't forget to explore our inception program for startups. It's a fantastic resource for innovators looking to accelerate their solutions. If you have any questions about today's session or would like to connect further, please follow the instructions in the chat box. Thank you all for joining us today. Keep innovating, keep building, and let's shape the future of robotics together. Thank you. Thank you. Thank you.