こんにちは GMOインターネットの友と申します本日は GMO GPU Cloud サービス開発の最前線と題してセッションをさせていただきます よろしくお願いしますこちらがアジェンダ このような章立てで本日はお話をさせていただきますまずは GMOインターネット株式会社や登壇者の紹介続いて GMO GPU Cloud という我々が作り上げたサービスの紹介をさせていただいた上でそのシステムの構成 ネットワークの構成 また発表者である私がネットワークの舞台であるためインターコネクトネットワークのほんの少し詳しい話をさせていただきます最後に構築のちょっとした話をした上で 構築の感想で締めさせてください40分の短いセッションになりますが どうぞお付き合いよろしくお願いいたします我々 GMOインターネット株式会社と申しますこちら会社概要になります GMOインターネットグループ株式会社のグループ企業のうち1社でパートナー数は昨年末時点で1561名 渋谷に本社を置いておりますが北九州や仙台など複数箇所にオフィスを持っております主要事業としては インターネットインフラ事業と インターネット広告メディア事業名前がよく知られたサービスですと ドメイン登録販売サービスのお名前.comですとかKonohaなどクラウドホスティングサービスを提供しております最近はAI関連事業に力を注いでおりましてプロンプト入力補助サービスの 教えてAIや返品AI昨年は今回紹介するGPUクラウドや Konoha AIキャンバスというAI画像生成サービスをリリースしました続いて登壇者 私の自己紹介になります名前はトモ元気と申しますネットワークソリューションチームという 部隊に所属しておりますGMOインターネットには2018年入社で主にクラウドサービスの基盤ネットワークなどの 設計・構築・運用など担当しております昨年度からGPUクラウドプロジェクトに ジョインしましてGPUクラウド環境のネットワークの 設計・構築に携わっておりますまたオフの話ですが 出身は大分県 趣味はアウトドア釣りや狩猟採集などやっており平日はAIやネットワークがどうものなど 行っていて週末は大体野山にいるという 対照的な生活を送っております本日はよろしくお願いしますさて それでは我々が開発した GPUクラウドサービスの紹介に入っていきましょう昨年NVIDIA社のリファレンスアーキテクチャを 活用させていただきましてGMO GPUクラウドというサービスを構築しましたまずは広報用ではございますが ビデオ動画をどうぞGMOGMOGMOはい 動画に出てきたサーバー でかくてかっこよかったですねサービス名 GMO GPUクラウドと言いましてAIモデル学習や機械学習に最適化された GPUクラウドサービスですチューニング表 シンプルな開発環境で計算資源を利用できる というところが売りでございますなんとこのGPUクラウド2024年の11月公開の スーパーコンピューターの性能を示す世界ランキングトップ500にランクインさせていただきました値は秒間38ペタフロップス10位は世界37位国内6位という結果をいただいております ありがとうございますちなみにネットワーク屋としての観点でのポイントは 標準的なイーサネットを用いたクラスターの中では2位1位はNVIDIAのISRAEL ONEですが その次にランクインしております他社様はインフィニバンドや 有名な日本のスパコンのKEIとかですとTOEF INTERCONNECTなど 独自のものを使っていますGMOのGPUクラウドは イーサネットを使って総合37位イーサネットだけなら2位 ここがポイントだと思いますざっくりサービス構成です簡単に言うとパワフルで 使いやすいサービスなんですがいくつかポイントを挙げさせていただきますとまずNVIDIA社のH200GPUを 8枚搭載した GPUノードを提供しておりますネットワークはNVIDIAの推奨構成であるリファレンスアーキテクチャに 準ずる構成をとっており高帯域 低遅延 そんなネットワーク基盤の上で GPUをご利用いただけますSlamを用いたシンプルな ジョブ投入システムで開発には AIソフトウェアプラットフォームとしてNVIDIA Enterpriseを利用可能となっておりますログインサーバーには SSHで接続しますこちらテラタームで繋いだ画面になりますこのようにLinuxベースで操作しますログインサーバーで開発や データのダウンロードなどジョブを実行する準備を 整えていただいた上でスラムコマンドを実行することで GPUにジョブを投入できますこちらの動画では簡単にRama3という言語モデルで 推論結果を出力させております続いて このGPUクラウドの中身システム構成の説明をさせてくださいざっくり画像図なのですが こんな感じ大きく独立した3つのネットワークと 2つのサーバー群に分かれています上から見て インターネットの出口がクラウドフェア経由で存在しますこちらが弊社のバックボーンネットワークに つながっておりましてその下にインバンドストレージと 呼んでいるネットワークがありますこちら いわゆる従来通りの サービス通信を処理するネットワークですこのネットワークに GPUクラスター ストレージ サーバーファームがつながっておりデータ転送や ジョブ投入システムとの通信を提供しております左下はOOBと呼称しているネットワークで 運用監視に使用するネットワークです最後に 図の右下のインターコネクトと 記載されているネットワークこちらは GPUクラスターのみつながっておりRDMA通信を行うネットワークですここに Spectrum X を採用しており今回のセッションで一番しゃべりたいのは ここ後ほど詳しく説明をさせていただきますこちら GPUノードの構成です筐体は DELL PowerEdge XE9680 という高密度 GPUサーバーを採用しています動画に出てきた 6UR いかついやつですねそこに NVIDIA の H200 GPU を搭載また Bluefield 3 SuperNIC という プログラマブルNIC を採用しておりますネットワーク観点では この Bluefield 3 SuperNIC が重要です詳細はネットワークのところで お話しさせていただきますストレージは DDN の ES400 NVX 2 を採用いたしましたNVIDIA 製品との連携に優れたストレージでNVM インターフェースを採用して 高速通信を実現しています容量は 100TB のリソースを提供可能弊社環境の実測値ですが 読み込み速度は 170GB 秒書き込みは 141GB 秒 の速度を確認しておりますGPUノードのジョブ管理システムは SLAM を採用しておりますこのように 利用者はログインサーバーにログインし ジョブを投入ジョブ管理サーバーでスケジューリングされ GPU サーバーがジョブに割り当てられる仕組みですはい ここまで ざっくりシステム構成のお話をさせていただきましたがここからは ネットワークの話をさせていただきますNVIDIA 製品を利用した ちょっと深い話をさせていただきますまず 先ほどお話しさせていただきました通りネットワークはインバンドストレージ インターコネクト OOB と3つ存在しそれぞれ独立し 役割が分かれております今回は その中で 特に NVIDIA 技術を採用したインターコネクトネットワークについて お話をさせてくださいGPU のネットワークに関わる皆さんは もうすでにご存じかと思いますがまた インターコネクトというのはGPU がノードを超えて連携する際GPU と GPU の間で計算情報をやり取りするリモート・ダイレクト・メモリアクセスRDMA 専用のネットワークでGPU 基盤になくてはならない存在ですGPU 基盤における通信は繊細で転送が遅延し パケットロスが発生すると計算のパフォーマンスが 劇的に低下してしまいますつまり ネットワークの遅延がGPU のパフォーマンスに直結するわけでどんなに大容量の通信を行ったとしても全トラフィックを適切に転送できるロスレスネットワークの構築が 要となりますその ロスレスネットワークを実現するため今回 ネットワークトポロジーにはRail Optimized TopologyFull By Section というものを採用しましたこの構成について これから説明をさせていただきますまず GPU ノードには 8 枚の GPU が乗っているわけですがこの GPU ノード単体で処理できる トラフィックであればNVスイッチというスイッチングチップでノード内で通信をしますしかし 1 ノードで処理できないほどの 大きな計算は複数の GPU ノードで連携をして 処理をすることになりますそこで インターコネクトネットワークを 使用するわけですねGPU の連携において 計算能力をフルに発揮させるためには1 GPU ごと 計算に必要十分な 転送能力を確保しなければなりませんこの単位をRail と言いますこのRail を最適化するためBlue Field Super NIC先ほど話した 高性能NICですねこれをRail の数だけ搭載さらに 400GB Optics を合わせ400GB インターフェースを1 GPU ごと 1 つずつ 割り当てる構成をとっていますつまり 1 GPU ノードあたり400GB を 8 本 割り当てる形になります1 ノードだけで 3200GB の帯域を占有する形になりますこれをインターコネクトネットワークの上位スイッチいわゆるリーフネットワークスイッチに接続するのですがレールオプティマイズドの設計では各ノードの同じレール番号を同じリーフスイッチにアサインすることで距離が最短になり パフォーマンスを最大化することができますまた今回 GPU ノードには 4 つの PCI スロットが存在しPCI スロット1 つに 2 つの GPU が割り当てられていますですので これも同じリーフスイッチに割り当てていきますこの図のように PCI スロット1 に GPU 1 にが割り当てられているのでセットでリーフ1 に割り当てるみたいな形ですねこのようにリーフスイッチ4台をアサインし各 GPU のインターフェースを同じように割り当てていきますこのリーフスイッチには今回NVIDIA の Spectrum X に対応した SN5600 というマシンを採用しましたイーサネットスイッチであり 800GB の OSFPこれ QSFP よりもちょっと太い新しめのモジュールになりますこれを 64 個収容することができますここに 800GB を 400GB にブレーカウトする400GB 2 SR4 モジュールを使用しておりますこちらの図のようなコネクタ形状をしておりMP用ケーブルを2本接続できますこの構成で1台のリーフスイッチに128本の 400GB ケーブルを収容することができますちなみに余談ですがSpectrum X という単語がたびたび登場しますこちら NVIDIA 社のネットワークソリューションでスイッチから NIC ソフトウェア パフォーマンス保証まで含んだもの多様スイッチと Bluefield 3 SuperNIC をセットで使用することで最大のパフォーマンスを出すことが有利とされており複装制御のところで詳しく説明しますさて このリーフスイッチ4台の下には32台の GPU ノードを接続しますこれを1つのスケーラブルユニット 略してSU と呼び 1つの単位として設計しております4台のリーフスイッチをアサインしており1台 128本ですので4台で 512本収容できる計算ですGPU ノードが32台あり 8本ずつ使用しますので256本 ちょうど半分ですねこれを GPU ノードの収容に使います残りの 256 本はSU を超えた通信を処理するスパインスイッチ向けに使用しますスパインスイッチ こちらも同じくSN5600を8台採用していますリーフの残り 256インターベースをこのスパインスイッチに接続しています8台のスパインスイッチで現在 SU を 3つ収容させておりますこちら最大で 4 SU まで収容できる設計をとっておりもう1つ SU を追加できる構成になっております以上 こちらがRail Optimized Topology フルバイセクションの構成になりますリーフと GPU の間で 768 本リーフとスパインの間で 768 本それぞれ接続ポイントは GPU 1 つにつき 400 インターフェースを割り当てかつ レール番号を揃えることでGPU のパフォーマンスを最大に引き出すというところになりますねしかし 帯域が太いだけではロスレスネットワークにはなり得ませんというわけで ここからはさらに一歩踏み込んで複層制御の話をさせてください複層というのは ネットワークトラフィックが1箇所に集中し 通信帯域を圧迫帯域が滞る状態で パケットロスの原因の1つですイーサネットベースのシステムではこの複層を前提に作られておりそもそも複層が発生しても問題ないようにアプリケーションが作られていたりどうしても遅延を減らしたい場合は例えば IP 電話など QOS で制御しています複層による遅延やパケットロスを防ぐ方法は大きく2種類1つは複層が発生する前に未然に防ぐ複層を回避もう1つは発生した複層の影響を抑える複層制御まずは複層制御の話から今回 イーサネット上でロスレス通信を実現するためにロッキーV2というプロトコルを採用していますロッキーV2とはRDMA Over Converged Ethernet Ver.2の略称でRDMAで使用するインフィニバンドパケットをイーサネットでカプセル化した上でECNやPFCなど複層制御を行いロスレス通信を実現するプロトコルですこちらを採用して複層制御することを今回前提としております次に複層回避としてスイッチ側でアダプティブルーティングを実装していますスイッチ上で帯域を監視し動的に制御するプロトコルで例えばこちらの図のように2つのノードが同時に通信を発生させたとしても均等に転送しかつ転送順序を維持するように動きますこれにより帯域を均等に分散させトラフィックが1箇所に集中し複層が起きるようなことを未然に防ぐことができますさらに今回のSpectrum Xを採用した本システムではこれらに加えてNVIDIA独自の複層制御が動いていますざっくり言いますとそれぞれのGPUサーバに搭載されたBluefield3 SuperNICで通信の状況を追跡しノードが自動的に速度調整を行うことでより複層支援を軽減する仕組みですこれをシンプルかつ最低限の設定で利用できるのがSpectrum Xの素晴らしいところ簡単に動作を説明します例えばこちらの図では左側のモード1、2が右側の対向ノードへ大きな通信を同時に発生させ右側のノードとスイッチの間にトラフィックが集中してしまい複層によりバッパリングが発生したパターンを示していますこの場合、複層を回避するためスイッチで通信制御を行われ致命的な遅延が発生しないように動くのですがこの通信制御は各スイッチに電波していきますそのため、モード3など他のノードが同じタイミングで通信を行っていた場合その通信もこの複層回避に巻き込まれ通信が遅延してしまう可能性がありますこのような事象をNVIDIA独自の複層制御を実装することで回避することが可能になりますこちらの図で示しています通りノード間でプローブのやり取りを行いお互いの通信を監視していますもし複層が発生した場合プローブでこれを検知送信ノード側で送信レートの制御を行い複層の原因となるトラフィックを未然に抑制することができますこのような動作によりトラフィックをノード時期に制御できるため他のノードに対する通信影響を最低限にすることができますこの動作でより遅延やパケットロスの発生しづらいネットワークを今回実現しています実際に情報を実行した際のグラフこんな感じですこちら20代のモモに対して簡単な情報を実行させその待機使用量のグラフを採取しましたちょっと画面の方が小さくて分かりづらくて申し訳ございません左から順番に同じSUのリーフ1台目2台目3台目4台目というグラフの配置なのですが各リーフで均等にトラフィックが分散されていることが分かりますはい以上ネットワークの構成や服装制御についてお話をさせていただきましたここからは構築のこぼれ話空浪話ノウハウや考慮点などについてお話をさせてくださいまずはこれでしょ最短最速でサービスを提供しなければならなかったこと昨年の4月この時点でGPUクラスター関係のサービスは提供していなかった弊社ですが計算省のクラウドプログラムに認定され今年の11月にGPUクラウドのサービスを提供するとプロジェクトが始まりました何といってもですねGPUは鮮度が命です日に日に性能が上がる分野であるため検証したり構築したりチューニングしたり時間をかけてしまうとせっかく用意した最新GPUを活かすことができません立案からサービスの展開まで最短最速のスケジュールで行う必要がありましたスケジュールを最重要視するため関係各社と密に連携をして今回構築を行いました必要に応じてアウトソースを協力各社に依頼そして工期を短縮定例ミンティングを通して部材の納期に遅れがないこと各構築のスケジュールにズレがないこと設計に問題のないこと度々確認しつつ構築を進めていきました協力各社には多大な労力をかけていただき本プロジェクトは進みましたこの場を借りてお礼をさせていただきます本当にありがとうございましたまた各タイミングでNVIDIAのフレームワークこれを活用しさらにスピードアップさせました最初の基本設計にリファレンスアーキテクチャが役立ったのはもちろん仮想環境に触れることのできるNVIDIA AIRでテストをしたり監視システムでネットキューを利用するなどスピードアップに大いに役立ちました中でもNVIDIAの構築支援が非常に助かりましたので軽く紹介します今回迅速にリファレンスアーキテクチャの認証を受けるためNVIDIAというサービスを契約しましたNVIDIAの構築サポート部隊ですそのサービスは初期のシステム構成の妥当性確認から始まり届いた製品の構築サポートを現地で行ってくださったり組み上げた後のチューニングをやっていただいた上でこれでリファレンスアーキテクチャに準拠し期待通りの結果が出せていることを保証してくださいました厳しいスケジュールの中で今回構築を行ったためNVIDIAのサポートがなければ間に合わなかった可能性があります非常に助かりましたちなみに余談ですが現地エンジニアだけでなくリモートからVPN接続でサポートをくださるのですがこのサポート部隊フランスやオーストラリアトルコなど様々な国のエンジニアがサポートをしてくださいましたNVIDIAはグローバルな企業だと実感した瞬間ですねさて続いて物理ケーブリングの話をさせてください本クラスターでは従来のつまりGPU特化ではないデータセンターを採用しましたGPU特化データセンターに比べ従来のデータセンターは電力・熱の制約が厳しく高密度化ができませんここでこれまでの構築にはない苦労が生じましたまずネットワークラックなのですがSN560マルロを1ラックに4台搭載このラックを並べリーフとスパインは同じラック列に集約させたのですがGPUノードは電力が熱をコールすると1ラックに2台までしか乗せることができずかつラックの間隔を広くとる必要がありましたそのGPUノードが96台もあるわけで結果ネットワークラックとの距離が生じてしまい30メートルから50メートル中距離の接続が生じてしまいました今まではこのように距離が離れていた場合パッチパネルを介してラック間を接続していたのですが事前に聞いておりました他社様のパッチパネルでケーブルの接続点があるとトラブルに生じる可能性があるという事例からケーブルの接続点を最小化するため直結する構成をとりましたリーフスイッチからサーバーレッツまでのケーブルは3SUで768本この膨大な量のケーブルを安全に配線する必要がありましたそこでデータセンター担当者と協議士ケーブルラダーを専用に付設画面に映っている黄色い線がケーブルラダーですこの上にケーブルを通すことで安全かつ確実にケーブルを配線することができました続いてBluefield 3 SuperNICの話Bluefield 3 SuperNICは400以外の対比に加えプログラマブルな処理のできるハイパフォーマンスカードですスペクトラムスイッチと連携することでロッキーV2やNVIDIA独自の帯域制御が動作するという今回の設計の要を担うパーツですこのニックに待機制御の設定を書き込むことで高度な処理を行うことができるのですがしかし構築時のGPUノードの再起動などでこの設定が初期化されてしまいました投資はなぜうまくいかないのか分かりませんでしたこちら最終的に例文を作成し設定を自動投入させることで対応したのですがプログラマブルニックに慣れていないこともあり次にネットワーク機器のオペレーションシステムの話スペクトラムスイッチにはキュムラスリナックスというオペレーションシステムが搭載されていますということで各ネットワークコンポーネントとNVIDIANVUEなど独自機能が動いていますネットワーク機器というよりどちらかというとLinuxに寄っている感じでしていつものネットワークの設定コマンドや確認いわゆる小コマンドなどはNVコマンドという独自コマンドでサポートされていますしかしNVコマンドでは取得できない情報が一部いや結構あるためLinuxコマンドとNVコマンドを使い分ける必要があり慣れていないと若干とっつきづらいOSですとはいえアンシーブルやシェルスクリプトとの親和性は高く自動化にも向いておりネットキューなどテレメトリ収集ツールとの連携もエージェントで最適化されていますソニックなどLinuxベースのファイトボックススイッチに慣れている方はむしろ触りやすいかもしれませんまた継続的な改善もされており現状足りないNVコマンドも全オペレーションをカバーできるようバージョンアップを進めているとのことでした以上簡単ですがネットワークのこぼれ話でした他にも話したい事柄はあるのですが話しきれないためまた別の機会にさせてくださいさて本セッション最後の章構築した感想を話して終わりにさせてくださいクエスチョン構築に関わったネットワークエンジニアとしてNVIDIAリファレンスアーキテクチャやスペクトラムXを導入してネットワークを構築したが何が良かったか対してアンサーですがまずはEthernetで構築できたこと慣れた技術で構築できたのはとっつきやすくて助かりましたそしてクラウドサービスに必要なマルチテナントこれを要員できたのもメリットでしたそしてネットワークのチューニングが不要でハイパフォーマンスが出したことこれはNVIDIAの支援も大きかったのですが我々だけでは十分なパフォーマンスを出すために何度も検証を繰り返す必要があったでしょう次にトップ500これに食い込めたことやはりランキングに乗ったというのは嬉しくあるものです最後に何より最短最速でサービスを提供できたこと総合して今回の構築はNVIDIAとTACでなければなせなかったと言います今回のセッションは以上となりますSpectrum XとH200 GPUで構築したGMO GPU Cloudトライアルできますぜひ一度使ってみてください以上GMOインターネットの友が喋らせていただきましたすべての人にインターネットご視聴ありがとうございましたおわりご視聴ありがとうございましたご視聴ありがとうございました