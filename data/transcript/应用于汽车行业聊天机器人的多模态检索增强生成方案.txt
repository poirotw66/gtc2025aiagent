大家好,我是来自英伟达的解决方案架构师邱赵鹏。今天我会和来自强城汽车AI Lab的责责人王光复共同分享我们合作开发的应用于汽车行业聊天机器人的多模态检索增强生成方案。我会首先介绍一下什么是检索增强生成,以及它的基本原理和在实际应用过程当中所遇到的挑战。然后是英伟达围绕多模态RAG进行了哪些优化。这些优化点可以帮助我们的客户解决传统RAG所遇到的性能方面的问题。最后是由王总来介绍城城汽车如何将这些合作开发的优化点应用落对于实际的产品当中。自从ChateGPT发布以来,无论是开源还是必源的大语言模型都在快速发展。而大模型为我们解决各种任务提供了强大的工具。然而在构建企业级应用的过程当中,我们还是遇到了一个挑战,那就是准确性。首先,尽管大模型擅长各种自然语言生成的任务,但是它们通常缺乏专有知识。也就是说,由于它们无法访问企业内部的数据,也就无法生成企业最需要的洞察,那它的实用性就会有所降低。其次,是有信息过时的风险。虽然大模型是在庞大的数据级上训练的,但是这些数据中的知识可能会过时,而商业数据是在快速变化的。如果我们完全依赖大模型来做决策,那么这些过时的知识可能会导致企业做出错误的决策。最后是我们称之为幻觉的现象。这是指大模型会非常自信地提供部分证据,或者是完全虚构的信息。换句话说,他不知道自己不知道,那么这种幻觉现象也会给大模型的使用者带来很大的困扰。那么我们如何来解决企业实际在使用大模型的所遇到的这些挑战呢?目前一个比较实际且相对低成本的方案,是将大模型和实时的企业数据相连接,以确保他们生成的信息是既准确又是最新的。这种方案就是我们要介绍的检索增强生成,或者简称为RAG。RAG的核心是使大模型能够提供最新的特定领域相关的答案。下面这个图就是一个典型的RAG框架图,可以看到整个框架被分成了四个关键的步骤,我们称之为构建知识库,检索知识,增强prompt以及生成。在构建知识库时,我们会首先对各种文档进行预处理,然后根据需求来对长文档进行切片,之后使用embedding模型为每个切片生成一个项量表示,并将其存储在项量数据库当中。当用户提出一个问题时,这个框架会先去企业知识库当中检索相关的知识,然后将查询到的知识和用户的问题一起输入给大模型,那么我们就会得到一个反映了最新且最相关企业数据的答复。经过RAG框架,用户不仅是在和大模型对话,也是在和他们内部的企业数据之间进行对话。而且RAG已经被成功地应用到了多种场景,首先是对话形式的文档文答,比如说针对内部员工的培训和信息查询系统,对外的科夫系统等。其次是针对开发人员的编程助手。最后,也有很多客户将RAG作为一个基本的技术组件,应用到各种各样的大模型助手当中,比如说汽车配置器。刚才我们介绍了基本的RAG原理,它能够帮助我们快速地去搭建一个POC,但是这距离着实际落地应用,还有很多准确率方面的挑战。这些挑战来自于多个方面,首先是文档解析,我们都知道其内部的数据格式是多种多样的,并不都是纯文版,还有非常难以解析的PDF以及Office文件等等,而后者其实才是大多数。而传统的基于大模型的RAG是缺少多么态能力的,这也就导致基础RAG是无法理解和回答多么态相关的问题的。其次是在检索阶段,很多客户会采用通用的开源向量模型。这些模型和大模型一样,也是在公开的预料上训练而来的,所以说他们可能也无法在所有的私有内部的数据上取得最佳的效果。那么如何对这些模型进行领域微调,如何低成本的去构建训练数据,也是客户所关心的。最后是生成方面,为了推理效率和成本考量,客户可能会采用参数量比较小的大原模型来进行RAG。那么这些小模型可能是发满足准确率要求的,如何对他们去进行进一步的优化,如何来构建数据,以及如何设计训练过程,也都是需要去解决的一些挑战。既然基本的RAG应用往往不能满足企业客户性能方面的要求,那么我们接下来就会围绕着多模态RAG,介绍一些能够提升RAG性能的,由我们和客户合作开发的方法和方案,以及在客户实际数据上的实验结果。为了提高RAG的准确率,并解决刚才提到的客户遇到的挑战,我们设计了更加强大的RAG的参考Pipeline,我们称之为Advanced RAG。相比于基础的RAG,Advanced RAG在文章解析,模型微调,检索生成等多个阶段,均做了很多的优化。接下来我会简要介绍其中的几个关键优化点。首先为了应对企业内部普遍存在的各种各样的多模态的无结构化或半结构化文档,我们的NEMO产品团队是开发了一个叫Multimodal PDF Data Extraction Blueprint的产品,它是结合了目标检测,图表模型,OCR模型等等,多个NEMO模型来从PDF当中抽取文本,表格图表的信息,并保存为一个比较结构化的JSON格式。除了刚才这个开箱级用的Bruppant的产品以外,经过我们和国内的一些客户,尤其是像长城汽车的合作,我们还设计了一个更利于客户做二次开发的,完全基于VRM和LRM的多模态文档解析Pepeline。我们会将各种文档类型统一转换为Markdown的格式,然后Markdown的格式能够以纯文本的方式尽可能的保存文档当中版面结构化信息,而且也更便于大模型理解。我们会以最具有挑战性的PDF解析为例,来简单地去说明一下我们的解析方法。首先,整个解析过程是分为了四个步骤。第一步,我们会使用一些PDF的处理工具,将多模态的PDF页面当中的内容去拆分成不同的元素,比如说标题,图片,表格等等。第二步,因为我们考虑到整个PDF可能是多栏,双栏甚至是多栏的,所以我们去设计了一些规则来基于Bbox,来对元素列表进行重新排序,还原出我们人类的真正的阅读顺序。第三步,我们会使用大模型,将这些元素列表转写为这个Markdown的格式。然后下一步,我们会针对PDF当中的表格和图片,分别使用VRM去生成一段文本描述,并将生成的文本描述,嵌入回原来的Markdown文档当中。而且我们会以超链接的形式来去嵌入这个Markdown文档当中,并把这个链接去指向本地保存的一个表格或者图片的截图。经过这样的步骤之后,我们保证了两点,一点是Markdown当中的所有的元素和原始的PDF当中的阅读顺序是对齐的。另外一点就是我们为所有的这种图像的模态,去做了一个模态的统一,都为他们去生成了对应的文本的表述。经过这样的解析之后,能够达到的效果就如右图所示。首先我们能够在这个答案当中带出合适的图片来回答用户的问题,做到一个图文并茂。第二点就是如果用户的问题是直接和视觉元素有关的,我们的这个Pepeline也是能够处理的。比如说用户去描述了某一个汽车座舱当中的图标,它是小乌龟形状的,然后这个抽象的描述也能够被捕捉并理解,并给出对应的这个图文并茂的一个答案。在知识检索阶段,还有一个非常重要的模型就是向量模型。它的效果决定了能否检索出和用户问题最相关的知识文档,进而影响到最终的答案的质量。对此英伟达提供了Nemo Retriever为服务,它背后使用的模型在开放的基准测试当中,超过了传统方法,开源社区流行的模型,以及很多非商用模型。而且还提供了友好的接触方式,开发者可以很方便地将微服务去集成到他们已有的工作流当中。如果企业客户发现我们提供的模型在内部私有的数据上的表现还是不够理想的话,我们也是提供了一个很方便易用的来对Embanding模型进行微调的一个工具。由于现在对于Birdle的Embanding模型的训练和微调已经在非常多的训练康架当中支持得非常完善了,所以说训练数据的构建才是对于模型微调最关键的一个步骤。所以我们开发了一个基于大模型的半自动化的构建方法来降低我们的客户人工标注数据的成本。具体来说我们会首先将客户的文档使用刚才介绍的文档解析工具进行一个前处理,然后会进行一个切片的操作,之后我们会随机地去采样一个切片,然后利用各种Prompt Engineering的方法来让大模型根据切片来构造一个事实性的问题,并且这个问题可以去使用切片当中所蕴含的知识来进行回答。那么我们将这个切片称之为这个Positive的Trunk,同时会随机采样一些Trunk作为Negative Trunk,然后通过对比学习的Loss来拉近用户的问题和这个Positive Trunk之间的相关度,拉远和Negative Trunks之间的相关度来微调我们的Embanding模型。但由于这种随机采样的副样本和正样本之间的区分度是比较大的,这也就意味着对比学习的训练过程收敛会非常慢,为此我们可以通过利用一些开源的Embanding模型来召回一些相关,但是不能作为这个正确这个背景知识的Trunk来作为一些蓝样本,来加速整个对比学习的训练过程以及效果。在有了训练数据之后,我们可以直接使用NVIDIA的NEMO framework对相关模型进行微调。下面这个表格是我们在客户数据上的一个简单的实验,可以看到在经过微调之后,新的模型在Top1的命中率上取得了非常大的提升。现在业内也有一些检索阶段的一些技巧来提升检索的性能,我们的方案也会将这些成熟且有效的方案集中进来,比如说Contextual Retrieval,我们会为每一个Trunk去生成一个相关的上下文,来建立起Trunk和Trunk之间的关联关系,来把这个全局讯息来引入,来提高整个检索的准确率。而Semantic Splatter是使用大模型来解决定如何去切分文档,来保证Trunk和Trunk之间尽可能是予以独立的。而这个Query的分解是针对复杂问题进行拆解,然后分别检索来提升准确率的。此外我们在Github REPO当中也有一个GraphRag的参考实验,它能够将向量化检索和知识图部检索进行一个结合。我们也将我们的方案在长城的数据上进行了验证,这个场景是长城汽车手册问答,汽车手册是一个图文混排的PDF,里面有很多的座舱示意图和图标等等。然后我们在这个数据上进行了非常详尽的溶解实验,也就是我们会从最基础的Rag开始,一步一步往上添加刚才介绍的一些优化手段。通过这样一个溶解实验的结果,客户可以看到不同的技术手段能够带来的准确率的提升,在设计其他的Rag应用时也会有一定的参考。总而言之,我们从最基础Rag只有58%的准确率,经过一系列的优化可以提升到93%的一个问答准确率。除了刚才讲到的优化以外,还有一种手段是对大模型进行微调,这种方式可以使得大模型更好的学习企业内部数据上的领域知识,并增强大模型根据给定知识库文档回答问题的这样一个能力。这种方式在2023年初还是非常流行的,但是由于现在的开源大模型的能力不断的提升,这种方式也比较少被用到。我会简单的去说一下我们对于大模型在领域上去进行微调的一个典型做法,以及在两个场景上的实验效果。首先我们会对大模型去进行一个post-printing,来注入领域知识。这个阶段的语料可以来自于各种渠道,取决于具体的客户情况。待会我们会有两个例子。其次是构建高质量的问答队。Input可以是这个prompt question以及检索得到的文档。然后答案可以是专家标注或者是自动化构造的。尽管对于这个参数量较大的模型进行微调的必要性已经不是很大,但是SFT还是有一些合适的场景,比如说对于1B、2B这种轻量级的大模型,或者是需要定制输出风格的时候,SFT还是有一定的效果的。这是两个针对大模型进行训练的实际应用案例。第一个是智能客户问答场景。客户期望大模型能够按照特定的风格来生成答复。在这个场景当中,我们能够用到的continual print training的语料,就是客户积累的一些FBQ的数据,以及经过脱免之后的和用户之间的聊天数据。而SFT阶段是由客户标注的高指量的问答队。在这个数据上,我们也进行了溶解实验,结果表明无论是continual print training还是SFT,都能够带来性能上的提升。第二个场景是针对NVIDIA Triton的技术文档问答。这个任务比较垂育,而我们能够搜集到的预训练预料就是开发者文档,和Triton GitHub仓库下开发者提交的艺术讨论等等。而SFT是我们搜集的同事去标注的一些真实的问题和答案。同样,实验结果也表明了对于大模型进行训练是能够在锤域的场景以及任务上带来性能提升的。以上就是我们围绕着多么泰RAG的一些优化点的介绍。接下来是由来自长城汽车的王总来介绍我们合作开发的这些优化点,是如何应用于长城汽车实际产品开发过程当中的。大家好,我是长城汽车AI Lab的王光辅。下面我将分享一下长城汽车关于RAG平台的工作。知识问答在长城汽车有着非常广泛的需求和应用,首先体现在C端场景。现在汽车的功能非常复杂,我们的用户可能对一些功能的使用方法并不了解,那么一个好的用车助手就非常重要。比如用户可以通过语音询问,HUD是干什么的,如何打开后备箱,车机会自动给出答案。在弊端场景,同样有着非常广泛的需求,汽车制造是一个非常标准化且复杂的系统工程。知识问答系统在延产供销服务各个阶段都有非常强烈的需求。比如在研发阶段,我们的工程师需要找到各种法规国标起标文件,保障设计的合理。在生产阶段,我们的工人需要根据不同的工位,执行相应的手顺。在营销阶段,我们需要关联地方政策,生成申报流程和合规方案。在售后阶段,我们需要快速响应客户提出的问题。因此一个响应高效,回答精准,迭代敏捷的问答系统就显得非常重要。传统的问答系统主要有两个方案,知识图谱检索和FAQ检索。知识图谱检索是通过对实体词提取建立三元组,找到响应问题的关联关系。FAQ是通过建立QA对,加上一些泛化处理,提升问题的命中率。两种方案均可以给出精准的答案,不会出现错误的引导,但也存在非常显著的问题。首先是知识由模型加人工提取,上线周期比较长,泛化存在上线,很难突破。并且多个模型串联,存在累积误差,问题恢复,比较死板。所以通过RAG的方式,建立知识问答系统是目前主流的方案。它可以快速响应,秒级生成答案,通过多模态检索打破跨平台跨数据源的知识孤导,也可以自动化对知识进行归导,降低知识损耗,通过唯一的数据源,降低了多部门协同的成本,通过知识结构化,降低人为操作的失误。大模型加RAG目前有很多开源的框架,比如Lama Index,首先它将数据结构化,再通过Index化处理。当用户提出一个Query时,将提示词Query和相应的数据送入大模型,获得最终的答案。这种方式存在很多问题。首先是文档解析较为粗糙,无法解析图片表格,对于召回排序,Token长度不一致会导致召回率不高,对于一些专有名词和口语化的表达识别较差,对于文档标题的信息命中率比较低,并且对于上下文指代多轮对话的支持是比较差的。因此,长征汽车对RAG流程进行了全链路的优化,首先我们对大语言机座模型进行了全参数Pertrain,构建汽车领域的垂域大模型Coffee Agent,并且在2024年获得大模型的国家备案。针对于不同领域的专有数据,我们通过小样本SFT的方式提升特定任务的准确率,对于某些常委场景,我们通过提示词工程的方式提升零样本问题的效果,另外我们设计了多路召回方式来提升问题的命中率。实现细节,在知识提取方面,我们保留文档、结构,对版面进行检测和分析,利用非视觉解析工具和视觉解析工具,提取标题、正文、插图、表格。切片策略,按照标题、段落、长度等多策略融合,并形成竖状结构。召回模块,使用向量、关键词等多路召回,对标题进行二次召回,对query进行上下文改写和扩展,并添加召回前置模块,来增加历史记忆和上下文解析,以及问题指代消除的能力。下面我们仇中介绍几个和英伟达专家联合优化的点,首先是对于非标准页面的信息提取的优化。我们的知识文档结构非常多样,图文并茂,有很多重要的信息是隐藏在标题,容易被长篇的内容所淹没。因此,我们以YoloV8为BaseModel,训练了11个类别,对容易包含关键信息的区域进行提取,并通过竖状结构解析,保证知识是完整的,可追溯的,来提升准召的精度。接下来是对召回模块的优化,由于我们的用户对问题的描述,差异性非常大,如何让用户以最自然的语言的方式来提问,也能得到答案,是知识问答系统最重要的能力之一。因此我们设计了多路召回机制,首先我们对用户的原始query,利用大模型进行扩展和改写,针对句子相量、观念词、标题等方式进行多路召回,再对召回的结果进行排序,利用大模型总结后返回结果。下面是一个具体的例子,比较有意思的是,我们通过大模型进行了多视角的问题扩展,比如,我们问如何清洗内饰,我们会让大模型以车主的身份,维修人员的身份,新手的身份对问题进行改写。问题扩展为,家用车快速清洗座椅的方法有哪些?高档真皮座椅保养需要哪些专用的产品?儿童在车内弄脏的内饰如何处理清洗?用化学、清洁剂清洗内饰需要注意什么等?以保证问答系统给出更加全面的信息。同样我们对照回模块进行了优化,首先我们对行业Embedding SOTA-based Model进行了对比。BGE模型效果最好,但是参数量比较大。考虑到了推理成本,我们最终选择了ACGE,并且在上面做了很多的优化。对于ACGE的训练,我们从三个维度进行优化。首先是数据增强。汽车手册数据中存在大量含义类似的专业术语,比如说引擎和发动机、波箱和变速箱。我们通过同一词替换,增加口语化表达的鲁邦性。另外,手册中含有特定的代号标识,比如P0171故障码,代表了空然比过锡。我们对故障关联进行扩展。另外,我们也通过contrastive loss的方式对模型进行优化,拉近正样本的语义,扩大复样本对的差异。优化后,召回率从70%左右,提升到了90%以上。在大语言模型方面的优化,我们利用DeepSeq进行了蒸留,提升基础模型的深度思考和推理能力。我们将满学版的DeepSeq模型当成老师,训练我们7B的学生模型。在问题理解能力上有了显著的提升。大模型迭代速度很快,我们也仅跟最新技术进行迭代。传承汽车将大语言模型和Rig系统做成了平台化的标准服务,供各个业务部门来进行使用。主要包含了鱼翼服务,知识库服务和生成召回服务。我们将各个模块分层结偶,增量式的迭代每个模块,增加了系统的灵活性和可扩展性。最后是长城汽车RAC方案和通用RAC方案的对比总结。首先,我们的汽车领域适应能力超过通用方案,召回率我们优化到了92%,远超于行业的通用方案。在困难样本挖掘方面,我们对常委问题的召回率进行了优化。最终提升至85%。通过工程优化方法来降低响应延迟,最终达到120毫秒以内。另外,我们将误达率控制在了5%以内,也是远低于行业的通用方案的。以上就是我全部的分享,感谢大家的聆听。感谢大家的聆听。感谢大家的聆听。