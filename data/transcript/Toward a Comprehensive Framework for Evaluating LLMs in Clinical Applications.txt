 Hello everyone, welcome to JTC. I am Praveen, a principal scientist at M42 in Abu Dhabi. Today I am going to talk about our new framework for evaluating LLMs for clinical use cases. Let me just start the session by asking a couple of questions. AI in general is moving pretty quick. New models are dropping almost every week. But do you think these are ready for clinical use cases? If so, how do you select which one to use for your needs? That's exactly what we are going to explore today. Before we get into the framework itself, let me quickly introduce M42 and some of our work. We are an Abu Dhabi-based company dedicated to advancing AI in healthcare. We operate at a global scale with over 480 facilities and a team of more than 20,000 employees. One of our flagship initiatives is the world's largest genomic sequencing program. So far we have sequenced around 750,000 genomes and we are on track to reach 1 million. What sets this apart is a unique ecosystem. Our technology team collaborate directly with our healthcare facilities. This means that every AI model, every system we develop is tested and refined in real-world clinical environment. With that, let me give you a quick overview of our healthcare AI portfolio. As part of our efforts to bring AI and healthcare together, we have released two versions of M42. These are one of the leading clinical language models out there. These are open access and available on Hutt & Face. You can download and play with it. Beyond that, we are proud to be one of the four global organizations to partner with Microsoft and release a healthcare AI model in their Azure model catalog. This was announced at the HLTH conference last year. Beyond large language models, we are actively conducting research in genomics, building genomic foundation models for data and tackling problems like antimicrobial resistance. Similarly, in medical imaging, we have developed our own imaging foundation models, which has already been put to use in real-world applications like the TB screening tool, which is designed to reduce workload of radiologists. To ensure its clinical reliability, we conducted probably one of the largest retrospective studies using around 1 million chest X-rays. You see, evaluation has always been big at M42. The full details of this study will be available soon since currently this is under review. We have covered a broad spectrum of our AI portfolio from clinical language models to genomics and medical imaging. But as we have seen, building these models is just one part of the challenge. The real test is always ensuring they are safe, reliable, and clinically effective when they are being used in the real world. But the problem is, there isn't always a widely accepted framework to assess clinical LLMs effectively. That's where MediFramework comes in. It provides a structured approach to evaluate these models, ensuring you get a sense of how these models would properly perform once you deploy them in the real world. Now that we have set the stage, let's go over today's agenda. First, I'll just tell you how clinical LLMs are traditionally evaluated and key limitations of these approaches. And then I'll introduce MediFramework and explain how it addresses these challenges. From there, I'll quickly dive into the core dimensions of MediFramework, the components that make it a MediFramework. And after that, I'll walk you through some of our experiments and results showing how different clinical LLMs perform under this framework. And of course, no framework is perfect. So I'll also discuss some of the MediFramework's limitations, areas where further refinement could be needed. Finally, I'll wrap it up with some of the quick takeaways and future directions. Okay, just look at how fast things are moving nowadays. After ChatGPT, there have been several LLMs and the LLM space itself is moving at a lightning speed. We've got major players like Google with MediFramework, there is Anthropik Cloud, and in terms of clinical LLMs, there is MedParm, MedGemini, and Envyutron. Innovative open access models like Med42 and Meditron, open by LLM. All are pushing the boundaries of what's possible. These models are showing remarkable capabilities on standardized tests. Some are even outperforming human doctors on exams like USMLE. I'm sure I'm not the only one who heard some of these crazy claims by companies saying we surpassed human-level intelligence for clinical use cases. But there is always a catch. Passing the exam is one thing, being ready for clinical practice is another entirely. Think about it. Would you really trust a doctor who only knows how to answer multiple choice tests? This gap is exactly what led us to identify some of these critical problems with current evaluation methods. Look at this JAMA paper on the right. It perfectly captures a current dilemma. Traditional evaluations of these LLMs, in general AI models, are primarily focused on accuracy in these medical exams and question answering. Now consider these actual USMLE question on the left. While it's obviously impressive that models can answer some of these questions, it barely scratches the surface of what exactly we need to evaluate. This evaluation completely missed crucial aspects like handling real patient data, addressing biases, and ensuring safety. Even if they don't, you don't have a way of knowing it until we measure it ourselves. Judging a doctor completely on their board's course while ignoring their bedside manner, ethical judgment, and practical skills in the clinic. This realization has led us to develop our Medic Framework that looks at the complete picture of what makes an AI system truly competent in healthcare. Let me just walk you through Medic's five core dimensions here. Starting with medical reasoning on the top. That's the basic clinical knowledge and decision making. Then we have ethics and bias concerns, data understanding, in context learning, and finally clinical safety and risk. So now we have mapped different stakeholders on the left. From clinicians to patients, students, researchers, and there are some various use cases on the right. So each stakeholder has unique needs and each use case requires different combinations of these competencies. The five dimensions that comprises medical. Now let's dive deeper into each dimension. Starting with medical reasoning, which forms the foundation of any clinical AI system. Medical reasoning is really about how well these models can think like doctors. Look at the key requirements on the left. Interpreting medical data, making diagnosis, and planning treatments. But the problem is that even the most advanced models sometimes struggle with what human doctors consider basic reasoning. That's exactly the reason why it is so important to measure medical reasoning in clinical setting. In the real clinical setting. But here is the thing. Medical knowledge alone isn't enough. A model could be greater diagnosis, but still be dangerous if it doesn't understand medical ethics and biases. That's where we come to ethics and bias of the medic framework. This is where things get a little interesting and also a bit concerning. If you can see the screenshot of Lancet Digital Health. Screenshot from the Lancet Digital Health. It highlights something really important. Even models with strong medical knowledge can still show racial and gender biases in healthcare. To assess that, as part of the medic framework, we are testing for obvious biases. But also see how models will handle sensitive medical information, respect patient privacy, and make ethical decisions. Now, closely linked to ethics is another big question. How well do these models actually understand medical data and language? Take a look at these findings from NEJMAI. There is a quote here that really stands out. All tested LLMs perform poorly on medical code varying, often generating quotes conveying imprecise or fabricated information. This is a big challenge. Understanding medical data isn't just about knowing words, but it's really about grasping these complex medical concepts and terminology. Our framework tests, at least tries to test how these models handle everything from unstructured clinical notes to structured lab reports. Think of it like a doctor reading a patient's chart. It's not just about the numbers or text, but the clinical meaning or the clinical context behind that is really important. But understanding existing medical info is only part of the story, right? What about adapting to new information? That's where the in-context learning comes in. This capability is absolutely crucial in medicine, where knowledge evolves rapidly. Not just in medicine for that matter, anywhere. Look at this NEJMAI diagram on the right. It shows how modern retrieval augmented models can incorporate new medical guidelines and research findings into their reasoning process. As part of this, we test four key abilities here. How well model new guidelines, adapt to patient-specific information, and recognize their own limitation. This is one of the big challenges, as you know, in current LLMs, where they confidently spread this information. And also integrate recent research. How well they are able to integrate the recent research. Just like human doctors need to stay current with the medical advances, medical advances do need to adapt to new information without forgetting their fundamental training. But perhaps the most crucial aspect of all this is ensuring these systems prioritize patient safety above everything else. When it comes to patient safety, there is no room for error. Absolutely. Check out this BMG strategy screenshot. It highlights some of the important issues with LLM safeguards against health misinformation. What's really interesting is how different models handle potentially harmful requests. You can see ChartGPT confidently giving misinformation on various healthcare topics without even being jailbroken. Probably you might have experienced this with ChartGPT or for that matter any LLM. To address this, we've built a rigorous testing framework. It evaluates four key areas. Spotting medical errors, recognizing emergencies, and rejecting inappropriate requests, and staying consistent with the safety protocols. Now that we are done with these five dimensions of the MEDIC framework, let's look at how MEDIC comprehensively evaluates these different dimensions through specific tasks and datasets. So our evaluation approach systematically maps specific tasks to each dimension in our evaluation framework. As you can see here, the check marks indicate varying relationship strengths. Some tasks assess multiple dimensions at once, while others focus on more targeted evaluations. This layered approach actually ensures that every critical aspect of clinical competence is tested. While some of these tasks like closed-ended questions with options is quite easy to evaluate, it's quite challenging to evaluate open-ended responses. Actually the tasks like text summarization and node generation or SOAP node creation. To achieve this, we combine some of these traditional metrics like rouge and bird score with some of the new frameworks like cross-examination scores. So that's one of the key innovations of our framework as well, cross-examination framework. A method that enables us to evaluate models outputs without relying on predefined reference outputs. This is particularly valuable in where there isn't necessarily always a single correct answer. And clinical reasoning matters just as much as factual accuracy. So evaluating open-ended generation, open-ended generation like summarization tasks has always been a challenge in NLP and machine learning. Unlike tasks with predefined answers such as classification, open-ended generation produces responses that can widely vary. This makes systematic evaluation quite difficult for tasks like summarization. There are some common metrics that exist like bird score, rouge and blue. Bird score for example compares generated text to reference text using contextual embeddings. And rouge and blue mostly rely on n-gram overlap, but they struggle to capture the richness of human language. And especially so in healthcare settings. The problem is though, all these methods that are conventionally used, they depend on the reference output. But in many real world use cases, as I said, especially in medicine and science, a single correct answer may not exist. So that's why we need alternative evaluation strategies. One that don't rely completely on references. This is exactly what we tackle in our evaluation framework. I'm going to give you some high level details on our cross-examination framework. In essence, this methodology is simply a way to fact check RLM responses. You can use this to evaluate not just summaries, but in use for various other use cases like structure node creation from patient dialogue, or jargon simplification, or even translation pipelines, as you will see later. On the slide, you can see there are three steps in this process. Question answer generation, cross-examination, and score calculation. In the first step, we generate, we have document and LLM generated summary here in this case. We generate closed-ended questions from these two sources, the original document and the LLM response. So these questions are simply yes or no question types. For example, if the document says the patient was prescribed medication X for hypertension, we might ask a question, we might generate a question that says, was medication X prescribed for hypertension? You see, the answer is always yes or no. Once we generate these sort of questions based on the document and the LLM generated summary, next we take these questions generated from the document and then try to answer them by giving the summary as a context to see if we get the same answer and the vice versa. This sort of cross-examination allows us to fact check the summaries generated by LLMs without needing a ground truth, if they've got all the important details right, or if they've added any unnecessary details that are not present in the original documents. So that's the good thing about this framework. You don't really need a specific, a perfect answer, a perfect ground truth for it. The process is self-contained. It uses the LLM response and the original document to do all this. Now let's break down the key metrics that we use to evaluate this AI performance, the LLM performance. These metrics help us understand how well the LLMs are doing. There are more, there are four important ones that are coverage, consistency and concisence. The first metric is coverage. This score tells us how well the LLM summary captures the original document's content. Think of it like this. If the LLM leaves out any important details, we'll see it in the score. So the higher the coverage score, the more the original content the LLM is able to summarize effectively with any important details. The way to exactly calculate these scores is clearly explained in the paper. Please do go through it if you want to know more about it. And then we have conformity score. It's also called non-contradiction score. This score checks if LLM summary contradicts the original document. Imagine the document says something like the treatment was successful, but the LLM summary might say the treatment was not successful. That's a contradiction. A higher conformity score means generated summary aligns well with the original document and avoids these contradictions. And then the consistency. This score, we call it a non-hallucination score. This score really talks about how factual the AI summary is. Does AI or LLM make stuff up or is just sticking to the facts? A higher consistency score tells us that the LLM is providing more factual and accurate and accurate with fewer hallucinations or accuracies. Finally, we have conciseness. This is just how brief the result is compared to the original document. It's purely calculated on the token count. So by looking at these metrics, coverage, conformity, consistency, and inconsistence, we get a very clear picture of how well the AI is performing in terms of generating accurate, concise, and relevant summaries. These metrics also provide valuable insights that help us understand strengths and weaknesses of different LLMs. The best part is, as I said again and again, this doesn't really require a reference output. It is self-contained and it's also highly adaptable. As you can see, you can use it for different types of tasks and different types of real world applications. Where perfect reference answers might not always be available. Before I show the results, these are some of the example questions we use to assess LLMs. As you can see, they span diverse tasks from close-centered questions to more real world tasks like summarization, soap note creation, EHR mining, and jargon simplification to create patient-friendly summaries. And also at the top, you can see, as we kept on saying, safety is the crucial aspect. We have some harmful requests that we prompt LLMs to answer. The main idea here is to how well the LLMs are able to decline from answering such questions. When we tested these models starting with close-ended question answers. In general, larger models tend to do better on these close-ended questions. As we can see from the top to bottom, as the model size is getting bigger, the average performance kept increasing. And there is another trend we observed where fine-tuned models are in general giving better results than their general purpose counterparts. But interestingly, this gap kept reducing as these newer models are being trained with more data. For example, when we fine-tuned a LLAMA 2 model, we used to get 15 to 20% improvement in the performance. But for LLAMA 3, we got barely 1% or 2% because the model itself has 14 or 15 trillion tokens. This really shows the foundational capabilities and using the more data helps. But to really understand what's happening with model scaling, let me just show you another visualization. This is where things get a little bit interesting. Look at the scaling curve. As models get larger, moving right on the x-axis, performance generally improves. But notice the curve start to flatten out as you increase the parameters. This is what we call diminishing returns effect in the AI community. Unless you have some sort of innovation in terms of data or architecture, you're not going to gain as much if you keep increasing the model size. While also another fascinating thing is the different behavior across these tasks. So for example, if you see MMU Pro line, it shows steeper improvements with scale compared to other metrics. When you look at Toxygen, for example, the top line, it actually is relatively flatter compared to the MMU Pro. This actually tells us the safety capabilities probably don't need really big models. If your requirement is completely safe, if your requirement is just about being safe, probably smaller models do quite well as well. Now, while these closed-ended results are impressive and these trends, the real test comes when we ask these models to handle open-ended clinical scenarios. So for the open-ended question evaluation, we used something called ELO ratings to compare models against each other. This is very similar to LLM chat arena. You can Google it, I guess. And similar to what's used in chess rankings, ELO ratings also give us a clear picture of relative model performance. So to determine these rankings, we used LLM as a judge approach. This simply means we use one LLM which acts as an Oracle and is given question and two responses from different LLMs. So the Oracle's job is to say which response when out of these two. This is done repeatedly for all the pair-wise combinations of models for several hundreds and thousands of questions to get the final ELO ratings. As you can see, unlike the previous closed-ended evaluation, bigger models may not always be better. So you have, for example, LLM3405B and GPT-40, we don't know the size, but still could be quite big, is outperformed by some of these other LLM3405B models and MET-4272B models. One of the reasons that when we went through why the performance is lower, a lot of these general-purpose models, they tend to decline to answer healthcare questions, maybe to be safer in all the environments which could have contributed to these lower scores. Nevertheless, this is what the ELO scores look like. What is particularly interesting here is the confidence intervals on the right. So they are for the top models and as you see, this is one of the performance differences that are statistically significant and it's just not a random variation. Now let's look at how these models handle one of the most challenging tasks, clinical documentation. So this is a summarization task and this is where we used our cross-examination framework to measure the performance of LLMs. Overall, the models evaluated in this study show really competitive performance relative to each other with particularly very high values and relatively low variation in conformity and consistency metrics across all models. This is a good news because it indicates in general, generated summaries are free from hallucinations on these datasets and also remain consistent with the input text. But as you can see in the top figure for the consistency scores, the fine-tuned models tend to do 5-6% better than the general-purpose models. So this means they, the models hallucinated less, which is a really surprising thing. And also some reason, mistral models are doing really well in terms of consistency where they hallucinated quite less. And there is also strong correlation, I mean negative correlation between coverage and conciseness. But probably this is expected because longer summaries tend to cover more details, hence the more coverage. And if the summaries are short, for example, here at GPT-4O, you have higher coverage and the least conciseness. So it produced a really long summary. And then for unlike summarization, we also tested these models on SOAP node creation where the input is patient and clinician dialogue and the LLMs need to create a structured report. So here we see similar patterns just like in summarization. But with, you know, for example, if you see ACI bench, the open by LLM, it has perfect consistency. That means it did not really fabricate any information and even the conformity scores are really good. But there is always the trade-off between coverage and conciseness here as well. And we are showing the same data as the previous slides here, but in a little bit different way as scatter plots. So when you do scatter plots between conciseness score and coverage and consistency score and conformity score, you see a little bit of patterns here. So on the left, when you have this conciseness and coverage, there are these groups forming. For example, here you see GPT-4O, Ground Truth and Med42. They are forming into one cluster, whereas the other models are forming into the other cluster. This is quite interesting. And also if you see the plot on the right consistency and conformity scatter plot, the larger models tend to be less prone to heliosynet. As you can see, 405b models, 70b models are all close to 100%, whereas smaller models tend to be a little less on that axis towards the left. So that also shows if you want models for, you know, the clinical documentation probably going with the larger models would be helpful in order to not fabricate information that is not there in the original document. Finally, I just want to draw comparison with the cross-examination metrics and the traditional metrics that are commonly used, for example, BERT score, blue and rouge. So if you look at this correlation metrics, we can see why traditional evaluation metrics aren't enough. Notice how coverage shows a strong correlation with traditional metrics like rouge, blue, but consistency and conformity, the middle rows show much weaker correlations. This actually validates a cross-examination approach. We're capturing aspects of the model performance that traditional metrics cannot. So they completely miss. So, and it has direct implications of how we should choose models for different clinical tasks. All these findings about medical behavior. Finally, we have gone to testing how safe these models are. This graph here shows the crucial things about AI safety in healthcare. We are asked, we asked LLMs, different LLMs to answer harmful requests. And we used LLM as a judge again to assign scores based on how willing are they to answer these harmful questions. Look at this dotted line. This is for our safety threshold that we defined. What's remarkable is that model size isn't the completely determining factor for safety, as we have seen from the closed-ended questions as well. In fact, some smaller models are really, that are really aligned in terms of reinforcement learning, using reinforcement learning, perform better than the larger counterparts. And also, if you see the error bars, they show that these differences are statistically significant. And the clinically tuned models and general purpose models, they tend to perform very well if they are aligned, like preference aligned. And to summarize, looking at these plots together, we have discovered some different patterns. There is no single model that came out on top. Each application requires careful model selection. Some models are good at closed-ended question answers. Some are good at medical documentation. Some are good at the same time. Bigger models may not always perform better, as we have seen. And there are some domain-specific fine-tuning that you can do if you want models that are good at summarization without any hallucinations. And if you are in a resource-constrained environment and you need some safety-critical applications, that would require preference aligned models regardless of their size. Now let's talk about what we have learned about our framework's current limitations. We've used LLM as a judge approach to evaluate most of the tasks. Look at this comparison table on the right, for example. It shows how our LLM-based judges compare with human clinicians. While they are quite aligned on patient safety and practicality, there is a notable difference in risk mitigation assessment. This actually highlights one of our key challenges, the inherent biases that might come from using LLMs as judges. And especially in a situation like healthcare, it's all the more challenging to get real human judges. But it's actually reassuring to see that the differences are relatively small and quantifiable. Understanding these limitations has also helped us map out what's coming in the future. Looking ahead, we see several crucial areas of development. We need to expand beyond current capabilities to handle multimodal interactions. Imagine evaluating AI systems that can interpret medical images alongside clinical nodes. And also all the evaluations are currently automated. The addition of human clinician feedback loops will definitely help us make this better. And also we need more specialized safety benchmarks in this framework. And perhaps most importantly, we also need to better integrate with the real world clinical workflows, which is necessary. And all of this leads us to our key conclusion about medic's role in healthcare AI. Looking back at where we started, this figure represents what makes our framework unique. We've created the first framework that truly bridges the gap between benchmark performance and the clinical liquidity and safety. But here is the crucial point. Medic is a leading indicator of performance and should be only complement to the real world testing, which is a lagging indicator. But this must not be taken as a substitute for the real world testing. This framework with its five dimensions and comprehensive metrics and different datasets provides a systematic way to evaluate and select appropriate models for specific clinical applications tailored to your needs. And with that, let me leave you with our contact information. Thank you all for attending. Feel free to go through the paper. It has a lot more details about experiments. Medic leader board is live and you can go check the updated readers and feel free to upload modules and we're happy to evaluate them for you. For questions or collaboration, feel free to contact me and feel free to connect with me on LinkedIn. Thank you all.