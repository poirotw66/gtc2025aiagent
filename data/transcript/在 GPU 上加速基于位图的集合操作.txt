大家好,欢迎来到GTC我是来自英文伟达GPU计算专家团队的黄赞在这里为您带来在GPU上加速基于位图的结合操作的分享我们会分五个部分展示这项工作的来龙去脉每页的标题均以中文形式展示但具体内容仍然保持英文表述以确保技术词汇的准确性我们发现很多应用都需要高效的结合操作来保证执行效率在搜索引擎中一次查询通常包含每个关键词所触发的文档结合求交集的过程在推荐系统里多路撞回结果的融合通常需要对物品结合去并集并筛去一些结合中的物品数据分析领域有时需要算用户存留率这就需要快速计算日活用户交集的大小建立索引通常能够加速数据库系统执行色扣查询的效率这背后也有关于集合操作的针对性优化近来受关注的领域比如基于神经网络的召回混合检索和RAG也就是检索增强生成也都需要高效的集合操作基于以上这些观察我们相信尝试对集合操作做进一步优化可能是很多应用受益集合操作底层可以基于多种数据结构我们在这里选择基于位图及bitmap或者叫bit set的方法假设集合元素以整数形式表示则每个元素通常要占四个或八个字节但在bitmap形式下每个元素只需要一个比特即可表示其是否存在于当前集合中现代计算设备只是高效的比特云算比如用pop count指令就能够快速的取得自断中已设为1的比特数目用bitmap来表示集合对象其中的元素是自然排好序而且对并行计算友好的如果集合较稠密存取和处理其中值为0的比特这额外开销通常可控综合以上几点我们预期使用bitmap能在稠密集合运算里降低存储开销同时提升计算速度我们观察到Rawing Bitmap被广泛用于处理CPU测的集合操作它将全集均匀的切分为等大的子集并使用不同的数据结构来存储不同吸收度的子集它基于硬件特性做的设计和实现可以确保集合操作效率比如制作8KB大小的子集所占的选择空间对缓存就是比较友好的多年的工程优化也可以确保它能够高效地使用AVX512或者NEO这样的CMD指令不过对于很多JPU应用而言在计算过程中将数据烤回CPU调用Rawing Bitmap做完运算之后再考回GPU开销扩大从物理层面上相同成本之下CPU内存在未宽等方面也可能限制其和操作性能的进一步提升基于以上原因我们尝试利用QDA所提供的能力为那些像含大量Bitmap子集的Rawing集合一样需要更高仿存带宽和比AVX512更宽的质量才能做技步加速的起合操作探索一种可能的路线在做相关的设计和实践之前我们先在CPU和JPU上做了简单的对比实验在CPU上分配2的32次方比特来表示全集元素对每四字节做逻辑与操作对应着集合求交测量所耗时间随后将全集元素填入Rawing的集合然后测试集合求病的耗费时间最后将结果集合的元素导出回整数状态测量对应耗时在GPU上也可以发起大量线程做类似操作对2的32次方个比特也按每四字节做逻辑与测量相应的耗时这并不是一个公平的比较因为CPU测其实只是单线程去运行而且也没有掉Rawing的Rawing Optimize等方法同时在CPU和GPU测都没有做数据预取或者是做特定的指令条优但是这个粗糙的结果依然能够反映不同硬件之间的差异用全级元素数2的32次方然后除以Zeroin做集合操作的时间或者是Kuda Kernel的执行时间在比较早期的NVIDIA T4 GPU以及和它相近待机的服务器CPU上GPU大约每纳秒处理528个比特也就对应了528个集合元素而单CPU纳秒这展示出了GPU设备上的显存的相对的仿存带宽优势以及GPU上更多的线程更便于充分利用仿存带宽的特点这鼓励了我们做进一步探索下面我们在Kuda GPU上做这项工作做一个具体的介绍需要声明的是像CPU上有STD Set Unit这样的APIKuda GPU上也有Thrust Cool可以提供基于排序数组的集合操作像CPU上有Boost里面的Dynamic BitSet这样的类在Kuda GPU上也有Cool Collection可以提供对应的支持在做进一步探索之前需要明确这些工具是否已经可以满足应用需求对需要进一步降低仿存开销或者是加速集合操作的应用而言我们假设集合元素已经以整数形式存放在GPU显存上了同时需要注意Kernel Launch和潜在数据拷贝的开销这种跨越host device两端的操作就像操作系统用户态和内核态的切换一样通常是伴有微秒级的食言的这也就意味着集合中需要有至少万级以上的元素需要处理才能够抵消这方面的成本还有需要说明的是我们尝试将Cuda GPU的仿存带宽在集合操作中更充分的利用起来但是算力其实没有被充分利用除非要做比如说大量稠密集合算两两交集的大小这样的操作才可能通过低精度矩针程做进一步的加速对接下来的实验会从一种比较浪费显存空间的形式展开就是说无论集合大小我们都为其分配全集所需的空间然后参照Rowing的方法将整个集合空间切成等大的子集因为每个子集添加技术变量方便短路计算将子集大小设为处理器友好尺寸的时候不可短路求值的子集间的运算就能够由多个流处理器去高效并行之行进一步的对于以整数形式表示的原始集合元素我们可以使用它的高比特位快速的定位到对应的子集然后利用它的低比特位做索引去操作相应的bitmap里面表示元素是否存在的比特数据对于整数数组或者区间形式给出的输入我们可以做类似的操作来像集合中添加或者删除元素或者是使用某一个集合去过滤元素以及统计指定区间内的元素个数通过每取非空子集我们也可以比较快速的将元素从比特形式转换为整数形式的表示再回到显存开销的问题上为很少的元素分配512MB的空间是不可接受的尤其是考虑到未来要支持更大的264四方的全集空间的情况下这个时候共享经济就能派上用场我们可以准备一个能在kernel内部调用的显存池然后在kernel执行的过程中动态的分配和回收固定大小的显存块通常一个基础的循环对列就能够实现索取功能然后只为那些既不是空的也不是满的紫期分配显存块这样就可以保证总体显存开销可控到这一步为止负载均衡其实还是个潜在问题流处理器间处理的紫期个数相对难以平衡而总体的操作时间取决于负载最重的流处理器为了解决负载均衡的问题我们可以沿用bitmap的结构使用其中的某一比特去记录某一特定次级是否为空以及是否为满这样既方便做短路运算又能够比较方便的每举出必须进行操作的紫期间的运算的紫期ID然后launch适量的block去处理实际的紫期间运算这个思路可以比较自然延伸到一个多层bitmap的实现以应对更大的全机空间的情况在总体思路确定之后具体设计上则有更多的考量延续之前的实验条件我们在1000万以内这个较小的区间内均匀的采样出了不同稠密程度的整数集合然后测试一个没有多层级设计的基础实现进行集合求交的性能如图表展示的引入显存池的话可能会带来一些比较明显的性能折损但是无论是否使用显存池在我们在这里所列的条件之下基于bitmap的实现做集合求交的速度都要明显快于对比方案而且在各类硬件上面其实我们能够比较快速的估算出一个性能上限就是用所需处理的比特数除以硬件所能提供的仿存带宽来估算然后后续的优化工作其实基本都是围绕着在不引入过高的额外开销的情况下缩减所需处理的比特数以及提升仿存带宽利用率展开的在后续实验中我们发现为了保证集合操作的行为或者是结果的正确性我们在整体的变形计算过程中有时会引入串行操作而且做负载均衡也伴随有额外的开销提升吞吐和降低延迟也不总是一致的两个目标具体而言将全集空间切分为等大的子集是便于做并行计算的但是在每个子集内部为了正确的技术子集中元素个数我们有时候会使用原子指令更新同一个技术变量这时候可能会隐含式的将操作串情化类似的一些显存值的实现也依赖原子指令以保证正确性这也会间接造成一些操作被串情化而且Kuda本身其实是支持在Kernel内部做Moloc和Fread调用的但是它在底层仍然是将这些操作串情化的所以效率难以提升有时候为了将集合元素导回回整数形式的表示我要确定在输出的Buffer里面的写入位置可能也会需要做一些串情的操作以确保它的正确性之前提到的多级Bitmap设计虽然有利于做负载均衡但是它其实隐含的把原本比特化的此级状态放大为了整数形式表示的此级ID其实客观上是增大了数据的读写量而且如果用多个Kernel实现多级Bitmap功能的话它可能会引入多余的KernelLunch开销还有一些潜在的D2H数据拷贝如果带触记的集合操作成P出现而且相互之间没有依赖关系的话每个操作其实用单个Block去处理相对于用多Block处理的话它的同步开销可能更低使得整体吞吐可能更高但相应的这个时候每个操作单独的时间会更大所以对于这些问题我们就需要做一些设计上的折衷更进一步的一些理论上足够高效的算法到具体硬件上做实现的时候需要考量的因素可能会进一步增多比如说这里右边所示的将Block导出成整数状态的代码除了里面所展示的原子操作的开销之外还有可能受到Warp divergence问题的困扰再比如如果每个子集所占用的空间我们将它划定为小于8KB打小的时候每个子集的装载率可能进一步提高但是子集ID就超出了Uint16可以表输的范围我们需要用更宽的数据字段去记录这些ID同时如果将这些子集ID缓存在室央的memory或者是速度更快的存储空间上面有可能能够提高仿存效率但同时也会受到存储空间容量的限制并有可能需要做block间的同步付出比较高的同步开销我们在实现的时候也可以分配更多的存储空间但这时候也可能引入更多的浪费我们也可以参照Rowing的方法引入速组和优生编码来存取集合元素但它们其实并不是GPU所青睐的仿存颗粒度和并性度有可能是我们难以从中获取性能收益我们也可以尝试用CPU、GPU协作处理集合操作但是这样其实很容易被PCIe带宽平均卡住最后就是就地操作有时候速度会更快但是具体实现的时候为了保证计算正确性有可能会付出额外的开销来处理就地操作时一些相对模糊的语义目前我们还带继续探索从算法和工程两侧进一步优化集合操作的方法在这里列出一些暂行的方案以应对之前所列举的问题像Rowing一样混用不同的数据结构来适应子集吸数度的变化看起来很合理但是在GPU测灵活地使用Beatmap这种数据结构缩减子集的大小也能够达到类似数组的节约存储空间的效果而多级的Beatmap设计用Beatway去表示自己的状态也能够在一定程度上起到游程编码的作用同时Beatmap又是一种对GPU更为友好的数据结构为了同时兼顾性能和功能需求所付出的显存开销可能无法支持在GPU上保存大量的集合对象但是GPU的流处理器数目会自然的限制住正在处理当中的集合操作以及集合对象的数目这样我们可以采取一种动静分离的设计具体而言就是我们可以以流处理器的数目为参照设置定量的动态集合对象而将非操作状态的集合用静态集合对象的形式存取并尽可能降低它的显存占用这样总体的显存开销就可控另外就如之前提到的各类跨越host和device间的操作带来的额外开销可能过于显著对于一些操作我们可以launch可驻留在CudaGPU上的最大数目的block超过数据所需的block数的时候就将这些block退出余下的block则可以在block内做循环去处理数据以规避一些需要多个cron来处理或者需要一些D2H才能够准确的让此所需block数的操作最后我们会分享一些实现时候的技巧并列举一些可能的后续工作为了保证操作效率我们有时候需要对指令做精调举个例子比如说利用循环对列实现简易的可在kernel内分配和回收定长显存块的显存池的时候我们可以用atomic add指令来操作游标然后在外面接上取摩运算来确保游标在即将越界的时候只回到对列的头部也可以用atomic ink指令来同时完成整个操作我们在这里实验对比了在kernel内发射同等数量的两种atomic指令操作单一数据字段的实验发现调用大量的atomic add kernel要明显快速调用了同样数量atomic ink指令的kernel这里面一个经验性的总结是通常提供了比必须的特性更多功能的模块也伴随着更高的开销同时JPU上设备显存的分配的细节其实也值得更多的关注因为它通常比内存分配的时候科技度要更粗即使只申请一个字节的空间kuda malker可能在底层仍然会划分256个字节的一块区域来满足具体需求这就使得比如说go在倒排索引的时候每条链路挂在比较小的结合对象那种场景它的显存开销可能会被放大尤其是对于每个集合都只有各位素元素的极端情况这个时候我们其实可以保持集合元素为原始的这种整数形式的表示然后分配专门的显存空间将所有的整数存入同一个数组里面同时记录对应区间的位置用于后期计算通过这种方式规避掉说原来原本的元素只需要十几个字节就可以存下但是却不断地开辟了至少256个字节的零散的空间造成的浪费的现象在未来工作方面我们预期COOL MEM是可以帮助进一步降低显存用量的而COOL GRAPH可以帮助减少一些Current Launch之类的额外的开销Rapis有可能能够提供更好的显存值的实现Graceropper以及后续产品则可以为CPU GPU协作处理集合操作提供更好的硬件基础最后我们还在评估是否要增加一个前端组件将集合运送的表达式解析之后再交由后续计算模块处理以更好的调度资源提升总体吞吐这一页里面我们列出了一些参考资料可以通过链接查看具体的内容以上就是这次分享的全部内容它同时也是在Cuda GPU上加速数据科学工作负载的一个缩影感谢各位的观看有任何问题可以继续沟通交流谢谢