 I Thank you. Hi, everyone. Thanks so much for joining the session. I'm Phil, the CTO at Basecamp Research. We are a data-centric AI company in the life sciences based in London and Boston and really excited to talk to you today about how we're overcoming the data wall in the life science industry and we've been building a foundational data set for AI and biology. At the start, I'll just do a brief company intro of who we are and what we've done and who we're partnering with and then talk a little bit more about the challenges that we've addressed with what we've built. We've just raised our Series B. We're backed by a whole range of industry leaders working in the life sciences, in the pharmaceutical industry, as well as in deep learning and in the tech space. And we're really excited to have this level of support. In addition to this support, we've been partnering with the leading labs and companies across the globe. That includes some of the leading gene editing labs, such as David Liu's and three of the top 10 pharmaceutical companies and consumer good companies that have been working on enzyme development. Most importantly now, I want to talk about what we've been building and addressing, which is the data wall in the life science industry. This is a resource developed by EPOC AI, which I thought is a great place to start this discussion because very excitingly, bio AI and applying deep learning to tasks in the life sciences has been such an exciting development over the past few years. But as you can see here, the development is kind of slowing down and in some cases, maybe even reaching certain limits. I want to talk to you about how we are looking at this problem and what we think some of the reasons for this is and whether that's just a compute or model problem or maybe it's actually a data problem. And that's something I would like to focus on a little bit more today. As everyone is aware, everything we do in the life sciences, everything relating to product development or the deep learning models that we build, everything we do, to some extent, it all comes back to how much we know about biology, how much we know about how life on earth works. And there are public databases that have representations and information from all of this biology with millions of users. A lot of patterns that derive from this information. And these are these public open source genetic information databases. Now, the problem with these databases is that when you compare this to the true space of biological information that exists on our planet, like the number of species, it's a tiny, tiny representation of that. This is a percentage I've written down. It's a zero point and then 17 zeros, 1% of life on earth is only represented in these public databases. I try to do the math to visualize this a little bit more easily. These public databases right now are basically just five drops of water compared to the Atlantic Ocean. The Atlantic Ocean being, that's what we don't know about biology, about life on earth. And that's something that I feel like is definitely worth addressing in this era. And obviously, all the deep learning models that have been developed over the last years for life science applications, they are trained on this tiny, tiny representation of life on earth. And that's something that we feel very strongly we want to address with what we're building. There's additional issues with these public databases. I wanted to address some of them, and it's not just the size of these databases. The first one is that actually for the majority of the sequences, like genome sequences in these databases, we don't actually know exactly where they're from. In some cases, the country of origin isn't known, the location isn't known. We don't know who the stakeholder of that information is, whether it's a nature park or a specific landowner in some cases. The data, as you know, as I've shown before, is not just very small and not very representative. It's also not very diverse. Because a lot of the sequences in there are from model organisms that are well studied, but they're heavily overrepresented in these databases. And then one final point is that as compute and model capacity increases, in some cases exponentially, the quality of the data isn't really increasing. But more importantly, also the size of these databases isn't really keeping up with all the other developments. And that's also worth noting in this context. Now I'd like to talk about how we're actually addressing this, how we're trying to overcome this data role in the life science industry. There's no shortcut. Basically, you have to go everywhere in the world you possibly can and collect as much data as you can, as consistently as you can. And so we've partnered with now 25 countries across five continents. And in partnership with local stakeholders, local governments, nature parks, we've been sampling and collecting data, biological data from all the unexplored environments and organisms on our planet. And there's a whole lot more information. Some of these partnerships we've made public, such as our partnership with Costa Rica, on which you can find more information on our website. And this is a really exciting collaboration that we're doing in many other countries as well. I'm showing this because there really is no shortcut to studying life on Earth other than actually going to these places. And these are some of the environments that's just a small representation of that. We're really trying to go to the extremes, including the Antarctic and ice caps, but also volcanoes. We're doing sampling in the oceans, but also some more adverse environments. And it's really important for us to make sure we're as representative and as broad as we possibly can be when it comes to covering sequence space, covering biological diversity across our planet. As a result, we've built a database that we call Base Graph that is much larger, much more high quality, and is growing at a faster pace than public data. The first thing to note is that every single data point that we have in our database can be traced back to their origin of location, to stakeholder consent. We have full IP and commercial rights, which is something that with public databases, we don't always know whether that's the case. It's obviously a much larger database. We are growing exponentially in terms of the size of this data set. And it's also a lot more diverse because we're really trying to cover the whole tree of life that includes all the parts of the tree of life that have been completely missing in these public databases. This is really powered and fueled by our economic supply chains that we've been building because we've tried to get the incentives right in a way where that means more data collection in partnership with the relevant stakeholders is actually incentivized. And what that means is when we license some of the products that we've developed to the pharmaceutical industries or biotech industry, a portion of that revenue we give back to biodiversity hotspots and our stakeholders, as well as non-monetary benefits. And that's really important because what that does is it gives value back to our planet's biodiversity, but also incentivizes even more data collection going forward. And going back to an earlier point relating to how all the exciting developments in deep learning applied to biological problems is still going back to what being trained on public data. We now have the ability to leverage our data advantage for deep learning applied to biology when we use a much larger data set. And what's exciting here is that previously we've applied a whole range of different models and tasks to the data set that we have. We've, for example, been able to be much more sensitive when it comes to protein functional annotations. We can actually annotate a lot more of what's called the functional DAP matter with 40% additional proteins annotated with a specific protein function. We've been able to improve significantly over alpha fold as well when it comes to protein structure prediction. That's because our sequences are a lot more diverse. We can generate a lot deeper MSAs. We've also built a language model for enzyme design where we can make the generation of enzyme sequences conditional on the enzyme function. And I wanted to talk a little bit more about actually comparing the databases with some specific information to show what the value proposition of such a database would look like. And I'd say that the kind of workhorse for protein AI in many cases is Unipro, that's the database that is often used as a training data set. And I've listed here some of the comparisons. There's not just the technical and biological comparisons that may be important, but also I think there's some additional issues to look at when it comes to questions around compliance and data traceability, data consistency, and so on. That's something that I think is worth pointing out in this space as well. In addition to looking on the protein level, we can also look at this from the genomic level. Our data is collected through metagenomics, and so one database to compare ourselves to would be Magnify. One of the problems with some of these metagenomic databases can be that the genomic context that's captured in such databases can be fairly small. And sometimes these public databases have very, very short genomic context, sometimes so small that they don't capture a whole open reading frame. And so we've been investing a lot into how we sample, how we sequence, how we do the assembly. And as a result, we have a lot more genomic context captured around our assemblies. And that's really important with respect to capturing more biological context that's meaningful. One of the databases or datasets that have been established recently for machine learning purposes is OMG, which is a combination of Magnify and JGI. And we've benchmarked ourselves against that as well because it's now kind of looked at as a standard in this space. And especially when you look at the size of these databases in specific filters and apply them to these datasets, especially when you look at long context and the importance of that, we have a significant advantage here. And that's, I think, important to bear in mind. We've done some comparisons on the size of the data and the diversity of the data, but I think another component that's important to point out here is that base graph, our database, is not just different in size, but it's also different in kind. What I mean by that is we're not just a longer catalog of sequences than what you might be able to find in public databases. We're really trying to be as close as possible to creating something like a digital twin of biology. And what I mean by that is that all these genome sequences, they obviously don't exist in isolation. They evolve in a specific environment. What that means is there's obviously chemical information, there's geological information, there's biological and evolutionary context that we want to take into account. And so on top of all these genomes that we have collected and protein sequences in our database, we're also capturing hundreds of different types of chemical measurements, geological measurements, and we are associating them with all of this genetic information in a knowledge graph. All right. After going through some of these database comparisons, I wanted to talk a little bit more about how we've been collaborating with NVIDIA. It's been a partnership that's been really exciting and important to us, so I can give some of the examples on how we've been working together. First and foremost, we've just invested in an on-prem high-performance cluster with H200s that we've been really excited to be using over the last few months already. It's been set up and NVIDIA has been incredibly supportive in getting this to work together with us. I think one thing that's really relevant in this space is that over the past month, and the growth rate of our data has really been accelerated, our database has been growing by the equivalent of one UNIREF per day. And actually, if you look at another exciting resource provided for EPOC AI, UNIREF is the most commonly used data set for protein AI applications. And so the growth rate of our data has been really picking up, and the HPC investment has been a really important part of making this possible. One thing that we had to look into was actually how we perform search in the most efficient way when it comes to dealing with such a large data set. It's obviously exciting when you have billions or tens of billions of sequences in the database, but this can really slow down a lot of the pipelines and tools that you have on top of this. And so one of the things that NVIDIA has done recently is make MMC one of the ways in which you can search sequences. You can generate MSAs with that. You can also cluster with that. NVIDIA has made this GPU accelerated recently, and there's some benchmarking out there. And we've actually tested this against our own database because such a large database, this is going to have a real impact on us. And so we've been really excited to see that we also experienced a significant speed up with this up to 13 times faster sequence search. On top of this, we also engaged in a research collaboration with NVIDIA that we're really excited about, because we really want to leverage not just the size of our database, which is obviously, as I pointed out, much bigger than public data, but also the added complexity of what's in our data. And so I've just shown this diagram to get this across with respect to what's actually happening on the biological side of things. Because if you look at genomes and foundation models for genomics, there's some models out there that are trained on kind of short DNA sequences that can look at or model DNA sequences with a short context window. Some of them can do this with long context windows. But I think, as I pointed out earlier, these genomes, they don't evolve in isolation in real life. They evolve in an environment that contains other organisms, that contain chemical information, that contain other biological information. And so actually doing this complexity justice, that's really what we're working on with NVIDIA with respect to the next generation of genomic models that we want to build. First and foremost, obviously, this is going to be a trade on much more diverse data. And so this is a visualization that I've done here, comparing some of the public data that's out there to our data. And this is actually only 10% of our data visualized here. But crucially, one thing that we're excited about going back to the added complexity that's captured in our data is that we also want to ensure that this is represented in some of the tasks that we can do with such foundation models. And that can be wide-ranging, such as antimicrobial resistance or gene writing and gene editing applications. But also, how does evolution happen in the context of a chemical environment? And can we understand this better? And can we control this? And these will be some of the tasks that we are really interested in working together with NVIDIA. Finally, this is more of a conceptual point, but I think it's really important. The compute capacity of what NVIDIA has been building with respect to their GPUs has been growing pretty much exponentially over the past few years. And that's really exciting for the field to be able to leverage. But I feel very strongly that when it comes to training foundation model, this has to be mirrored on the data side as well. So this is why I think the collaboration that we have with NVIDIA is incredibly exciting, because we can actually match these two developments. I want to say thank you to everyone at NVIDIA who we've been working with, and thanks to the entire Basecamp team, all of our collaborators around the world, which is incredibly exciting to be able to work with such a wide range of people in so many different environments and so many different countries. Thank you so much for listening. Feel free to reach out. Very happy to invite collaborators and work on exciting scientific problems together. Thank you so much and enjoy the rest of GTC. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.