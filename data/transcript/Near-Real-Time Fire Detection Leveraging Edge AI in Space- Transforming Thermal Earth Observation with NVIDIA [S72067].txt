 Music Thank you. Hi, my name is Julia Gottfriedzen and I lead data science and AI at Dorotek. I'm really happy to walk you through some of our technical innovations and main achievements within this presentation. I'll be joined by my colleague Fabian Schöttl and senior software developer later. Topics of this talk will include why we're sending satellites to space, how we use NVIDIA for onboard processing and how data from space can help fight wildfires. Let's go. We're Dorotek. We have been founded in 2018 in Munich. We're now over 130 employees in over four continents and we have above 300 users in 20 countries. Right now, we're ingesting over 27 satellite sources into our platform and we have two satellites up in space operational. Our IP is within our thermal camera. We're building a constellation more on that later. We use, of course, AI-supported analytics and our data is accessible via our own visualization platform or via API. Some news from the last month. So we won a 20 million contract with the government of Greece. We secured 25 million additional funding through our CRSB and we won a 72 Canadian dollar contract with the Canadian government to build a constellation for wildfire detection monitoring. So it's going quite good for us. Also, we're becoming a Copernicus data provider. That means we're recognized by the European Space Agency to become a data provider within their Copernicus contributing mission program. Now I want to shift perspective to our users. Wildfires happen all over the globe and they become more and more severe. Here you can see the impact and the burn scars of a wildfire from last year in Greece. Clearly, wildfires impact vegetation, they impact people and they destroy livelihoods. This was the first detection of this wildfire from a satellite at 3 p.m. in the afternoon. At this point in time, the wildfire is still the size of a small tree or a shrub. So there's no indicator how large this wildfire will grow over the next hours. In our platform, our users can simulate the progression of the wildfire over the next hours. As a data scientist, you often don't want to be right when you see predictions like this. Unfortunately, we were right and the fire became one of the most destructive and severe in the Greek history. There were over 700 firefighters fighting the fire, 17 aircrafts in the air and overall a large amount of resources spent to try and contain the fire. From our customers, we're hearing that satellite intel is becoming one of the most critical sources for observing the daily progression of the fires and producing those progression forecasts to know where to place the resources that fight the fire. So our customers are actually making decisions and spending resources based on the intel we provide to them. Another story is the wildfire that recently happened in L.A. where no aircrafts or helicopters could go up anymore because the winds were so strong, which is often happening in wind-driven fires. And then to know where the fire is right now and how it's progressing, satellite information is the only information that the firefighting agencies can rely on. So it's becoming more and more important with fires becoming more and more severe. We just saw that the satellite that detected the fire first was satellite at 3 p.m. Unfortunately, there is a data gap in the afternoon, especially between 3 and 5 p.m. and also earlier between 1 and 3 p.m. where most fires ignite. So here you can see on the y-axis the average fire frequency and then over the x-axis the course of a day. And we can clearly see with the red dotted line that the fire peak or where most fires ignite is in the afternoon. And unfortunately, there is very little satellite data available at the moment. And that's exactly why we're building our own satellites. So it's not enough to ingest the data from all the existing sources because there is a gap. And that's where we launch our satellites into. So this is FOREST 2. It's our workhorse that's up in space at the moment. It was launched in June 23. It has a 410 km swath. That means the field of view of the satellite when it passes over any place on Earth with a 200 m GST. Its sensitivity is at 4 x 4 m. That means it can recognize fires that are as small as 4 x 4 m. And it has three thermal channels. One in mid-wave and two in long-wave. And we'll explain what that means later on. It also has an RGB sensor on board. So an RGB camera. And that's the specialty. On all our satellites, we're flying an on-board GPU for image processing. Now, where we want to get to is illustrated in this slide. So we had FOREST 1, our technology demonstrator. Now FOREST 2, which you've just seen. We launched FOREST 3 a couple of weeks ago. And this year, we will launch 16 more payloads to get to a high revisit rate. And that's what's critical with active fire detection. It's because you want to revisit every place on the Earth as often as possible. Because with fire, every minute counts when you detect them, when you want to detect them early. Because if I detect them after a couple of days or weeks, it's too late. And so the temporal coverage is critical. And then also the latency with regards to the downlink of the data. So that's the two things we're optimizing for. High temporal coverage and reducing the latency to as low as possible. We're talking minutes here. And how we're doing that, I'm going to explain later. We're launching our satellites in a batch of 8 satellites in a plane. So they're flying in formation, equally spaced. And that way, they're able to capture every place on Earth twice per day. In the critical afternoon window with our first plane. And then the more planes we launch, the more coverage we get to. Now, what do our satellites see? I mentioned MVIA and LVIA before, but those are quite abstract concepts. So our satellites fly in lower Earth orbit, which is pretty close to Earth, 500 kilometers height. And that's what our satellites can see. I wanted to share an image with you from Ghana, where you can clearly see that the river is depicted as darker. That means colder. And the land surface is brighter. And that means warmer. So our satellites can see temperature, which is more than the human eye can see. As humans, we can see in the optical range, but we can't see thermal. So that's also great when you work with the data, because you can really, yeah, you have superpowers, basically. And see the thermal radiation of Earth that's backscattered. How does a wildfire look like? So here you can see the wildfire in the optical domain. That means you can see the smoke of the wildfire, but you can't see through the smoke. So if the wildfire is still active or not, it's obstructed. And that's why we need the thermal channels. So here in the medium wave infrared, we can clearly see that there are two hotspots still active. Long wave then is more suitable for a land surface temperature derivation, but it can also give indicators on fuel dryness or other relevant indicators also for fire management. Now, I mentioned the electromagnetic spectrum beforehand, and this is how it looks like. So our wildfire detection bands, also suitable for FRP called medium wave infrared, are in the range around 3.8, centered around 3.8 micrometers. Then we have two long wave infrared bands for land surface temperature detection. And as you can see, our satellite is at the bottom, and we co-align our bands with public missions, because we need that for crossover passes to frequently compare if our satellites match what other satellites are seeing, way larger satellites. And that's a way for us to improve our data quality through the band and overpass alignment. Now, you'll hear the term data products more often. That's how we think internally. So on the top, we have our satellite, our sensor that produces data. And our first data product is level one top of atmosphere radiance. And that means we're looking at the top layer of the atmosphere, so cloud tops, but we can't look through the atmosphere yet. So we still have atmospheric distortion in the data. And from this data product, we're deriving two other data products. One is ActiFIRE, which is our main data product. The ActiFIRE product from our satellites will be released this year. And the other data product is level two LST, land surface temperature, which covers not only the anomalies, but the whole temperature signal of the ground. And from there, you can guess it already, we're going to level three to level four. So the higher the processing level, the higher the processing of the data overall and the more additional data sources you pull in. Now, these are the specs for the remote sensing experts beneath you of our level one data product. We provide it in radiances and brightness temperature. Our GSD is 200 meters. As mentioned, sensitivity is at four times four meters. Our radiometric accuracy is supposed to be aiming for eight Kelvin in the ambier and three Kelvin in the albier band. An image is of 400 times 450 kilometer size. And as mentioned, we'll get to twice daily revisit time already this year. And now, to explain how we're actually processing our data on board and the first processing step, I want to invite my colleague Fabian Schöttl, who's a senior software developer with us. And I'll join you later on to explain where we use machine learning on board for ActiFIRE detection. Hi, my name is Fabian. I'm a software engineer at Aurora Tech. And I was mainly involved in the on-orbit fire detection and the L1 processing for FOS2. So, before we go into the details of the processing, let's first have a look at how we actually record the infrared data in orbit. And so, as mentioned previously, we record infrared data in three bands, two long-wave infrared bands and one mid-wave. And when we record a single frame or an image, it's essentially three separate strips of data that we see off the ground. And when our satellite moves now along its path in orbit, we scan across the ground and eventually we have seen every point on ground in all three bands. To get something useful out of the data, we process the data with what we call our L1 pipeline. And in Earth observation, there are these different processing levels that describe how much corrections have been made to the data. And without going into too much detail, you can say that level 0 is your raw sensor data and level 1 is your application-ready data. And in our case, we also have individual sub-levels, so L1a, L1b, L1c. And for now, it's just important that from level 0 to level 1b, we work with per-frame data, meaning that all the operations we do stay in the context of a single frame. But since mentioned previously, we have the three infrared bands distributed across our sensor. So, if we now want to correlate the data from one infrared band of one frame to the data of a different band in a different frame, we somehow need to co-register these frames. And this is what we do in the L1c processor. There we basically resample all the frames we record into one big target grid. And since the frames have all kinds of different distortions, and we have to handle quite a lot of data, that this can be really computationally expensive. And so we knew from the very start, basically, that we have to get this step as efficient as possible. And so we developed a small processing framework that allowed us to chain different processing steps together, independent of whether they were implemented on the CPU or for the GPU, while still giving us enough control to optimize the performance. And QDA was a really important tool for us, because we could port a processing step, for example, from Python to C++, and then just dispatch it on the GPU using QDA. And that workflow was really essential for us, because it meant that we could keep our data science engineers in their comfort zone, which was Python most of the time, and we could easily deploy these processing steps on QDA afterwards. And as you can see, we didn't really bother, or we bothered only with porting the L1c and L2 processing steps to QDA, because that was where we spent most of our processing times. And by being careful with our memory management, we were also able to start processing of the next frame with level 1a and level 1b, while we are still processing the previous frame on the L1c level. And so we had this hybrid CPU-GPU processing pipeline that was, in the end, really efficient. And on the right, you can see how much speedup we gained by just enabling the QDA processing on our Jetsons. And it's really critical to stress how important processing times are in the context of on-orbit fire detections, because we have this real-time requirement, meaning that we cannot take longer for processing than it took for recording the data. So, in a sense, our processing performance limits our recording rate. And what that means is that with better performance, we can record our data with a higher frame rate. That, in turn, means we see each sample on ground more often. And if we now combine all the different observations of the same point on ground together, we can reduce the noise, which allows us to make the fire detection algorithm more sensitive and that, in turn, means we can detect fires earlier or also smaller fires. So, there's this direct correlation between processing performance and detection accuracy. And the NVIDIA Jetsons have been really in this Goldilocks zone for us with regards to processing performance and power consumption. And thanks for having me. For the remainder of the presentation, I'll hand back to Julia. Thank you so much, Fabi. Now, we'll get to how we're using machine learning for active fire detection on board. So, as mentioned earlier, we're really trying to reduce the latency from image acquisition to customer notification. And here you can see a schematic that illustrates the process. So, first, the satellite detects a fire. You have the image acquisition. Then we do the lean processing on board, as just explained by Fabi. Then we run the fire detection algorithm and downlink the data through our own unidirectional ground station network called Firelink. And the user gets the notification in less than 10 minutes. And here you can see the whole chain that gets us from a large image to the kilobyte domain. So, the information the user gets in the end is just location of the fire and time. And that way, we're able to reduce the latency that much. Now, I want to talk a little bit more in detail on the fire detection element. So, as already mentioned, we are flying NVIDIA GPUs, jets and caviar, on board. And that way, we're able to reduce the latency from the usual hour-long latency of the existing missions. You can imagine if you get a fire detection hours after the fire, it's not usable anymore on ground. You can use it for retrospective fire analysis, but not to inform your resources on the ground on how to manage the fire to five minutes. And why are we even investigating machine learning? I brought an example with me. So, here you can see our mid-wave infrared, fire detection, bright, again, very bright spots are the active fire fronts, and then the ground truth of the fire mask. And you can see that with regular thresholding, we are not able to capture the ground truth that well. I mean, there are also pros for, pro arguments for using thresholding. It's super fast. It's not that complicated. And you can get pretty far, actually, with thresholding. But you can become even more accurate with machine learning. And that's our approach. So we're in this study comparing a transformer-based architecture versus CNN models. And we're using a backbone that was provided by NVIDIA via Hugging Face. And it's a mixed-image transformer backbone with 3.7 million parameters, so quite small. And that's important for us because we want to run it on a satellite. So we are restricted when it comes to power budget and also sheer size. So we're not able to run foundation models, for example, on the satellite. We need to have models that have smaller parameter sizes. And we compared it to ResNet-50, MobileNet, and EfficientNet. So classic CNN architectures. So we took the backbone and then we fine-tuned it with the dataset that we collected from 23 to 24. On the right-hand side, you can see a plot where we sampled the dataset from. So you can see it's globally distributed. And on the bottom right, you can also see how a data sample looks like. So on the left, we have the mid-wave infrared image. And then on the right, the label mask. So the active fire detections, which are binary. So fire, non-fire. That's a daytime sequence from Ghana, where we have a lot of active fire detections. And so we used over 40 scenes and manually annotated them. And then we cropped them into smaller pieces. And the class imbalance is, as always with fire detection, quite drastic. So the positive examples were only at 0.04%. So it's a heavily imbalanced problem. And we used some augmentation technique, classics, random cropping, flipping, brightness and contrast. And this is the results we were able to get to. So you can see on the left, the three columns are the convolutional neural networks. And on the right, you can see the transformer architecture that outperformed the convolutional neural networks with an F1 score of 88%. Besides active fire detection, we are also, it's part of our active fire detection, model suit, is cloud detection. And why is cloud detection relevant? Because clouds are always persistent. So we have over 66% global cloud cover in the mean. Then also data downlink is one of the bottlenecks in remote sensing. So you don't want to downlink all the data you have on your satellite. Because sometimes overpasses of ground stations are not available. And if you could cut the data you're downlinking, you become more efficient immediately. So for example, if you know if there was a spot on Earth cloud covered, you can throw it away already on the satellite. And then, which is important in the context of fire detection, clouds can have a very similar signal that's similar to those of fires and lead to false positives even. And then on the right, I brought an example with me from Landsat 7, where you can see in the RGB, we have this thin cloud cover and you often don't see it in thermal infrared. So having a good cloud mask also helps us in having a more accurate fire detection data product. For this, we again collected a data set between 23 and 24. 24 sequences, again distributed all over the globe. And the glass imbalance was 70% clear, 30% cloudy. And in the end, we achieved to get to 528 samples. Again, that's quite a small data set. And so we additionally integrated Landsat 7 data and also trained the model with Landsat 7 data. For this, we used only the long wave infrared data. And here you can see now how the different models perform. So in the first column, you can always see our data sample, the data input. Then the column afterwards, you can see thresholding. And on the right, you can see the deep learning based algorithms. And the first example here in row three is actually a lake that's quite bright with the darker surface surrounding him. So just regular land surface. And you can see that the thresholding algorithms really struggle with recognizing those at land surface. So let's say it's a cloud, which obviously isn't. And here in the fourth row, we can see mountain areas. So cold peaks and snow covers often also confused with clouds. And here we can also see that the deep learning based models on the right really outperform the more naive methods. And for the thin clouds, we also have more false positives with the more basic algorithms. So that's always important for me to only use machine learning or deep learning when the more classical approaches don't work anymore. We always try to use them first because they have other pros. They're more explainable. They're often faster, easier to deploy operationally. But here we can clearly see similar to fire detection. We have a case where machine learning would actually provide added value or deep learning. And here we plotted F1 score versus overall accuracy. So the results. And we can see that the primitive approaches such as always clear, always cloudy or more random classification model, they performed well, way worse than the machine learning and deep learning based approaches. Thresholding is in the middle. And we can also, we really saw that you saw before that our data set was quite small. And through the expansion of the data set with Landsat 7 imagery, we were able to get another performance boost. So these are already the results together with the Landsat 7 training data increase. And we get to a macro F1 of 81% and accuracy of 87%. And this is all still ongoing, whereas the active or ongoing research, whereas the active fire machine learning based detection algorithm is running operationally already on our satellites. And for the cloud detection, similar to the fire detection, one thing we're always optimizing for is a fast inference on board. So we try to get to the order of seconds. And for cloud detection, we already proved that we get to below five seconds in orbit. For active fire detection, it's even less. And we optimized the models via the, we patched them as ON and X and then used TensorRT for further optimization on the GPU. And how this then looks like, so our combined aggregated active fire product then uses our own detections, but also integrates the detections from other satellite sources. And this is how a fire then looks like, like a Palisade fire in LA, which we all remember from the past weeks. And so we're going one step down in the processing levels. We're now going to the aggregated active fire data product that ingests over 25 different sources. So it's one of the most complete active fire data products out there. We're optimizing for low latency. And we also provide contextual metadata with this dataset. So like wind, weather, land cover, fire weather information. There were lightning strikes in the area. So that we're not only saying fire, yes, no, but we give an uncertainty quantification with it so that our users then can act based on the data, leveraging additional information. So that's the case. As mentioned, we aggregate several LEO and GEO satellites. And we also have our own way of direct broadcasting to further decrease the latency. And why is this relevant? Because we can use this dataset for validation. Because if you have more sensors confirming that there is actually a fire, you increase your fire confidence is how we call it. So, for example, the Canadian fire weather index being used or cumulative fire radiative power or geo-persistence. How many geo-satellites do see the same fire? Or how many independent satellites do see the same fire? And that, again, increases our certainty. And of course, we're also using data from official providers for ground truthing. And we always have a direct line of communication between our customers and us to learn more about what causes false positives and how our data is being used in the field operationally. So we're really vertically integrated from sensor to customer and user that act based on our data. And now I want to go one level further, actually to level four, to forecast fire spread. So now you know what is. You know you have a detection. But the information that I think is always worth, really worthwhile to have is knowing what will be. And that's why we developed fire spread, which is a mix of physical and machine learning based methods. So it's a hybrid model. And we were, it was quite a long development journey to get to the right resolution, to model fuel properties accurately, because fuel is one of the most important drivers for the fires. And we also supervise resolving wind forecasts. So we combine topographical information and weather forecasts to get the two main fire drivers right, fuel and wind. We also use our data archive that we've built to forecast short term fire hazard. So one of the predictors that's used in the field is the fire weather index, for example, we were able to outperform the fire weather index by 44 times. And how did we do that? So we're not only collecting fires in the moment. We've also been collecting fires for the past years and built a large data archive over two petabytes. And at the moment that contain fire events all over the globe, but also the conditions that led to the fire events. And that's of course a great basis to train machine learning models. And that's what we did in that case for predicting fire hazard for the next days. And as a summary on what I just explained to you now is we have fire spread modeling. That's hourly predictive model. Then we have daily predictions, which is short term fire hazard. And then we're also investigating seasonal fire hazard prediction, which is a longer time range. So thanks for listening that far. I'm really happy. You also think that's one of the coolest topics to work on. Now I want to switch it a little bit. So first we were talking about fire a lot. And I mentioned land surface temperature a couple of times already, and I brought a few examples with me. So we cannot only derive fires and anomalies. We can also derive the temperature of the land surface. Here we can see an example. So now the color map is changing. The dark blue spots are the colder regions and the red spots and dark red spots are the hotter regions. These are the data product specs we're aiming for. So again, GSD 200 meters, radiometric accuracy of up to three Kelvin. And our delivery speed, our latency we want to get to here is within three hours. So still almost near neural time, and we will get to a revisit of twice daily already in 25. And this data is not existing as of now. So here I brought an example of LA on the right, where you can see an RGB image on the left and then thermal infrared. You can clearly see it's a nighttime image that now the ocean is much warmer and the regions in the north. So the mountain ranges are cooling down, but the city is not cooling down quite as much. So you have urban heat phenomenons that are easily recognizable in our data. And that doesn't only, is not only true for large cities, but I also brought you an example from a smaller city in Brazil, Linares, which is a small city with an extension of like three kilometers roughly. And here that's Linares during daytime. And you can clearly see it's much hotter than the surroundings. And during nighttime, we can interestingly see that also the city doesn't cool down quite as much, not as much as expected. And this is crucial information for land managers, for city planners. But it also affects us as humans and our health to be exposed to high temperatures in cities. And this is now a completely different field of use cases we can also investigate and build upon. And for data validation, we are validating our land surface temperature data against ground stations. So temperature that's being measured on the ground. And one of the most popular networks here is the Suvrat network in the US. And we can see on the right plot that we can actually already get to quite a good agreement between stations on the ground and our calculated LST data. And what's our vision? Where do we want to go from here? So as I mentioned, we're getting to twice daily revisits already this year. But now imagine you know how the land surface temperature is every 30 minutes. It's like a thermal digital twin of the earth, knowing which places are cooler than normal, which places are hotter than normal, which places don't cool down as much as we expect. And this can also provide indicators for irrigation management, for example. And there are much more data products that can be derived from land surface temperature. But what I want you to understand or to having that as a takeaway is this data is not existing as of now. And we are struggling to understand our climate and our planet. And this is a vital information to have if we also want to understand climate change better and if we want to understand the changing dynamics of our globe better. One of the last slides now is also how we use machine learning to increase the resolution of our data. Here you can see agricultural fields in Kansas where we're getting from 200 meter resolution through super resolution algorithms to 30 meter GST. I'm not going to expand on that today because it's really important for me to talk about on-world processing and how we leverage NVIDIA technology, especially GPUs and also pre-trained models for active fire detection. But I'm happy to talk about that topic if you contact us directly. Now we heard about LSD, land surface temperature. Of course, there's also SSD, sea surface temperature. You can see the ocean currents here at Anticosti, surrounding Anticosti island in Canada. And just to spark your imagination, what else, what would you do with thermal data every 30 minutes? Then, to conclude this talk now is our data stream. The data we work with is we ingest over 5 terabytes per day of third-party data, our own remote sensing data. Then, as mentioned already, we have this large database of fire events around the globe. And our customers can access our data in two ways. First of all, via API, if you have your own platform. And then via our own customizable data visualization platform. And that's it for today. Thank you very much for listening. I'm excited to hear your questions and feedback. Thanks a lot. We'll see you next time. Check it out! You are comforted in you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.