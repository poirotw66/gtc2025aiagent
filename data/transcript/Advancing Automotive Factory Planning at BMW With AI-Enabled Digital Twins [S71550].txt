 Music Thank you. Good afternoon, everyone. It's a pleasure to be here at NVIDIA GTC conference and to present our work on advancing automotive factory planning at BMW with AI-enabled digital twins. I'm Harshit Raiker and I'm joined by my colleagues Felix Teuter and Tobias Delago. Hi. Hey there. Together, we will walk you through some of the exciting advancements we have made in this space. To start, let's dive right in and watch a short video that provides insight into the virtual factory at BMW. This should provide some context for the discussion that follows. Let's take a look. Our world is rapidly changing with globalization, sustainability, and digitalization defining our time. To meet evolving market and customer needs, our development cycles become shorter and our system landscapes more complex. But what if we don't just keep up? What if we shape the future and build new digital worlds? Worlds that become reality. With the virtual factory of the BMW Group, we bring together what belongs together. In this process, we unify our existing data streams and create an exact virtual 3D image of the production system. This serves as an entry point for global teams to validate and discuss their planning, which means collaboration in real time from anywhere in the world. Planning alternatives, space requirements, and danger zones can quickly be visualized and reviewed in the overall context of the virtual factory of the BMW Group. With this, we contribute to a more sustainable and efficient planning process. Dive into the virtual factory of the BMW Group. Collaborative, efficient, innovative. Thank you, Harshit, for the introduction. In the video, you saw what the virtual factory means to us at BMW. Now, I'm giving an overview about how we started with the virtual factory and how our partnership with NVIDIA evolved over the years. In 2018, our colleague Ross Grumburger began using Unreal Engine for factory planning in Oxford. We still use Unreal Engine for specific use cases today. Two years later, in 2020, the virtual factory Group was established. In the same year, we took our first steps into Omniverse. At the GTC, four years ago, in 2021, NVIDIA and BMW announced a strategic partnership to jointly develop a digital twin of our production system in Omniverse. In 2022, we had our plant in Hungary fully represented in Omniverse for the first time. To bring more users into the Omniverse, we transitioned from on-premise hardware to Azure Cloud in 2023. To further develop and maintain the system, we significantly increased our external development capacity through long-term DevOps partnerships in 2024. Finally, this year, we will continue with the development on NVIDIA's cloud infrastructure. For the virtual factory, we see significant economic benefits through scaling the system. By increasing the number of users and the data available in the virtual factory, we gain network effects. As I mentioned in the previous slide, we started with the plant in Depritsen, Hungary. From there, we rolled out more and more plants into the virtual environment and applied specific use cases at each location. We are working on use cases such as deriving AGV maps from our data, running semi-automatic collision checks, and using the virtual environment for human simulations to evaluate ergonomics. Those are just a few use cases among many others. Now, in 2025, we are moving forward with scaling the previously implemented use cases by integrating them into our business processes. We utilize Omniverse for a variety of applications, including the creation of massive 3D models, training robots, and enhancing our planning processes. One of the key features we value about this software is the open-use data format, which offers high customizability to meet our specific demands. So why did we decide on building up our virtual factory based on Omniverse? We were looking for a software and 3D data format, which can cope with the complexity of our use cases. Our users, the factory planners, play a critical role in ensuring the production of our products, the vehicles. They are dedicated to continuously monitoring, optimizing, and deploying changes to our production facilities and processes. With 600 planners, each making an average of 3 changes per week across 33 factories, managing this vast amount of data and collaborative effort presents a significant challenge. So what do we achieve with Omniverse? Through fully virtual factory planning and optimization, we leverage Omniverse and OpenUSD to create massive 3D models that visualize factories up to 7 million square feet and over a mile long. The multi-user collaboration capabilities allow planners connect in a global network, enabling real-time collaboration and faster optimization. By creating a cloud-connected network of planners, each BMW factory becomes a node in our shared global network of Omniverse interconnected intelligence. And this all is accessible through a web browser. This connectivity allows planners from around the world to quickly collaborate while working alongside AI guides to design and optimize every stage of production. This enhanced connectivity and collaboration lead to quicker feedback loops and improved iterations, ultimately driving our production efficiency. Now I'm handing over to Harshit, who will elaborate on what we use large language models for in combination with OpenUSD and Omniverse. Thank you, Felix, for providing insight into evolution of virtual factory at BMW. Our virtual factory implementation at BMW centers around two key value propositions. First, the holistic file type. This is based on Universal's scene description with an underlying data integration pipeline, which allows us to create a comprehensive digital representation of physical facilities. Second is customizability. We are leveraging the customizability of the platform through a suite of OmniverseKit extensions, specifically designed and implemented for BMW's unique needs. While these innovations bring many advantages, they also introduce undesired side effects. These challenges that we are tackling using LLM-based Intelligent Assistant. Let's take a look at the first challenge. Let me walk you through our data integration pipeline architecture. At BMW, we deal with various source applications that are integrated to different planning processes, such as structure planning, process planning, logistic planning, human simulation, and so on. Each of these applications use its own 3D data formats, such as Revit, JT, DAN, DGN, and so on. Our data integration pipeline handles several critical tasks, establishing connectors to source systems, converting these diverse formats to unified QSD formats, managing localization, implementing rows and writes for proper access control, performing verification and validation of the converted data, and lastly, conducting thorough quality checks. The result is a series of QSD files representing both static and dynamic virtual factories, which has surface area spanning more than 1 million square meters. To give a perspective, this is equivalent to 140 football fields. However, this immense scale introduces a significant challenge. However, how would users effectively navigate in such an enormous virtual environment? I will now hand over to my colleagues, Felix and Tobias. Could you kindly please demonstrate how we are addressing this navigation challenge with the help of AI? Hey, Tobias. So, I have now the tool open. Can you quickly walk us through what we see here? Yeah, sure, Felix. So, this is our standard Omniverse interface, where one factory, more specifically the new plant in Debrison, has already been preloaded. And the intelligent assistant is now directly integrated in this frontend, right? Yeah, exactly. So, the IA is nothing different than all the other extensions the Omniverse is built of. In this case, we can access it from the top right corner. We can see some general welcome information as well as some sample questions. The user query then goes directly in that little bottom field. Okay, great. Then let's have a look on one specific pain point Harset has introduced before. The navigation and how the intelligent assistant could support here. I would like to look for where the doors are going to be assembled. Exactly. So, just type it in there. Okay, I asked a question. Okay, so now I got two responses, both in German, as the POIs were created in German. we have the T端ren Einbau and the Vormontage T端ren but I'm more interested in the T端ren Einbau so I assume I can directly jump there exactly just click on the jump to button and is this actually the station you have been looking for Felix? yeah absolutely this is what I was looking for okay perfect so this was actually a pretty simple example so we just did that translation as you could see from doors to T端ren but now feel free to try something a little bit more difficult okay let's see if we can find where the chassis is assembled to the body of the car okay let's see how that goes okay perfect yeah it identified the Hochzeit as the as the point of interest this is exactly where the chassis is being assembled to the body of the car and yeah quite quite interesting that this works so well considering the LLM only has the information Hochzeit so yeah this information seems so the LLM somehow seems to get this corresponding POI when I look for body and chassis okay that's exactly how it works so let's jump back to the slide so we can dive deeper into that right so as we saw what we actually did there is to really start from the pain points and needs of our users the factory planners in this case to navigate those huge factories and the intelligent assistance simply provides an additional approach to solve this complex task by using the semantic capabilities of an LLM we see two key benefits on the one hand we can use this semantic capabilities to detect the actual user intention behind the questions as for example has been seen with the body and chassis example to then find the marriage on the other hand the intelligent assistance also helps us to compensate for data inconsistencies of course in an ideal world there would be no such thing that we would have perfectly labeled data multilingual support and so on but we are far from that in reality and the intelligent assistant helps us to mitigate those issues by compensating for example spelling mistakes multilingual support and so on so having a look at a very very simplified architecture it all starts with the user inserting their input prompt this one gets then passed to the intelligent assistant which is based on a python backbone we also implemented a sophisticated guardrail to comply with our company internal regulations we use a hybrid approach for this step which combines the LLM but also a filtering mechanism this helps us to increase the meaningfulness of the results increase accuracy but also to reduce token costs for example when we are in a specific building and have not loaded the entire plant we only want the intelligent assistant to search within that building when the question is elaborated it goes back and forth to the server where the LLM is hosted while the 3D data is directly pulled from the nucleus once an answer has been found it will be provided back to the user via the output mask as we saw previously the jump to function is not done automatically but via button we have done is this way so that the user has full control where to jump to in the factory because sometimes more than one result is possible when clicking on the jump to function in the 3D viewport then the relevant geometries get highlighted and the camera hovers to the correct spot while the initial feedback from our users has been very positive already what we originally wanted to do is to simply see if the intelligent assistant can be seamlessly integrated in the existing technology stack paving the way for future use cases which then leads me back to harsh it so let's see what other use cases we have here today thanks thank you for that demo now let's address the second value proposition which i mentioned earlier the customizability which is achieved through our suite of kit extensions built within the omniverse platform these kit extensions are developed based on the nvidia sdks kit framework kernels and kit foundation extensions which include rendering ui and asset management capabilities essentially everything in our implementation is an extension requirements from our internal customers have led us to develop numerous extensions several dozen features in total including live session capabilities point cloud visualization factory filter and many others this extensive customization however creates another challenge we have essentially built tools that require specialized knowledge how do we ensure that the every everyday users can effectively utilize these sophisticated tools let's look at another brief demonstration how we are mitigating this usability challenge using ai felix and tobias over to you okay felix we are back in our omniverse environment and i see you have already initialized a new conversation with intelligent assistant yes we are here at the tilt where certain manual processes take place i would now like to showcase how the intelligent assistant can help non-expert users to use our tools sure so again just type your question in the input mask in theory there's no need to know exactly what you're looking for but the more specific you are the higher chances that it will return what you actually needed okay great so on one of our projects and i see you have already initialized a new conversation with intelligent assistant um is is is is is of the factory. Let's see what it responds. It will take some time to analyze your request, but eventually if there exists such a feature, it will let you know in the output mask. Perfect. It started the keyboard cut feature and with this feature I can specify or define an area of the scene and cut it out as a separate use default. As you can see, there is also now the possibility to get into some additional Q&A with Intelligent Assistant if you need some further support while using the suggested feature. Perfect. Let's get back to our slides. The Intelligent Assistant serves as an additional method to make our historically expert tools more readily accessible. The key advantage we see here is to understand what functions the user actually needs. So we are slowly moving away from telling the software what to do, but rather telling what outcome we want. This not only helps us to increase acceptance of our tools, but also to significantly increase productivity and efficiency. As shown here, the very simplified architecture remains very similar to the previous use case. The user again uses the frontend to insert the questions. Then we use a hybrid approach, this time however, in combination with RAC, or Retrieval Augmented Generation. Add methodology, frequently used nowadays in LLMs, to input outside information and facts. However, for this system to work, of course, the knowledge repository needs to be large enough so that the RAC approach can be fit with sufficient data. You can imagine that there has been a lot of groundwork done over the past few years to now reap the rewards. Finally, as depicted here, the Intelligent Assistant now directly interacts with the 3D viewport. For example, as we saw, by displaying the 3D QBitCat function directly. So, to conclude, building on the previous use cases, we wanted to show that we can have this tight integration with our actual Omniverse functionalities. And we are also able to integrate outside knowledge repositories. Thank you, Felix and Tobias, for those interesting demos. We hope we have provided valuable insights into how AI is enabling and enhancing our digital twin implementation at BMW. By addressing the navigation complexities of large-scale virtual environments and simplifying the user experience of our expert-level tools, AI is proving to be an essential component of our digital transformation journey. These features, what you saw, are currently being rolled out to production, with many more under development. With this, we are at the end of our presentation. We now would like to extend our gratitude to our partners, Nmedia, T-Systems, and CDW, whose support and collaboration have been instrumental in the success of this project. Also, a special thank to our colleague, Mara Geisler, from the communication team. We are now open for questions. Thank you very much for your attention. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.