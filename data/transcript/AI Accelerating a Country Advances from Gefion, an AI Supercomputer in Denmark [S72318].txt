 Thanks everybody for coming to this session. So we are at GTC and GTC is all about AI acceleration. We've probably all heard a lot of sessions about AI to accelerate different tasks, AI to accelerate workflows, AI to accelerate even entire industries whether it's pharma or industrial automation. What I'm going to be talking to you about is AI to accelerate an entire country which is quite a task, right? Quite a session. So my name is Nida Carlsten. I am the CEO for the Danish Center for AI Innovation and today I'm going to talk to you about Gifion, our AI supercomputer that is pretty much brand new at least by our timelines, maybe not by AI timelines, but it just launched in October and I'm gonna tell you the story about why we decided to build it, how that journey has been and more specifically also how our customers are using it because that is really important. So how did this journey start? Like many things with AI it started at GTC and it started only last year. There was a delegation here you can see on the right side from Denmark that came to GTC specifically to make this big announcement which was that NVIDIA had partnered with the Novo Nordisk foundation and they were going to team up to build this big AI computer, something that was bigger than has had ever been built in Denmark. And they had this big vision that was a year ago but there was still no GPUs or you know let alone a computer. It was just a very ambitious vision and agreements to work together on that. And this is how it's going. So this was at our launch in October and we had the pleasure of having Jensen come to our launch in Copenhagen and in case you don't recognize the other guy, that's actually the king of Denmark. And that just goes and the other person of course is me, hopefully you recognize that. But it just goes to show you the importance of this project. For Denmark it's a huge thing but also for the whole AI community it's been a really, really big thing and we're so grateful that it's also been a really important thing for NVIDIA and that they've been so supportive of this huge project. So what we've done is built an AI factory and we've done it in record time. And so that's the story I want to tell you. We can basically divide that story into four parts. Starting with the planning phase and also including establishing the Danish Center for AI Innovation. Which was really the key part because somebody needs to own and run this computer and that's us. I'm also going to talk about the building phase. That's the fun part, assembling the computer and making sure that it's working. And then the even more fun part which is getting users on the computer. And the beauty about that is once you get customers on any product really, but especially your computer, they start breaking things and then you have to rebuild again. So that's been a really fun journey, iterating with a lot of our customers. And then I'm also going to touch on the impact that we are already making just a year into this whole project and that we hope to continue making. So let's start with establishing the Danish Center for AI Innovation. A lot of you may be asking, why Denmark? Just out of curiosity, is anybody here from Denmark or related to Denmark? Oh wow. Okay, a lot of Danes. Wow, that's amazing. So for those of you who are not from Denmark, you may not realize that there's a lot of amazing things going on in Denmark, especially in the tech and digital space. It is ranked in the top 10 of the most innovative countries. It's number four in digital competitiveness. And generally, I would say there's a huge culture around tech adoption and just making sure that there's acceleration by adopting the latest technology. Another really interesting thing about Denmark, which is very relevant for AI. AI is all about data and the quality of the data and the uniqueness of the data. And Denmark is fairly unique in that it has a really big collection of very rich and complex data on its citizens, especially when it comes to health data that can be very useful because you can get a lot of insights that you wouldn't otherwise have. So there's a lot of potential to do AI on that data and that gets something very impactful. The other nice thing is it's a very interconnected AI innovation ecosystem. It's also very central in Europe and there's good relationships with other European countries. So there's a lot of really, really good elements. And there was this vision that there really needed to be accelerated computing on a national scale to do something really cool with all those elements. One thing to mention and that there's already even before we existed, there was quite a strong deep tech ecosystem in Denmark. There's a lot of quantum. There's a rich history of quantum in Denmark and there's a lot of research centers going on there. Life sciences, that might be obvious. But there's also AI and data science initiatives going on. And this is just research projects and some very large research projects. But I should mention there's also a lot of big companies in some of these spaces as well. You probably know Novo Nordisk, Lundbeck, Leo Pharmaceutical, all Danish companies. So the need for an AI factory came down to four recurring themes that a lot of innovators and researchers kept bringing up. The first one was that there wasn't enough advanced computing hardware. It was just hard to find GPUs and it was hard to find good quality GPUs. And it was especially hard to find them in very large numbers. There was a second issue of access. Access to GPUs is difficult. And what I mean by difficult is that there's an issue of price for anybody that is not a large company that can afford to pay premium prices. So academics and startups were getting left out. And there was also the desire to be able to access things in a timely manner. And for people doing research that was really pushing the envelope and using the latest in AI, having to wait several weeks to access other national resources and European resources was just taking too long. Another thing that we're trying to solve for is the support needed. So it's not easy to get started on large scale AI computing and do parallel processing. And there's a lot of user support that's needed beyond just access to the GPUs. And finally, the data sets that I mentioned. There's a lot of potential there. But there's also a lot of requirements around sovereignty of that data. So a lot of that data cannot leave Denmark. And so in order to get the analytics and the insights that you want out of it, you have to do all of that compute in the country. So like any company, one of the first hurdles was finding funding to fund this. In our case in Denmark, there's this system of enterprise foundations that are quite common. And we're lucky to have as our investor one of the biggest foundations, which is the Novo Nordisk Foundation, which dates all the way back to 1924. And you can think about it. It's very similar to the Bill Gates Foundation, for example, the Wellcome Trust. It has a philanthropic mission to improve people's health and also the sustainability of the planet. They fund a lot of research and innovation. So this is right in their wheelhouse. And actually a lot of the projects that they are funding could benefit from AI. So this made a lot of sense for them. It aligned very nicely with their mission. But it's also a very big investment for Novo Nordisk to be doing. We're also funded by the Export and Investment Fund of Denmark, which typically invest in startups and funds that invest in startups. But they also make strategic investments when they think it's going to raise up the entire ecosystem. And we're one of those strategic investments for AIFO. So all combined, the initial investment was about 100 million euros. That was just the startup capital to get the initial hardware and get going. One of the things that makes us unique and is especially relevant because of the funding structure that we have is we are set up so that we can serve the entire ecosystem. We are able to work with academics. We're also able to work with businesses and that startups as well as large companies. So we are a commercial model. Everybody can use our computing capabilities and they all pay a fee. But then we have different mechanisms to make sure that people can afford to pay if they have that need. We also work with the community at large, which is also really important to be able to work with public sector institutions, for example. And even individuals in some cases. And if they're not otherwise represented, then they need to access the compute. So our value proposition is that we are one stop shop for advanced computing, especially advanced computing that is going to accelerate research, where AI can make a positive impact on different research areas. So we offer the cutting edge compute. This is something that we are now well known for, and I'll talk a little bit more about that. But also our model is to remove any barriers to access that compute. So we want researchers and innovators to be able to access GPUs in hours and not weeks and not months. Usually the longest is actually just for lawyers to agree to sign the agreement. But otherwise it's really quick. We can turn on access in minutes. Another reason customers come to us is because of the security and compliance. So because we knew from the beginning that we were going to have some use cases where data sovereignty was very important, as well as commercial customers that have very high security requirements, we took it upon ourselves to have a very strong roadmap around security and data compliance. And that's another differentiator for us. And that's another differentiator for us and another reason people come to us. And of course we have the expertise and that's the expertise in how to program these GPUs, everything related to the infrastructure, but also some domain expertise and how to do drug discovery use cases on a large cluster or how to do quantum simulations on a large cluster. And that comes from us, the DCAI team, but also our partners, including NVIDIA and other experts that we can tap into the community. So the second piece was actually building the computer. And that started in earnest in the summer. Gipheon is a large scale DGX SuperPod. So hopefully after this keynote, you're all very familiar with what a DGX SuperPod is. We are still based on the H100 GPUs. We have 191 DGX systems. There's eight GPUs per DGX systems, which is how we get that final number. We picked the architecture very deliberately because it's a reference architecture for NVIDIA and we knew we wanted to future-proof the investment. And so with this, we can scale, we can upgrade fairly easily, and that's the intent. So Gipheon is the 21st fastest supercomputer in the top 500 list. One of the first things that we did after the launch in October was to get it benchmarked in November. And we're very pleased that it ranked so high on that list. It took a lot of optimization of the GPUs to get there, but we're very proud of that performance. Which is 66.6 petaflops per second. We're also very proud of the fact that it's the seventh fastest storage system on the IO500 list. And this is thanks to our partners at DDN. We're very proud of that because we are an AI-optimized system. And so for us that meant that we shouldn't focus on just the compute, but offer everything that is going to enable large AI workloads. And that was a key part of it. So we have the premium hardware from NVIDIA, but we also have specialized software from NVIDIA. And that is also something that our customers have really been appreciating, having the entire software stack, as well as some of the specialized frameworks and software, including CUDA for Quantum, but also things like BioNemo for drug discovery. So after this morning's keynote, you may think, well, maybe Gipheon is a small potato as compared to some of the numbers that Jensen was throwing around. But one thing to keep in mind is what we're trying to do here for Denmark. So if you take all of the available compute, so all of the initiatives that had gone into funding large scale compute, we get to about 4,300 teraflops. And that is already a fairly big number. But as you can see, that looks fairly small compared to what we bring with Gipheon, which was a 15x in the total computing power available in the entire country for researchers. So huge increase and also explains why we've been getting so much attention. Another thing I want to mention around AI optimization is the networking in our GPUs is also something worth mentioning. We are using the OctoRail InfiniBand for our GPUs. And that means that we have very fast connectivity between GPUs, which is really important. That had also been a roadblock when we talked to some researchers about the types of workloads that they were doing, that sometimes it just was taking forever because of connectivity issues. So that is also something that we optimized. So the fun part. We started onboarding customers. And from the beginning, we had this notion that we wanted the platform to be very versatile. We did not want to be a computer just for life sciences or a computer just for quantum research. We really wanted to enable multiple types of workloads and multiple types of customers. So we very deliberately built the computer in a way that was application agnostic, which also meant more work for us. Because we had to make sure that we were compatible with a wide variety of use cases. But these are some areas that we heard that users had a lot of interest in running on GIFION. So we had a lot of interest in natural language processing, which is probably not that surprising. When people hear large-scale super pods, they think LLMs and they think training large models. So that one made a lot of sense. But we also had some very interesting conversations around doing analysis and training on different types of data. And video and audio came up quite a bit as areas where there hadn't been as many models or the models were not as good. And there was a gap in doing something interesting. Biomedical research also comes up a lot. Anything from protein structure prediction to doing data processing on biomedical data. And simulation is also something that a lot more people are interested in doing than I actually had assumed. Originally when we started, we really thought it was going to be a lot of training. But simulation is also a big need. So when we launched, we really wanted to put the data on the data. We wanted to put the computer in the hands of customers as soon as possible and see what they could do with it. So we made this open call to the community and asked them for big ideas about what interesting things they could run on GIFION. We did not specify any areas that they should focus on. We did not specify a certain number of GPUs that they had to use or anything like that. We really wanted them to be creative. But what we did do was we deliberately picked different types of customers. So we picked two start-ups, Teton and Go Autonomous. And we picked DMI, which is a public sector organisation. And then we picked some academics from different universities. So on the research side, it was interesting to get some really interesting projects in a lot of different areas. And one of the ones we selected from DTU was around looking at new material development and new ways of doing catalysis that could help with the green transition, that could help do chemical reactions in a more sustainable way. So we thought that was really interesting and had a high level, high potential for impact. So we selected that. But we also had from Copenhagen University an interesting project looking to do multimodal genomic data. So combining data from different sources, DNA, RNA and so on, and creating a new foundation model. And that could also be really, really impactful in terms of research. So we were seeing very creative and potentially big impact projects coming from the academic institutions. And they had never been able to do this type of training without the type of compute that we're bringing to the table. So we're also enabling impactful innovation in addition to impactful research. And that was exemplified by the two startups that we worked with during this pilot phase. So Go Autonomous, for example, had this big vision of getting rid of repetitive tasks. So anything that is a repetitive human task basically should not be done by humans. It is well suited to be done by an AI so that the human can focus on doing something more interesting. And one of the first things that they decided to look at is being able to identify information and transfer that information back and forth into multiple platforms. So automating a process that has a lot of applications. Also interesting to a lot of companies looking to do business transformation and automation of business processes. And they've been on the platform since late December and getting quite good results. On the other side of the spectrum, less about business and more into healthcare, Teton is also doing something really interesting where what their roadblock was, was that they wanted to train on a large amount of video data, which is challenging to do for a lot of different reasons. And they have been able to do that on Gipheon. The reason that they are doing that is because they're trying to have a system where they can identify a patient that's about to have a fall. And that's really hard to predict. And it can be quite costly to just have a human there watching a patient. Also a little strange, right? Just having somebody waiting for somebody to fall and to catch them. So this is a really nice way to use AI in that healthcare setting, which just has a huge benefit to the patient as long as it's done well. And they've also been working with us since end of December, early January. So we've had some really good successes with these pilots. And now of course we want to open the door to even more customers and broaden the impact that Gipheon can do. One of the big things for us is going to be Gipheon's impact on the life sciences industry in Denmark. And that is already a big industry in Denmark. We have all of these big companies that I mentioned earlier. And the life sciences sector is a huge part of the Danish economy. So what we think we're going to see on Gipheon based on some of early customers includes of course this notion of having a health data cluster where we can use the fact that Gipheon is a sovereign system in Denmark, highly secure. And we're hoping that customers with very sensitive data and very high compliance requirements will come on Gipheon and run these workloads on us and be able to for the first time get insights that they couldn't before because they couldn't do that compute before. So that is the holy grail for us. But we're also seeing really impactful research and impactful use cases even without the health sensitive data. There's a lot of interest in assisted drug discovery. So using AI to assist in the drug discovery process both to make things more efficient also compress the timelines of being able to go from a molecule to a drug that can be used in a patient. So you can use AI really at all the different parts of that process and we're seeing interest at all the steps of that process. Computational medicine is also something that's really interesting being able to develop new models based on genomic data and other types of data and going all the way to personalized medicine and having improved treatments based on genetic profiles or predictions of certain diseases predictions. So a very big area for us. Another really big area for us is quantum. And here I'm going to pause because I've been talking to you about an AI supercomputer. So you may be thinking, why are you talking to us about quantum? The two disciplines are different, but they do overlap in quite a few ways. And one of the ways that they overlap is in the types of things that they can impact. So both quantum and AI have a lot of potential for things like chemistry and physics and the development of new algorithms in those areas. So one of the things that could be done with an AI supercomputer like Giffion is actually using the computer to simulate quantum systems. So there's a lot that can be done before quantum computers, quantum hardware, actually get to the maturity level that is needed to develop quantum algorithms. So in some ways we're bridging the gap. So that's really interesting. And there's also potential for AI to help control and optimize the way that quantum computers run. And that's also a very interesting area is where we're constantly talking to quantum computing companies and figuring out how could they use AI to manage their systems better or do better quantum error correction, for example. So to give you a very specific example on this, we've been working with a team from the University of Copenhagen along with NQCP, the Niels Bohr Institute and many others. What they've been looking at is can they put together a system that simulates quantum so well that they can test their quantum algorithms on it. And they had previously been doing this on CPUs and even using very large scale CPUs and still being blocked by the amount of compute that they needed. So one of the issues when you're trying to simulate a quantum computer is the number of resources increase exponentially. So you very quickly, if you're trying to simulate more qubits, you very quickly get to the limit of the number of CPUs you can reasonably buy or have it at your disposal. With AI, that's not the case. The curve doesn't go up that high. So they've been using an AI system for the first time. They've been using Gipheon. And what they were able to show was that they saw 100 times speed up using Gipheon over the CPU cluster, which was already very large, that they were using previously. The data at the bottom is actually brand new. And it shows that they're able to simulate a much higher number of qubits than before, which is very impactful for their research because what they're trying to do is predict how quantum algorithms will work. So if they're able to simulate it on a higher number of qubits, then it more closely looks like what actually will happen when they're able to test it on a quantum computer. So they've been doing this for the last few weeks and are continuing to tweak these results and get even better ones. One thing I should mention is they did all of this and had this huge improvement and they're not even using the full size of the computer. They're using only about a third of the number of GPUs that we have because we were throttling all of the pilot customers. So at some point, we may just remove that cap and let them take over the whole computer and see what they can do. Another really interesting use case in terms of the types of impact that can happen when you have a large number of GPUs when you only previously had CPUs is DMI, the Danish Meteorological Institute. And their challenge was trying to see if they could develop an AI-based weather model that could make predictions just as good as traditional models. So traditional models are physics-based and they are fairly accurate, but they take a lot of computing power to run and that means you can't run them as often. And DMI had this vision of being able to use machine learning and use AI to accomplish their mission, which was to be able to predict the weather in such a way that they could make sure that the society benefits and that they're able to prevent large-scale events where weather can be very dangerous for a large segment of the population. So one really cool thing is I have the privilege of actually announcing the fact that DMI has just signed a long-term agreement with us following the results of this pilot. So basically during the pilot phase, which only lasted a couple months, they were already able to find that the new model that they developed was almost as accurate as the traditional model that they had been using for the last 20 years. But now the model is much more efficient, which means they can run it faster and run it a lot more often, which means more accurate predictions. They were able to show this for both temperature and wind, and they were very happy with the results. And so now we are very pleased that we get to work with them in the future. And they get to expand this model to now more variables like cloud cover and other things. And they get to keep training the model to make it even more accurate. So that's a really great piece of news and one of our first really big customers, and we're really proud about it. A couple of people have asked me if the reason we picked this as our first customer to announce was because the weather is such a big thing in Denmark. I'd like to say that we planned it that well, but that is not the case. It just really worked out very nicely that they had this great team that had been working in trying to identify ways to use AI, and we were able to provide that wonderful compute. And we actually have DMI here at GTC, so if you want to learn more about this model and what they've been able to do, you should stick around after the session. So I hope that gives you a sense of what customers have been able to do so far with a super pod available in Denmark. We're only at the beginning of the story, right? So I told you the first four chapters. Hopefully there are many more. We continue to scale, and we're continuing to find different ways that we can accelerate the future. So if you have any questions or if you're interested, of course, feel free to contact us at that email address. And I think we have time for a Q&A. I'm looking at Nick. Yeah. Thank you. Thank you. And we do have time for some questions. So if anybody has a question, put your hand up and I'll bring you the mic. Thank you. This was awesome. I'm Daniel. How are you finding, you're the seventh largest storage provider as well, right? Seventh fastest storage, yep. Fastest. Got it. How are you finding the EU AI Act going to be affecting you going forward? I don't think it affects us so much as it may affect our customers. And so that's always something to consider. Generally with these policy related issues, it's the uncertainty that affects the rate of adoption. And especially with large enterprises, that could be the difference between they make a huge investment in AI now versus they wait three months until their policy people say, you know, it's okay. So we've seen a little bit of hesitation from the very large companies. But I would say from the most part, so far we haven't seen an impact because there was such a pent up willingness and desire to be doing these large workloads. And the roadblock had been the availability of the compute. Thanks. We have another question over here. Thank you. Yes, I'm Edwin. And my question is, in as much as we're GTC this morning, Jensen was speaking about speed up and speed out. I realize it's early days for you all, but how do you perceive that trajectory? And maybe you can speak about perhaps the upgrade cycle as well as it impacts you all. Thank you. Yeah. I mean, one of the good things about GTC is you get to see all the cool stuff that's coming out. And one of the bad things about GTC is you get to see all the cool stuff that's coming out and you feel like you're already old when you're not even one year old. But I think it's a great thing. I think for us, with having multiple types of users and very different types of workloads, there are customers that will be happy using the H100 for a long time. And it's a very good GPU. And for a lot of people, it is the best that they've ever imagined that they could possibly get their hands on. And for some customers, it makes sense to have the latest. So, I think the nice thing about the architectural decisions we've made in the beginning is we have that ability. So, I would definitely love to get my hands on the new generation. And I think we'll do that as soon as it's practical. And for us, it's based on customer demand. So, if we have more than one customer saying, I want the latest chip, then it's easy for us to just add it and make it available. And I think we'll definitely do that very soon. Thank you. Awesome. Thank you. Any more questions? Hello. A couple of questions. So, you mentioned before NATO is in Denmark. Is this because it's in Denmark or is this our relationship with using the computing resources? NATO, that could have been the accelerator. That's part of our deep tech accelerator. There's a Diana accelerator that's funded by NATO. I think if that's the slide I'm thinking you're referring to. Yeah, that's one. Thank you. How much time did you spend to deploy the whole thing? Only a few months for the actual building of the system. Because there was quite a bit of planning in the first few months. And then the computer itself got assembled relatively quickly and shortly before our launch in October. So, everything happened really fast. Super fast. We're doing not just AI acceleration but timeline acceleration as well. But it took all of the partners coming together to make that happen. NVIDIA of course was amazing in that process. We had Eviden also delivered the computer. And so, everybody worked to make that timeline happen. But it is, yeah, it's not easy but it was worth making it happen. But there's a, you know, to Edwin's question earlier, it's not a static thing, right? You're always upgrading, you're always adding things. So, I think the real answer to how long does it take to build the computer is you're never actually done. You're always tinkering. Awesome. And any more questions? Yes. So, you have built it in half a year. That's really impressive. And the question is, is it, it was 60, 100 or 66 petaflops, is it fully occupied right now? So, it is close to but not. And we did that deliberately because we haven't fully launched as an open service yet. We're saving some spare capacity. But based on the demand that we have, we would fill it up if we removed all caps. Do you think about opening up for the broader Europe? Sorry? Do you think about opening up for the broader Europe customer? I'm actually really glad that you asked that because I should have mentioned that. Despite the fact that we have Danish in the name and of course part of our mission is to lift up the Danish ecosystem, we are actually open to users outside of Denmark. And we think some of the best way to positively impact the Danish ecosystem is to allow customers and users from Europe and elsewhere. And of course the access to the computer is completely remote. So, users can come from everywhere. And we definitely encourage European researchers and startups to reach out and get time on Gifion. I think we've got time for one more if there's another question. Yes. My question is more of a curiosity. Have you got any requests to have an LLM trained in Danish language with high quality data? Because we are seeing these trends in other countries. Only about twice a day do I get approached about a Danish LLM. There's actually some valid reasons to do it. I mean, I'm obviously not Danish and I've been living in Denmark since August since I took this job. And I can vouch for the fact that Danish is a very tough language and a very unusual language. And there are a lot of areas where it makes sense to have something that's actually trained on the Danish language or at least the Germanic and Nordic languages. There are a couple of groups that are thinking about how to best do that. And of course we can and will support them as soon as they are ready. The type of infrastructure that we have is very well suited to do that type of work. So, thank you. Thank you. Okay, great. Well, please let's give one more huge warm welcome to Nadia for talking to us today. Thank you. Thank you. Thank you.