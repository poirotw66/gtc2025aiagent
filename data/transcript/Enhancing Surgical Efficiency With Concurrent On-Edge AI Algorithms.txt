 Hello everyone, my name is Pieter de Bakker. I'm an engineer who subsequently trained as a robotic surgeon and I'm responsible for Orsi InnoTech. Orsi InnoTech is a part of Orsi Academy which is Europe's largest robotic surgery training center and at InnoTech we focus on using AI to leverage surgical skills, surgical training and progress the field of surgery as a whole. And today I'll be talking to you on how we enhance surgical efficiency with concurrent on-edge AI algorithms and show you some of the work we've been doing in collaboration with NVIDIA. So to start off, what is surgical efficiency? Well there's a lot of parameters that we can use to parameterize surgical efficiency. We can look at the time a surgery takes, the time it takes for a patient to be discharged, the outcomes of the patients. We can think about things such as a workflow efficiency. We can think about quality markers for both the hospital as well as for patient satisfaction. We can think about efficiency as a matter of cost efficiency. So there is a whole myriad of options when thinking about surgical efficiency. But if we take one step back, there is one crucial parameter that really impacts all of these different parameters and that is really surgical training. So looking into surgical training and we can impact all of these aspects and I'll quickly show you why and how. So today residents are trained to be competent which means that they can troubleshoot their own problems but they cannot self-correct. And what we teach at Orsi Academy is to get to the level of proficiency where you understand the larger context and you can self-correct which is crucial as a surgeon because if you run into a problem you should also be able to correct. Proficiency does not mean that you're an expert. To become an expert you really need to get more cases and really even further increase your exposure. And so proficiency is really the holy grail in surgical training. But how can you really summarize proficiency? Well basically it's about preventing errors which impact outcomes. So proficiency assessment happens in a binary way. You either make errors or you don't make errors and if you don't make certain errors in a specific amount of time and you can rest assured that you'll end up in the right position or the with the right outcome for the patient. And we've shown in several prospective studies and a meta-analysis that this entails a 60% error reduction when comparing people who are trained according to proficiency-based progression when compared to classical training such as see one, do one, teach one. Of course this methodology is very rigid, it's very coarse but as it impacts outcomes it's really about the patient. It's a very standardized workflow in which you have several steps that are being used. If you reduce 60% of the errors this is a huge reduction in cost for both the hospital system as well as for the society because all these surgical complications are costful knowing that yearly several billions are spent in the US on complication management. And of course if you're very proficient in what you're doing you will probably will also not linger too much on your surgery. So a small word about robotic surgery as a start. So in robotic surgery a surgeon sits at the console and then you have the bedside assistant who handles some instruments towards the surgeon. The abdomen is inflated and the surgeon really works with a magnified 3D view from a distance and operates the patient with a also in a tele-manipulated fashion connected through wires so not through wireless or 5G which is the classical definition of really tele-surgery. So if you look at how this looks like you see these small instruments, needles, wires and this is an example of our training methodology at Orsi Academy where we try to suture two tubular structures to each other which is the basic essence of robotic skills is suturing two structures and knotting them. And one of the errors that can happen is that we defined with a panel of experts is needle out of view. So this is one of the suturing operative errors and you see here the needle going out of view while trying to suture these two structures. We use computer vision to count the number of frames in which these the needle really goes out of view and by that we try to automate the surgical skill assessment. You also see that of course all the tissues, wires and so forth are being tracked. But this is a fairly simple error and more difficult errors are for instance a incomplete or repeated bite. The surgeon engages with the tissue, decides not to push through and then he or she retracts the needle and then you see that we're puncturing tissue but we're not suturing so there's an increased risk of leakage. And so then you no longer need to track your raw objects, you also need to know if your needle was touching the tissue, if the tool was touching the needle, you need to really know where your needle is because suddenly the needle gets deoccluded. And like this we add in complexity in using computer vision for automation. But time and time again we get 60% error reduction. So how can we use AI to optimize the OR for surgical training or for surgical efficiency? We first we need to provide unlimited access to surgical knowledge. Secondly we need real-time surgical information. And thirdly we need integration of image guidance. So training or efficiency that is in my helicopter view more or less the same thing because if you're very well trained you're most likely going to be more efficient. So let me go through the first point of unlimited access. We base our decisions in surgery and our training on evidence-based literature and we preferably learn in a safe and stress-free environment. So what we did to simulate or to enhance this aspect is build a surgical co-pilot which is really a true AI surgical teacher. So this is a chat-based app in which you upload a picture of your procedure and you can ask it whatever question you want. The co-pilot will assess it and will provide you with an answer. So you can ask about procedural duration, tissues, general knowledge of the procedure. So what are the benefits of robot assisted prostatectomy in this case. And the system will answer according to what you asked. And so this is a system that can work real-time. You do not longer need to disturb the surgeon while asking questions during his or her surgery. So let me quickly explain you how we built the system and then I'll also show you in the end how this works in real life. So what is the procedure at hand? We really tailored this co-pilot to one single surgery, which is the radical prostatectomy. Again, robot assistant, the surgeon sits at the console and the prostate, which is sitting between the bladder and the urethra, which is the duct for urine passage. This gland needs to be removed for prostate cancer. So what happens is that after that procedure, then you try to re-suture the bladder to the urethral stump or you need to suture that and you remove the whole prostate with the seminal vesicles. So firstly, as for any AI project, we need to generate data. And so our data pipeline for this vision language model, so it's not just a large language model, it really combines vision and language. We needed to provide images which have a question and an answer for every image. And so to get to that point, we firstly derived labels from the surgical scene, we generated captions for all of these images, and then we generated Q&As. So firstly, labels. For each image, we provided segmentation masks for instruments, for the tissues, we provided the face in which that image is in, as well as that map. So we could know if one instrument before the other is one instrument touching some tissue or another tissue and so forth. Then from this rich data, we need to generate captions. And this is what you see here. So basically, we take into account all this information and then per picture, we say what we are seeing derived from the annotations and the different depth masks and segmentation masks. So you see the display of a specimen removal phase, the location of the instruments is precise, is further elaborated upon what you see in the center and so forth and so forth. Then having that caption, we use that caption to generate a question and answer for that specific image. And this is being done by an LLM, which is grounded in all of the surgical knowledge we could get for prostatectomy. So we grounded it with all papers of prostate cancer, with documentation of the robots, with all educational folders we could find. And we used that to prepare our LLM. And then we added all the data. And so the surgical scene understanding is really derived from instruments, organs, tissues, events and phases. So as a recap, we have per image all the different tissues that are seen, like pubic bone, prostate, we have the different instruments, and we have the face. In this case, prostate is being put into a bag after the procedure during the apical dissection. So we also know all of that. And now we ask questions such as what is the surgeon doing now? And then the LLM needs to generate an answer that says the surgeon is now bagging the prostate, which is needed to remove the specimen out of the body, something like that. And like that, we generated about a million questions and answers. And we introduced diversity by having more than 20 different personas. So we had personas such as expert surgeons, as well as people interested in surgery, like a 12-year-old boy who wants to become a doctor. So there's different ways of explaining this, because you want to be able to explain this to a young, for instance, to a child, if of interest, but also to a rather old patient, a geriatric patient, as well as to peers, as well as to a medical people also present in the operating room. And to create diversity in the Q&As, we use multiple LLMs, such as the ones from Meta, Mistral, and Microsoft. So the final data set, as I told you, contains one million question and answer pairs for 20,000 images, scene-specific questions, and also general image knowledge questions on prostate cancer surgery. Then, so we fine-tuned a VILA 1.5 vision language model using this synthetic data of visually grounded, deep, knowledgeable Q&As. So going from the image, taking all of the metadata inside, generating the Q&As with an LLM grounded in surgical reality. And that's, I'll quickly go to some of the metrics of the results. So basically, the vision language model is very good in classifying instruments, and these performances that you see here are state-of-the-art for image recognition. Also, if you would just be using computer vision algorithms or classical vision encoding, decoding. Phase recognition is also a state-of-the-art for rather complex procedures, such as prostatectomy. We already get 82% frame level accuracy. So the system decides on one single frame what the current surgical phase is, and does that surprisingly well. So it has no temporal context on what happened before or what will happen in the future. And so this shows that there is a lot of lessons to be learned, and there's a lot of cross-pollination between the different modalities that the system has seen. Then we asked, based on the annotation, does a large language model think that the answer provided by our VLM is accurate. So we basically had the Q&A cross-checked by providing the description to another LLM. And then you see that our fine-tuned model, of course, rather than the base model, the fine-tuned model gets way more messages correct or answers. Q&As are classified as correct. And only a very small percentage is really wrong. So about 16-17% is wrongly answered. If you can have everybody ask questions in the end, there is still a big chunk of learning happening. And that's very much important. We also had a human evaluation. So we asked seven of our residents and three engineers to ask any question and then score it binary as question is right or question is wrong. And then again, you see that 86% of the answers were rated positively and only 20% of the answers were rated negatively. And so this really shows potential. And at the end of the presentation, I'll show you how we apply, how we do this live. Second thing in surgical, for surgical knowledge distribution or access is really video-based learning. And what we have in videos is that we can share videos and we can anonymize and move all metadata. But inside the video is not, it's not always really that anonymous. And what you see here, here you see me as bedside assistant. So my identity is being disclosed. But if the patient has a prominent tattoo or there is paper on the wall stating his or her blood type, again, you can have a very important patient privacy or GDPR breaches. And what we did for that is we developed an AI based anonymization algorithm that detects when the camera goes out of the body and then completely anonymizes that video stream. And so this is again, AI that enhances surgical safety and surgical efficiency, because we can now live stream what we want. Then on to real-time surgical information. So in the OR, everybody wants to, needs to be on the same page. And the only person that really completely 100% knows what is going on is the surgeon. There's a surgical resident who is, who knows, who, or who should know as much as possible, but still there is a knowledge gap. There's a scrub nurse with often a ton of experience who needs to know when he or she needs to hand in different types of instruments, when surgery is about to happen, if there's things that are deviating from the normal course. And then there's the anesthesiologist who does a ton of procedures across different disciplines, who needs to know, will the surgery be about to end? Is there a chance of high bleeding? Should I reduce the blood pressure? Is my patient still deeply asleep? Or is he waking up because some of the abdominal wall muscles are starting to contract? So there is really a need of distributing all knowledge and really creating situational awareness. And so for that we need that real-time surgical information. And so one of the, one of the steps as, that observers such as young medical students have is that they go into the OR and they prepare, and I have seen this myself, I prepared using all of my anatomy books and I renew all of the anatomy on images like this. And then you look at the surgery for the first time and you see images like this and you don't understand what is going on. You need to ask, but you don't dare to ask. So this is where again, VLM comes in. But this is also why we need to find a way to enhance surgical knowledge and efficiency, so that everybody in the OR is on the same page. And this is how we can do it. We basically segment all of the different structures in the, in this case of the abdomen. So this is a kidney surgery. You see that we can deline the tumor, the colon, the fatty tissue, the abdominal wall. And like that, if we can do this real time, we can really help to get everybody on the same page. And this is what you see here. Here you see the tissue of the, the fatty tissue, the garrota fascia of the kidney being incised. Once we dive under that fat, the kidney starts to appear. And thanks to the color code, you see that actually indeed this purplish part is the kidney. And as we dissect further and further, we also note that we can get the, the, the, we started to detect the, the tumor, for instance. And so like that, everybody's is truly getting their, their, the same information. And I have seen this work in real time. And I have also seen this help, for instance, scrub nurses with 20 years of experience that start to better understand what the surgery is about because they are basically doing things on, on automatic, on automatic pilot, not a co-pilot or an AI co-pilot, but on automatic pilot. And this really grounds people back into reality. Likewise, we have surgical phases, which can help us. So in this case, we have a, the tumor that is being identified. So the system detects tumor identification and this computer vision algorithm, it is an ultrasound happens. And then the surgeon dives underneath the kidney. And this is really where we see the algorithm changing from tumor identification to a higher dissection in this case. And this is useful because every face has different types of instruments, different types of requirements. Every surgeon has his own preference. And so like this, people in the ORC, can anticipate. And you see here, when we go towards the vessels of the kidney, higher dissection is started. Okay. Then onto our third point, integration of. So how do we integrate image guidance? Well, basically we can have a CT scan of which we create a 3D model patient specific, fully using computer vision or neural networks. We can segment the structures at hand. Then we can optimize this with a manual quality check. And we can even start to plan for, in this case, for removal of this kidney tumor, which arteries we should be manipulating and which arteries we should not be manipulating during surgery. But then the golden, the golden endpoint is really to be able to get that information into our surgical scene. And I'll show you some, some work that we did in that part to really also increase the efficiency, increase the safety of the operation and get people into their their next step or their, their, up, up on their expertise, even, uh, faster. So this is how augmented reality looks like. If you would use Google maps to come to our facility, nearby Ghent. And for our surgical strategy, this would look something like that. You see a kidney and then you project the kidney on top. Okay. But if you look very closely in surgical, in navigation, the, having an error that says moving to the right is actually quite dangerous because there can be a pedestrian behind it and you might run over, um, this, uh, this girl. Um, and likewise for surgery, you're occluding the instruments, scissors, um, graspers, whatever. So you can be manipulating tissue that you actually didn't want to manipulate. So actually you didn't really, you did not really help in improving efficiency and safety. And so what we did is we used the AI-driven instrument delineation and, uh, to improve augmented reality. And, um, the question then becomes, okay, all of these algorithms, how can they really run on edge? And I'll take you through the different steps of these, these parameters that enhance surgical efficiency, um, to really improve, um, the, um, to show how, uh, on edge can really improve efficiency. So this is our, uh, the, the nervous system of what we do. So we have an IGX ORAN, which takes in the, um, uh, laparoscopic video from the robotic console. We have a Delta cost capture card, which uses an RDMA connection to the direct GPU so that all frames are read in, uh, uh, uh, instantaneously and with, uh, near zero latency. Then we, um, have all the, um, the different inferences run and we send everything back to the, um, vision card where the surgeon can then also see in a separate, uh, window, um, the AI assisted, um, uh, view if need be, or, um, he or she can get extra, uh, information, uh, should that be. We also tap that, uh, signal, uh, to, um, external monitors. For instance, if you want to do surgical streaming and we are able to manipulate the ORAN so that if we want to manipulate 3D models, we can remove certain 3D models, we can activate certain algorithms and so forth. So firstly, so we have five algorithms running in, uh, real time in parallel on the IGX ORAN and the first one is anonymized live streaming. So this is during, uh, the EAU last year. So the European Association of Urology, we streamed the surgery to 3000 urologists. Here you see the CEO of Orsi Academy and you see that the image when the camera goes out is automatically anonymized. So again, we increase, um, the efficiency of surgical streaming. Then, um, we have the semantic segmentation algorithms. So, um, at the end of February, we organized the second edition of our surgical AI conference and we had, uh, all of the algorithms running in real time. We had the panel commenting on it and, um, I'll have our, um, uh, our CEO, uh, Professor Alex Motri, who also did that case, uh, explain you how he, uh, experienced this, um, these, uh, honest AI algorithms. ...side of the body, you have an automatic, um, uh, blurring. Uh, let's show it again. Can you take out the camera again, please? Is that possible? Just take it out again, please. It's just to show to the people. So I don't see it, there it becomes blurred immediately and very specifically in my, uh, in my, uh, console, it stays visible so I can have a look at the nurse and also to, uh, Sarah, who is, uh, um, helping me while you see only a blurred image. Good. So let's go in again and let's now see whether the, there is a, uh, organ, uh, detection. So, uh, number one here you can see, uh, the liver that is identified. Uh, the kidney, uh, the bowel, uh, Jasper tell me. Yes. And also the fat. Of course, the tumor is still completely in fatty tissue. So you see in this case that, um, these algorithms run in real time and, uh, everybody in the OR, um, or in the room, also a lot of engineers were, uh, present in the, at the conference who have never seen the inside of an abdomen before in this way, uh, everybody gets access to, um, that same, um, information. And in the meantime, we also detect the surgical phases. Uh, here you see another case. Uh, this was, uh, streamed in, uh, Bologna in Italy. Uh, the tumor is being detected. We had an audience looking at this surgery in real time, kidneys being detected, uh, arteries and veins are being detected. And, um, um, um, this really helps in, uh, in, uh, we think this really helps in surgical efficiency. So then surgical phase recognition, I already hinted, um, to give you a short overview of how a complex procedure such as a kidney surgery, there's a ton of, uh, different phases that can go back and forth. Sometimes some of the phases are skipped. This is something completely different than the golden standard, which is called cystectomy. Um, some of you might have already heard about colic 80 data set. I really applaud that data set, but you see that real complex surgery is often another ballgame. And so here we show some of our results. So here you see, um, a live surgery again, where we just went from, uh, dissecting the hyaline and now we start to mobilize the kidney, freeing the kidney from the fat. And you see that that new phase is, uh, instantly, um, being, uh, detected once the phase is really sufficiently ongoing. Likewise, here, we just started to identify the tumor, the small rim, and then we start to really excise. And then you see that the face jumps to, uh, really tumor, uh, excision. Um, so phase detection, um, um, works in real time as well. Then onto our augmented reality instrument, the occlusion. So this is really using a real time, uh, instrument segmentation algorithm to put our instruments on top of the augmented reality stream. So this is a case of a, um, migrated vascular stand. Um, you see the overlay of that stand. This is the Vena Cava and that stand is somewhere positioned here. And you see how all of the instruments are actually on top because the detection of the instruments is on. When the detection of the instruments is turned off, we can do that. Then you see the previous state of the art. So you don't see anymore where the instruments are when the detection is off. And of course, this is a lot more difficult to do image guided surgery and to increase the efficiency of these new technologies. This also works for liver surgery. So here you see a lesion of the liver. Again, the instruments are put on top. And so this scales across different types of surgery. We also tested it for instance, in lung surgery. So that tends to be very efficient. And, and from a technical point of view, this is how things look like. So again, we capture the, um, the view with a delta cost operator. Um, then we drop the alpha channel. We have a format converter that crops, that resizes, um, and then we pre-process everything for, uh, with a custom CUDA pre-processor. And then these different, um, AI algorithms are in first. So we have the organs, the tools, the anonymization, the phase, and all algorithms were optimized for real time. So we converted from, uh, ONIX to TensorRT, where you see a speedup that goes from, uh, six times to, uh, 50 or, uh, almost 100 times. Um, so this is a big gain in, uh, going towards real time. Then we need to do some post-processing, of course, some activation functions. Also after, um, cropping, inferring, we need to resize and rescale back to the original image size. And then we visualize. And we allow our 3D model to be inputted, as you can see here. Um, we use the whole ScanSDK and some of our pipeline is also, uh, available online if you want to build further on this and you want to really, um, help us push the field forward. Um, we use VTK for rendering, uh, of the 3D model and overlaying. But then again, we still have a problem of organ delineation and surface reconstruction. As you can see here, the surface is really, uh, um, the kidney is moving. We, the, the tracking of the 3D model is not automatic. This is where we teamed up with Infusion to solve tissue tracking. Um, what we have is we now have a, um, segmentation which identifies the target tissue. We have stereo depth estimation to get an idea of how the surgical scene looks in, in 3D. And then we also have optical flow so that we can estimate what the motion of these, um, the, the, the, the, the 2D object is from a frame to frame basis. And all of this was implemented in Infusion's, uh, processing, uh, image processing framework and execute it on an IJX or, and here we see some more, uh, images like that. So we have frame matching between the different stereo images. We're able to match, uh, from one frame, the reference frame to the current frame. We're able to see where things are going. Also in 3D, we're able to map. And this means that we can rotate the 3D model concurrently. So here you see a case that, uh, that we did. So you see how the surgeon is manipulating that kidney and you see how automatically now thanks to, um, the, on a JI algorithms were able to follow that motion, um, instantly. And again, we tried this, uh, we streamed this real life with the more difficult tumor that you saw, earlier today, which I'll be showing to you now. So you see that, uh, that kidney tumor of that life case we, uh, hinted to earlier is covered in fat. So here the algorithm actually has more difficulty. So we're not completely there yet, but we're getting there and I'll again leave it, uh, I'll give you an impression of how it went. Pull up a bit. Yeah. And now to the right and make it a little bit bigger. A little bit bigger and to the left, a little bit smaller, a little bit smaller like this fix it like this. It's fixed. It's fixed. You make a question. So you see already how fantastic this is with the instruments that are outside, eh? So I think that is already fantastic technology. As a matter of tracking overlapping because the vessels are straight in a way or another might be easy. So, but look how it is coming. You see that? Let me move now to the right. And you see it's not completely perfectly aligned, but again. So we are getting there, but some cases are really better than others. And I think this is also bridging engineering and surgery is really, uh, trying out and getting feedback from end users. So, uh, all of these run in, in real time, but then still we have our co-pilot also running on an IGX or an IGX or an. And so we use that during the real time surgery. So, um, this, uh, here we have Dr. Don doing a prostate case. Uh, all images, um, are being loaded into the VLM. And when somebody asks a question, that frame, uh, current frame is being grabbed and the system answers questions about that surgery and everybody in the OR can just, um, log into that video feed and, uh, ask questions. And I'll show you, um, some examples of how that, uh, how we experienced that session. So it's a bit, a bit sped up, but here, uh, we go into the surgery and, uh, Dr. Don is explaining where he is in the surgery. And then we also asked, so what is the current surgical phase dissection of vast difference and seminal vesicles? And this is actually correct because here you see the the vast difference of, uh, this phase. Now you can ask more common things that are not really relevant to the image. What is the usefulness? Yeah, of course, this is essential for removing of the prostate. How big is the prostate? We didn't input into the system that all patient details, only some patient details, but not the MRI. So it cannot answer that question. But we did input to the BMI. And then it knows that this is actually not mentioned in the video stream, but that it's a 50 year old patient with the BMI of 28. And so this is derived from, uh, from the, the metadata. Remaining surgical duration. How long is this phase supposed to still take? Which can be interesting for the OR planning, for the efficiency of the OR, for the anesthesiologists. Question about instrument usage, uh, difference between the different instruments that are being used. So there is, uh, really several ways that, um, this can be used. Uh, pathology, uh, planning of, um, of the surgery up front. Um, what is the structure that is being inside? So this is the denovillier, uh, denovillier's fascia, but it thinks that it's the endopelvic fascia, which is not, uh, not true, but you need to go through the endopelvic fascia. So it has some understanding of where it is. And then you ask, can you explain like a five-year-old and it starts to explain, imagine you're playing a video game, et cetera, et cetera. So for really people starting with to starting to learn, this is really, uh, what we, uh, what we think that, uh, that really matters. Um, so then we have some, uh, we had some, uh, uh, or less, uh, some other questions to see if it would hallucinate because we had a lot of people, um, asking questions at the same time to our, uh, uh, IGX or, um, and so we see really see, uh, the potential of this, uh, uh, this type of technology in, uh, distributing surgical, uh, uh, uh, knowledge and, um, lowering barriers. So the surgeon doesn't see all of these questions, uh, and in real life surgery, he gets all of these questions and he needs to answer them while focusing on the surgery. Now we can just ask, um, ourselves and see, um, uh, and, and, uh, and get more, uh, more knowledge. Wow. There we go. Um, so, um, this was the final part that I really wanted to, uh, to highlight or to show. Um, I think it's important to note that this shows how already today AI algorithms, uh, that are working on edge can improve how people interact in an operating room and how this might lead to surgical efficiency. But the true this in the numbers. And so we really need, um, systematic, uh, trials that show that people with this technology outperform people without this technology. And this is really what we need as a surgical society. It's not necessarily pushing, uh, uh, 98% to 99%. It's really showing the value of these systems and bringing it into the, into the hospital. And this is like my key message, my, my takeaway message. My second takeaway message is that this is a ton of work of engineers collaborating with surgeons. And, um, I would like to, uh, as an end note, uh, thank, uh, the whole team at Orsi, but also, um, the different clinical collaborators, as well as, uh, the collaborators at Nvidia to help push this field. If people are interested, feel free to, uh, to reach out or to send me a message using, uh, the email below. Thank you very much. Thank you very much.