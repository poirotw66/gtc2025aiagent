 Hello everyone. I'm He Wang. I'm the founder and the CTO of Galbot. It's my pleasure to talk here at GTC Online. Today, the topic of my talk is going to be End-to-end Embodied VRA, Empowered by Synthetic Data. First of all, I'm going to introduce our company. Galbot is founded in May 2023. It is a cutting-edge company that focuses on developing embodied AGI agent aiming to offer general-purpose robots worldwide. Today, we will revolve around the topic of generalist robots. Definitely including NVIDIA, a lot of cutting-edge companies have developed their own generalist robots, such as Tesla Optimus and NVIDIA Project Groot, as well as our Galbot G1. We believe generalist robots are going to be very revolutionary compared to specific-purpose robots. They provide general-purpose mobile manipulation for any applications. You can communicate with the robot via natural language. You don't need any coding. You can use the robot very well. You can also do zero-shot and few-shot learning to get more skills. To empower such generalist robots, we believe we need an end-to-end vision language action model. Vision language action model is proposed by Google in the paper RT2. In this paper, they basically proposed such a large model that takes language, vision, and other perceptive and sensor signals as input. That's why it's called VL. Its output is action. So adding action to VL is VLA. This VLA model is end-to-end. Also, it is an embodied model. Basically, similar to other vision language models, it takes language input here. It takes what should the robot to do task, and those text input. The text token nether will convert those texts into language tokens. The VIT will convert visual observations via VIT into visual tokens. The other tokens will be sent to a transformer, and then this transformer is going to output action tokens. Those action tokens can be de-tokenized into robot actions, which are delta translation and delta rotation at each time step. Such an end-to-end VLA model basically handles visual perception, and also task planning, and also action generation. So we can draw an analogy to other large models. Basically, all the large models, we can classify them into disembodied models and embodied models. On the left side, we will see many popular disembodied models. For example, LLM, GPT-4, DeepSeq, and VRM. For example, GPT-4-O, and video generation model, for example, Sora. All of them take digital inputs and output digital things. So they basically stay in the digital world without a physical body. And then we can go to the right side where we see embodied models. Right now, we have a good example of such embodied models, which is Tesla full self-driving model. And this full self-driving model, Tesla FSD, is an autonomous driving policy. It takes vision and navigation inputs and output actions on steering wheel and accelerators. So, you know, going one step further, for generative robots, you know, navigation signal is not going to be enough. We can do both navigation and going beyond navigation, we should do manipulations, operations. So that's why, you know, like rather than taking navigation signal as input, we should take human language instructions or text inputs. And that's why, you know, to some degree, VRA is going to be the next step of Tesla's full self-driving model. It takes more general instruction input, which is text. And compared to Tesla FSD, so like VRA takes vision input and they all output actions. But for autonomous cars, the action dimensions is quite low, which is just two. And for humanoid robots, the action dimensionality is quite high. You know, one arm can have six dimension or seven dimensional actions. And the whole body, you know, action of humanoid robots can easily go up to 20, 30, or even 50-ish. So like an embodied VRA model would be more, you know, challenging to develop than FSD in terms of action dimensionality or the things it needs to handle. And recently we have seen many progress in VRA. This work, like PI0, published by Physical Intelligence, they show a very interesting VRA that can take, you know, human command, for example, clean up the table as language command and use, you know, different robotic arm to do the task. And you can see, like here, they have random objects on the table or even if they have X and they learn to put the X into the box. And all of them are powered by a same VRA model. And it seems like PI0 is quite successful. However, I have to say, or I have to point out that, you know, the data collection right now becomes the bottleneck of PI0-like VRA model to really, you know, like harness any task, do any jobs. And the reason why, you know, VRA data becomes a bottleneck. This is, you know, robots needs robot data to learn, right? So, you know, autonomous driving, we have so many drivers, they, you know, they are, you know, volunteer to drive their own cars, right? So that's why you will, the car companies will have endless driving data every day, every hour from the drivers. But for human-wired robots, you need to hire people to remote control, where we call teleoperate the robot to collect data. On the left, we show, you know, Tesla's, you know, teleoperations factory, where Tesla has many human, like every person, wear motion capture devices. And they also wear VR glasses, they can see through the eye of Tesla Optimus robot, and they will, you know, move their arm accordingly to control the arm of the robot to do things. And also, on the right side, we show, you know, the teleoperations system developed by Stanford, which is called Aloha, where, you know, people have like four arms, two master arms and two slave arms. They use this master-slave system to collect the data. And both of these two approaches can collect the data. However, both of them are really time consuming and really expensive. For example, in Google's paper RT1, like it takes them 17 months, like with 16 people and the 13 robots to collect in total 130,000 demonstrations. And, you know, we can imagine how many driving data, how many hours of driving data do we have to train Tesla FSD. It definitely needs a lot. And just, you know, collect 130,000 demonstrations, it takes 17 months. So that's why, previously I mentioned, data becomes the biggest challenge and the bottleneck to develop a very functional and generalizable, you know, general purpose VRA. You know, to confront with this challenge, Google people proposed to, you know, outsource their data collection task. They gather data from different labs worldwide and they in total get 1 million trajectories across 22 different robots. And this data set definitely cover a lot of different skills and different scenarios, environments. However, you know, it's intrinsically in nature, heterogeneous, the data and data comes, come from different robots. So I have to say, it may not be optimal for, you know, learning skill on a specific robot. So that's why, you know, right now we only have two choices. Either we have many people to collect data in the real world for one specific robot. This can be very expensive. And at the same time, you know, when you want to upgrade, update the hardware of this robot, you want to change the kinematic structure of the robot. Then the data you already collected, the effectiveness of those data you collected previously on a previous hardware will be degraded significantly. So the other choice is like this open X embodiment of the data set. You have to tolerate the differences across many different robots, which may not, which may be suboptimal to train a high performance VRA for a specific robot. So in this talk, we propose an alternative data approach, which is synthetic data. So, you know, synthetic data can be really scalable. During the past few years, my team and also our startup Galbot put many efforts on pushing synthetic data. We, you know, curate our own object assets and we label the part information on all those manipulative articulated objects to form Gaparnet, which is a CVPR 2023 highlight paper. And on those object assets, we also label action. So, for example, our dexgrasp net, which is the worldwide first million scale dexterous grasping data. We synthesize a million dexterous grasps on 5000 different objects. And we believe this is going to solve the data bottleneck to learn dexterous grasping. And at the same time, we also try to understand the gap between simulation and real world. We simulate the depth sensor to understand the failure cases, the artifacts in the commercial depth sensors. So we believe we are, you know, combining objects, actions, and sensors. We're going to create an environment, a significant sufficient amount of data to train very powerful and generalizable VRA without any real data. And so to prove this is doable, we start with something easier. So we start with the vision language navigation task. So here, our model also takes visual streaming video inputs. And at the same time, it takes human language instructions. For example, walk out of the bedroom, turn right, stop before the stairs. And the only thing, you know, we, you know, make it easier at the very beginning is that the task is just a navigation. So our agent, our robot only need to follow the human language instruction and the based on the visual observations. And this is also an end to end, a single model. We want to prove that the robot can navigate in a human-like manner, understand human instructions and do everything correctly. So in 2020, the RSS conference at 2024, we, you know, proposed this work, NAVID, video based VRM plans the next step for vision and language navigation. This is a 7 billion parameter model that takes video and language as input and directly output, you know, actions. Basically forward by how many centimeters, turn left by certain how many degrees or turn right by how many degrees or stop. And this model is completely trained on synthetic data. So we collect our own navigation data based on our R2R dataset training split on VRNCE simulator. It in total include 10,819 episodes across 61 different scenes. So in our training data, we have both, you know, like a video, like a combined with detailed navigation instructions. And also, you know, you know, you know, you know, a whole training data on the internet, which is, you know, like a video data without any actions. Basically on the left, all the video and the language data come with action labels on the robot. So on the right side, the general vision question answer data, it just, you see some random images or random videos. And there are some questions regarding the image and the videos, and there are answers. So we can leverage both embodied data and disembodied data. And such combinations allow our robots to learn general navigation across 61 scenes. And at the same time, see all kinds of environments on the internet so that it, like, transfer the navigation capability from the limited 61 scenes to all kinds of different environments. Right now, I will directly show you that our robot can be directly tested in unseen environments without any fun tuning or, you know, training in real world. Here, we show that we ask the robot to go straight to the wall, then turn left and walk to the door and stop. You can see it follow the instruction really well. Even if like the robot never see this environment before, like it, it is worth of mention that we. This navigation is only done by a single VLA model. We do not need any mapping module, for example, SLAM. We do not need an odometry to measure the distance that the robot travel. And it also, we do not do any segmentation or, you know, object detection. This is just a one end-to-end model, very similar to, you know, Tesla FSD. The only difference that we can take language inputs. And so you can see even like this, go straight and move close to the plan and then turn right facing the door, then walk to the door and stop such very complex instruction. Our robot follow it really very well. And just compare with, you know, these embodied multi-model models, for example, GPT-4B and 4.0. Our model actually achieves way better performance than those. And since we have the like a large scale synthetic data in the simulators to empower the robot with general navigation capabilities. Like in this work, our model just take, you know, continuous RGB observations. But we immediately realize that this may limit the performance of our robot because it doesn't have the, you know, spatial intelligence. And so we also observe the many concurrent work from Google. And this is spatial VRM. They try to distill information from depth information. And so that, you know, the VRM can better understand the spatial relationship. Which one is far, which one is close, and which one is big, which one is small. And this tackles the, you know, current shortage of spatial reasoning in state-of-the-art VRMs. And in our work, we also try to incorporate modalities that comes with spatial information into our end-to-end navigation models. In our recent work that's gonna, you know, appear in ACRA 2025, this work is called NAVID-4D. So we further take depth observations as input. And this NAVID-4D model right now can understand, you know, both, you know, position on 2D image as well as the depth. So that's why many tasks previously we cannot do very well. Right now we can handle very well. For example, you know, we can ask the robot, ask the robot to enter the room through the door, walk past the chair, and wait behind the chair. Here we use red letters to highlight, you know, things that require spatial intelligence. Or, you know, now our agent knows, does know, know to compare sizes, right? So we can move to the bigger plan and stop. And it also aware of distance. It can walk forward and wait by the furthest chair. And it also can count which is the first, which is the second to do. For example, go to the first chair on your left and wait. So this is a comparison. The left side is NAVID-4D. The right side is NAVID. So now by taking additional depth information, our NAVID-4D agent can easily understand which chair is further, which chair is closer. And, you know, if the agent only takes 2D RGB information, then it's hard to say whether, you know, the chair is closer to you or it's just that the chair is bigger than the other. And the depth information helps us to know very, help us easily know, identify the front chair is, the right chair is actually closer to the robot. And now it could do better jobs on those spatial reasoning and gain more spatial intelligence. So it seems like our agent now can follow some instructions really well. But, you know, we humans usually do not give such detailed instructions to other person. We're going to say what is our demand, what thing you want your friend to help. So in our very recent work, just submitted, we build a general purpose navigation agent that allows different kinds of language instructions. Previously, the instruction has to be very detailed. So this is the vision language navigation setting. And now you can give other type of instructions. For example, search for a chair and stop by it. This is basically the object goal navigation task. And also what is the room of the bed located in. So you just give a question to the agent. Ask the robot, you know, walk around, try to find the answer of the questions. This is called embodied question answering. And also you can ask the robot to follow a person. And this is a pretty dynamic task. And you do not specify a static destination, but you give a dynamic target to chase. And in total we synthesize more than 3.6 million trajectories for all these four tasks. And each trajectory come with language instructions and also first view videos from the robot. And we use this data to train a very powerful and unified navigation model. I can show you this. So our agent right now, like can take such a instruction, move forward 10 steps and then turn a large left and search for a person, follow the person. What is the color of that person's clothing? So after, you know, find the person and our robot is following the person. And after the robot, after the person stop, our robot also stops. And it's gonna answer the question. Yeah. Yeah. The color of it's black. Right. So you can see right now we can mix the four different kinds of things into the same task. And our robot can, you know, handle all of the task together, all together. And we also see emergent behavior from our robot. During training, we only train the robot to follow human. And those human are actually digital human and with very limited variation in their appearance. But because we train this UNIN-NAVID model with internet data and co-trained with internet data, it has seen many different human on the internet. And at the end, you know, it also see many different creatures, for example, dog or even robot dog on the internet. So in the real world, we can directly ask our robot to follow a robot dog. Note that we never train this skill during training. The robot never sees another robot dog during training. And in the real world, it can directly do this task very well. So this is the benefit of building a VRA model. It basically shows that, you know, the knowledge can be shared across the theme to rail. And also, we can leverage 3D modality in VRA to enhance spatial intelligence. And those emergent ability comes into being, leading to a decrease in the marginal cost of learning new skills. We never train our robot to follow a dog, but it naturally knows how to follow a dog without the cost of capturing new data. And this is the benefit of building a large model, end-to-end VRA. It absorbs knowledge, both from your simulation world, both from the internet data, and at the end, directly transferred to real robots. So, yeah, this is pretty much the part of navigation. Hopefully, I have convinced you that sim-to-real is really powerful. It provides a really low-cost data plan for building VRA. But again, people may think, ah, you only do simple tasks like navigation. It's just for a car-like thing, very low-dimensional actions. Why don't you try something like manipulation? Yeah, you're right. So, you know, this type of pick and place require high-dimensional output. For a single arm, it is 6 degrees of freedom or 7 degrees of freedom. So, we, again, leverage synthetic data to perfectly done the job. So, in our work, we leverage NVIDIA, like a synthetic data suite. And it, you know, it, you know, includes the robot model, object asset, material assets, and it allow us to generate all kinds of different environments. And also, Galbot has our own self-developed action synthesis pipeline. So, given static scene, we can synthesize, you know, grasping trajectory, and make sure our robot can do this job. Same, same, exactly the same in the real world. So, we, for the first time, synthesized 1 billion frames of robotic grasping data on FRANCA, IMEKA arm. So, you can see, you know, our data compresses all kinds of different background, table color, object layout, object geometry, and, you know, different language instructions, what to grasp, for example. And the lightning keeps changing. And also, the object, you know, the way to grasp are randomized. So, this billion scale synthetic data allow us to learn all kinds of grasping in all kinds of environments. And we then train our VRA model on this billion scale synthetic data. For the first time, we observe all kinds of good properties, including, you know, illumination generalization, background generalization, 3D position generalization. They are, you know, from vision perspective. And also, we observe category-level generalization and open vocabulary grasping. This is from language perspective. And also, our robots have real-time closed-loop reaction and can robustly handle dynamic distribution and out-of-distribution cases. This is from action perspective. Basically, here, we believe they define the golden standards of VRA patroning. And here, I will show you, you know, at different, you know, environment, even this really challenging disco lighting. The X input is to grasp a dock and our arm can grasp all the docks at challenging, you know, lighting conditions correctly. And you can see that this is not a pre-recorded trajectory. At the middle, when it turns light to the dark and we move the dock, and the robot arm immediately reacts to grasp the dock. This shows our model really generalize across diverse illuminations. And also, we show that our model can work at different, you know, backgrounds, generalized across different backgrounds. And we change the, you know, the table texture. And we change the, you know, the other cases, our robot arm successfully grasped the strawberry. And we also show generalization across different height and positions. right here. Here, we ask the robot to grasp a basketball, like a volleyball, soccer ball, and baseball. Yeah. So, you can see, you know, although the balls are really small and they're really tricky to be pinched by, you know, the two fingers, but our model successfully grasped all of them across all different kinds of positions and height. And also, you can specify any object categories. For example, here, we ask you to grasp some unusual objects, including a shopping cart, this toy, a truck, and the, you know, swimming goggles, also electric meter, right? So, our model can, you know, easily recognize the object you mentioned and identify them in the scene and do closed-loop VRA-based grasping together. And this, like after our training on purely synthetic data, our model are also really robust against the perturbation. It showcases its real-time closed-loop reaction. We keep changing the position of this water bottle, but always our robot arm manages to grasp it. Even if you put it down, right, it will change, you know, rotate the gripper and get it at another place. And we can do it more crazily, right? So, there are four objects on the table, and now you just randomly throw many objects onto it, try to stop it from get the object. But, yeah, it works. It works. It successfully gets all the objects in the four cases. And even if you use very challenging way to block the observation, try to fail it, our robot is still able to grasp the object and regardless of this out-of-distribution, distribution, distribution, distribution. Okay. So, this is pretty much what we have after pre-training on purely synthetic data without any real-world data. And then we will be able to do some post-training if needed. In what cases we need post-training? So, for example, in this case, we have like a 5 by 4 in total 20 bottles in a box. And right now, if you ask the robot to, you know, grasp a water bottle, it doesn't know which one to grasp. So, in order to let the, like, our very understand of human preference, we can capture just 200 trajectories, which is finished by a single person within 4 hours. And just to fine-tune on these 200 trajectories, our robot learns successfully to grasp a water bottle one by one from the left side to the right side, from the front to the back, and exactly follow your human preference. And this pretty much resembles the RLHF stage, post-training stage in LLM, where you only need a little data compared to the huge amount of data you need in LLM pre-training. And this showcases very high efficiency of our pulse training. Only in these cases, you don't lack, you know, the random grasping way from our training stage. You want to specify your own grasping order or way to grasp such specific type of objects, you will need real data. Again, you only need very little real data compared to the real data needed in PI0 and OpenBerryA, RT2, etc. And the good news is you only need to collect the data on this one brand of bottle. You can directly test on other brand of bottles. For example, this is Nongfushanquan, another Chinese water brand. And on the right, this is Dongfanshuye, which is like a tea bottle. And you can see the color of the lid have changed. And also the size, the height of the lid are changed. But it's okay. So without capturing any data, new data, the model has this emergent behavior to handle these new bottles. And this is the great power from VRA. Only need a very small amount of real data, and you do everything from the same category correctly, successfully. So how about summarize our, you know, grasp of VRA technology? So it basically points out the foundational nature of pre-training. You really need a very high quality and large scale data to make sure that your model learns to grasp anything in any environment. And this can only be done by pre-training on high quality, single embodiment synthetic data. And then after this pre-training stage, only if needed, you can do some post-training using just a little bit real-world data. And this will help the model quickly align with your specific preference. And you will be able to finish the deployment of such robots in the real world within a single day. This shows really high efficiency of post-training, really low real-world data collection cost. And such training and data plan provide a truly scalable data solution for embodied VRA. And the previous I mentioned, you know, right now, the hardware of human noise are still upgraded frequently. If you collect a lot of data using one robot, then when you update your hardware, you actually have a big real data burden. You do not want to change your robots anymore because you already have enough data of one specific robot type. And also if you capture data from many different robots or even cross different robot companies, then your data quality will be degraded, which are suboptimal for VRA pre-training. So that's why, you know, leveraging synthetic data for single object embodiment avoid other issues. And we can embrace frequent hardware updates and all the data cost is like pretty low. And this is truly scalable. We believe in total, this grasp of VRA plus previous navigation works points out a new paradigm of embodied VRA. So we need to do a pre-training on synthetic data and then only do post-training on real data if needed. So our robot, Galbot robot empowered by our own VRA models right now have been applied to retail scenario, reception tasks, healthcare applications, and also many industrial applications. we say in the book, Meituan exhibition, we show that customers can place orders on the iPad and our robot can fetch the according medicine boxes from the shelf. And also our robot is able to do restocking. restocking, you have a basket of medicine boxes, and then our robot can recognize the medicine boxes and place them on the correct spot on the shelf. This pretty much defines a 24-hour unmined operation scenario, which will benefit many pharmacies and retail stores. Actually, we have opened several stores and we plan to run many stores by the end of this year. Our robot also works in Beijing Mercedes-Benz factory. Here we leverage our vision system to identify all kinds of errors and failure cases in this rooftop transferring task. Our physical interaction skills can easily correct it. In Z-Curve factory, again, based on this vision closed-loop control, our robots are able to transfer this 3x3x3 in total 27 different boxes. And you can see it actually handles different boxes, different ways, many and very intelligently. So some box, it needs to pull a little bit and then get the space for the other hand and then use two hooks to get it and then move it to the production line. And our robotic design allows our robot to lower down its height to get the box on the second layer and even at the bottom layer. So it can do this task really successfully. And at average, our robot can transfer even more boxes than a person that needs some rest after doing long-time labor work. So I believe we have shown the applications in both retail and car factory scenarios. The objective of our robot is actually to serve thousands of industries and empowering billions of families worldwide using our own robot and VRA. Hopefully you enjoyed my talk today. Thank you very, very much for the time. And please, you know, subscribe our media and probably look into our recruitment channel. Thank you very much. Bye bye guys. Bye bye guys. Bye bye guys. Bye bye guys. Bye bye guys. Bye bye guys. Bye bye guys. Bye bye guys. Bye bye guys. Bye bye guys. Bye bye.