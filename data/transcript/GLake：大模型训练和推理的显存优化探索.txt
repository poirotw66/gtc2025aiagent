大家好,我是来自蚂蚁AI Infra推理引擎团队的张瑞很高兴今天可以在GTC 2025 China AI Day分享我们过去两年时间里在大模型训练和推理上的一些显存优化工作首先,我将介绍我们在训练上做的显存优化工作GM Lake该工作已发表在S Plus 24上主要是解决在大规模训练过程中的显存碎片问题主要分为如下三个小节来展开介绍第一小节主要来介绍Background and Motivation众所周知,近几年大语言模型高速发展模型迭代加快,模型规模越来越大但是GPU的显存增长速度要远远远远于模型规模增大的速度从上面的增长曲线可以明显地看到模型规模和GPU之间存在严重的内存墙使得我们需要更多地去关注如何优化训练过程中的显存为了解决训练过程中的显存问题业界提出了很多的训练框架和技术而GM Lake主要focus在训练框架中的缓存分配器这个层面如右图所示在模型的训练过程中从时间角度来看显存分配呈现出很规律的memory pattern所以训练框架中多采用缓存的方式来减少显存频繁分配和释放时引入的巨大开销随着模型规模越来越大训练时一些中间结果变量会占用很大的显存空间导致在backward的处理前很多显存处于闲置状态所以业界提出了recomputing技术在前向阶段保存一些checkpoint释放大量显存在后向阶段过程中通过保存的checkpoint重新计算出前向过程中的一些关键中间变量这种方法可以有效提升显存使用效率增大训练过程中的batch size右上角的图为正常的训练过程可以看到显存分配比较规律但是右下角的图显示了当使用recomputing技术之后显存分配变得很不规律以及分配大小很不均匀导致在缓存分配器中会出现很多的显存碎片当缓存分配器中无法找到匹配相应大小的block时会触发GC操作释放掉一些空闲显存然后再按照请求大小重新申请此时不可避免会引入较大的分配和释放开销我们详细分析了PayTouch中用于统计显存效率的指标我们将框架层中的显存利用率定义为Active Memory比上Reserved Memory我们首先评测了多卡对显存利用率的影响发现当设计的卡数越多时由于一些变量被切分更小会引入一些显存不均匀分配的情况使得显存利用率降低同时当叠加更多的显存优化技术之后也会使得显存利用率更低虽然这些手段可以使得模型训练更高效但是会降低显存利用率使得存在很多的显存碎片还没有被有效地利用起来为了解决显存利用率低的问题我们深度挖掘发现是因为缓存机制在显存分配不规律以及不均匀时很多大的显存会被切分为小显存而小显存无法被有效利用缺乏一种有效的Stitch机制为此我们调研发现因为达提供了一种新的显存管理接口将虚拟显存和物理显存的管理拆分开我们可以先按照一定的力度大小创建很多的物理显存然后根据需要分配一些大小不同的虚拟显存再根据实际的显存需求映射一定数量的物理显存到特定的虚拟显存上使得显存的管理变得更为的灵活但是这种接口同时也带来了很大的挑战即里面的显射开销很大物理显存力度越小显存分配的开销越大从我们的实验结果来看分配两GB的显存如果按照两MB的物理块的力度来分配相比于直接Native的方式来分配会有115倍的性能开销所以我们在GM-Lake中设计了一种新的管理机制既可以利用VMM带来的灵活显存管理的便利也可以减少分配引入的开销问题接下来我将分享GM-Lake中的实现部分GM-Lake扩展了PyTouch中的缓存管理器利用我们新设计的Stitch API引入了更有效的显存管理数据结构具体来说在分配操作中我们引入了新的Stitch操作在释放操作中我们引入了UpdateStitch操作除此之外我们构建了多级显存池来支持更高效的Stitch操作最底层我们利用了Cuda VMM API来实现这些操作这里我将具体分享下我们的分配策略给电一个分配请求GM-Lake会根据请求需要的Size在多级显存池中寻找最匹配的Block来返回这里我们将该过程概括为四种具体的Case在Case1中在多级显存池中如果能找到恰好Size匹配的Block我们会直接返回该Block供上层使用在Case2中如果匹配到的Block的Size大于请求Size我们会将该Block切分为两个SubBlock其中一个SubBlockSize满足RequestSize返回给上层使用另外一个SubBlock放到MemoryPort中供之后的请求使用在Case3中如果有多个Block加起来总的Size比Request的Size大此时我们将最后一个Block切分为两个SubBlock例如Block4切分为4和5234通过Stitch操作拼接成新的Block返回给上层使用而5放到MemoryPort中在Case4中如果MemoryPort中的Block的Size不够Request的Size我们会重新分配一个Block与MemoryPort中的Block一起Stitch为一个更大的Block满足RequestSize返回给上层使用如果在前4个Case中都无法找到合适的Block会返回OOM这里需要注意的是我们Stitch给出的Block当使用完成后会被放回到Stitch的MemoryPort中如果这个Stitch的Block中包含的PrimativeBlock都是空闲状态时就可以被重复使用通过Stitch操作我们将物理显存在训练阶段高效服用起来在训练的前几个StepStitch操作会潜在把一些可以分时服用的物理显存拼接在一起在后面的训练过程中就可以高效服用而基本不需要再进行Stitch操作这样就可以把VMF的开销分摊在整个训练过程这里我们给出一个具体的优化效果的例子按照时间维度这里有6个显存请求在原始的缓存分配器中R1和R2由于显存池中在初始状态时没有任何显存所以会新分配两块显存B1和B2而当R3请求到来时由于其显存比单独的R1和R2都大只能重新分配B3R4请求也是类似比MemoryPort中的所有Block的显存都大也是只能重新分配B4而当R5请求到来时由于其大小与R1请求相同所以复用了R1请求的B1R6请求时MemoryPort中没有匹配的Block只能新分配B5所以可以看到其原始的分配方式只复用了一个B1的块剩下所有请求都需要重新分配物理显存这里再看使用GM-Lake之后的效果R1和R2请求在初始时都需要重新分配B1和B2符合前面一节中的Case1而R3请求大小可以附用B1和B2Stitch之后的块符合前面一节中的Case2R4请求和R6请求都是尽可能附用显存尺中空闲的物理块然后针对不足的部分重新分配新的块符合前面一节中的Case4而R5请求直接附用B1从上面可以直观看到使用GM-Lake之后可以有效优化显存的消耗接下来我将分享我们的一些评估结果我们在16个A100GPU上利用多种训练框架以及叠加使用不同的显存优化策略来评估GM-Lake在多种模型上的效果在Training Engine层面我们选择了Pytouch FSDPDeepSpeed ZeroClose AI Germany在模型我们选择了8个业界主流的模型OPTWikunaGPT2GPT-Neo-RX4等我们总共测试了76个PhyTuning Workloads在策略评估中叠加更多优化策略后不管是显存消耗还是显存利用率GM-Lake都明显优于Pytouch平均单个GPU上可以减少10GB的显存最多单个GPU上可以减少17GB的显存在分布式评估中随着设计的GPU卡数越多GM-Lake同样都显著优于Pytouch平均单个GPU上可以减少9GB的显存最多单个GPU上可以减少17GB的显存在Batch Size测试中随着Batch Size逐渐增大Pytouch在到达一定程度后会直接OOM而GM-Lake可以稳定训练平均每个GPU卡可以减少7GB的显存而单个GPU卡最多可以减少12GB的显存在不同的训练框架中测试GM-Lake平均每个GPU卡可以减少14GB的显存而最多单个GPU卡可以减少25GB的显存在所有的测试中GM-Lake的吞吐均以Pytouch保持一致而Pytouch在OOM之后GM-Lake仍可以稳定训练最后我们通过一个Memory的Trace来展示一些Valuable Insight在图上可以很清楚地看出在进行几次Step之后Pytouch由于Reserved Memory超出单个GPU的容量会导致OOM但是我们从Trace来看Active Memory是要显著小于Reserved Memory这种情况揭示了Pytouch中有很多的显存碎片没有被很好的利用展示了其显存利用率低的问题而GM-Lake在进行几次Stitch操作之后显存趋于稳定完成了整个训练过程展示了GM-LakeStitch操作的优势本章节开始我将介绍我们在推理场景进行的一些显存优化工作第一个工作是VTensor提供了一种Pageless的Tensor管理框架提供更大的灵活性第二个工作是LayerKV主要是通过LayerWise的KVCatch管理来优化手自延迟并在满足特定的SRO情况下提升整体的吞吐我将通过如下三个小节来进行分享第一小节会来分享Bugground and Motivation在23年6月VM Page of Tension推出以来因其在KVCatch管理效率上的提升受到了业界的广泛关注使得推理效率有极大的提升但我们发现其引入了一个比较大的限制因其配置的KVCatch管理显著区别于之前顺序寻止的方式是通过一个PageTable将多个不连续的虚拟地址关联起来需要将这个原数据信息传入到Attention Kernel中使得每个Batch可以找到其对应的实际的KVCatch的地址这种方式将KVCatch的管理以及Attention Kernel的实现紧密偶合在了一起使得业界很多的Attention Kernel如果不针对KVCatch的循止进行改造是无法使用配置的管理方式业界的FlashAttention框架在经过比较长的时间才接入配置的管理方式说明对Attention Kernel的改造复杂度和难度较高会影响推理框架的迭代效率这里我们将PageTableTension的部组概括为两个主要的方面第一个部组主要是将显存管理和Attention Kernel的实现紧密偶合在一起而Attention Kernel针对Page的实现需要进行额外的改造、适配和调优复杂度较高、周期长业界涌现了众多新的Attention Kernel如果针对每个Attention Kernel都进行单独改造会引入很多不必要的工作降低推理工作的迭代效率第二个不足的方面是当前PageTable是通过将一个大的连续内存块在旗上划分不同的block来管理具体的token映射需要提前将一大块显存映射好在一些推理混布场景下会导致开启Valm的实力占用了大量的KVCash空间使得另外的进程无法启动或导致性能很差但推理实力的请求具有潮汐现象会有比较大的波动而一直占用着大量显存没有被使用会导致整个集群的很多显存处于闲置状态没有被有效地利用起来基于前述问题我们设计了新的Tensor管理结构GEO Tenshen Kernel的实现与KVCash的管理这里主要有三个小目标第一是需要能和PageTenshen一致解决KVCash中的碎片问题能够支持推理框架容纳更多的请求提供更大的吞吐第二是同时要能保证不影响Attention Kernel的性能最后可以很方便地根据不同的推理场景集成多种定制的Attention Kernel例如针对吸收和量化场景可以方便地对接业界多种新的Attention框架我们在进一步分析推理请求中的调度策略时发现当请求越长或负载越大时KVCash很容易打满此时请求的首次延迟会变得很大因为首次延迟主要有两部分构成prefill实际执行的时间以及请求的排队时间排队时间主要依赖于前续的请求处理时间这个是很不容易预估的导致线上很容易出现TTFD的超时使得这个请求失败我们发现核心原因主要是当前Verm中在prefill阶段需要按照Promit请求将显存一次性分配如果显存不足如果显存不足需要等待前续请求处理完成之后空出一定量的KVCash之后才可以继续处理高负载时会导致TTFD急剧增长但实际上prefill阶段产生的KVCash是不会被访问的这使得我们可以针对这部分prefill阶段产生的KVToken进行更灵活的调度来减少prefill请求的排队时间接下来我将分享我们VTensor以及LearKV的实现部分这里是我们VTensor整个框架的示意图包含了很多调度以及管理的组件VTensorManager是高效显存管理解决碎片的核心组件在其内又包含了三个小的组件VTensorSchedulerVTensorPort以及VTensorOperationVTensorScheduler通过VTensorPort以及VTensorOperation来实现有效的显存调度VTensorOperation通过抽象VM的API来实现具体的虚拟显存和物理显存的管理VTensorPort则缓存了多种不同状态的VTensor根据VTensorScheduler的调度决策返回合适的VTensor给上层不同的请求VTensorScheduler从调度器接收指令请求根据原信息创建特定策略原信息包含多个数据结构如Hassey表用于记录和跟踪内存相关细节如未分配的物理块和VTensor的状态基于这些策略VTensorScheduler通过VTensorOperation来执行分配或释放内存包括GPU上一步执行执行结果返回到VTensorPort中VTensorPort保存了所有的VTensor包括物理块与虚拟内存之间的虚拟内存地址映射信息也负责更新VTensorScheduler中的语言信息这里我们来看VTensor内部的实际实现主要是由三个部分来构成最上面红色部分是虚拟地址并不实际占用物理现存会按照模型最大长度来分配绿色部分为PhysicalHandle在Scheduler层面来帮助判断每个VTensor实际对应的物理块的数量以及可继续承载的Token数量当判断物理块不够时会通过VTensorOperation来创建新的物理块并映射到对应的虚拟地址上即图中蓝色的PhysicalTrunk因为VMM接口中提供的最小的物理块的力度是两兆币如果按照VMM中每层layer使用不同的Tensor来存放每层不同的KVCatch会导致每个请求每层最后的一个两兆币的物理块可能有比较大的浪费所以我们为了解决这个问题采用了新的Shape将KVCatch的管理布局成Batch SizeSiquest LengthNumber LayerNumber HeadHead Dimension五个维度即将Layer维度加入到KVCatch的管理中这样每个两兆币的块就会存放多层的Token减少每个请求每层最后一个两兆币的浪费从右图可以看到通过将Layer维度加入到KVCatch管理中可以减少两兆币块内部的碎片为了支持更丰富的场景以及更灵活的显存管理我们抽象出了很多新的底层的管理接口这里以三个例子来举例Allocation Module这里面会初始化新请求会使用PeerLock和VLock来分配必要的物理块和虚拟内存地质然后VTensorOperation将使用Map将虚拟内存和物理块映射关联起来组合成一个完整的VTensorExtend Module对应Decode的过程中动态生成新Token动态扩展策略对解决KVCatch显存碎片很关键为此VTensor内置一些优化来提高效率包括预分配和异部分配物理HandlePrefixRecord和PrefixMatch对应多轮对话场景注意这里没有新分配物理Handle而是更新现有的显存指针我们调研业界工作发现Prefill阶段计算耗时相比于PCIe的传输耗时来说还是要大一些的然后我们根据当前使用的多种型号的GPU卡分析了计算耗时和传输耗时的比例发现将KVCatch传输到CPU的耗时总是可以被Prefill阶段的计算掩盖的这使得我们可以在Prefill阶段当遇到显存不足时可以临时将Prefill阶段产生但不会使用的KVCatch临时传输到CPU上并且不会影响Prefill阶段的计算效率基于前述的观察我们设计了一个新的LayerWise的KVCatch管理框架可以支持封层申请KVCatch的显存减少整体显存的依赖并在Prefill阶段支持封层将产生的KVCatch临时传输到CPU上在Decode时分层PrefetchKVCatch我们在Scheduler层面也增加了一些新的调度策略基于显存压力和对列的请求长度动态决定是否激活LayerKV我们在这里也总结了业界多种框架内对于KVCatch管理KVCatch Offloading以及SLO Aware Scheduler的支持情况可以看到在KVCatch管理方面业界的VRM、GIS-SERV以及DeepSpeed FastGene都只支持Request-wise的KVCatch管理而我们可以支持LayerWise的KVCatch管理在KVCatch Offloading层面VRMod支持的也是Request-wise的KVCatch的Uploading而DeepSpeed以及DeepSpeed FastGene再还不支持在SLO Aware Scheduler层面VRMod上前还不支持而DeepSpeed FastGene以及DeepSpeed支持静态策略而我们是可以支持动态策略根据显存压力对列请求长度来灵活地调度当前的prefuel请求最后一小节我将分享我们在推理优化工作中的一些评测结果集成VTensor之后既可以有效消除KVCatch中的碎片问题并且还可以根据实际的请求复载按需使用物理显存为混布场景提供了更灵活的物理显存管理方式允许多个进程按需使用物理显存使得可以根据不同任务的负载动态在多个任务之间灵活地切换物理显存的占用量潜在可以提升整个集群的显存利用效率使用VTensor仅需几行代码的修改即可集成新的Eltension框架并且消除KVCatch中的碎片我们已经针对业界SATA的一些Eltension框架进行过验证例如FlashTension2、3、PyTouch中的FlexEltension以及一些希书Eltension等我们将业界SATA的一些Eltension Kernel叠加VTensor的实现做了很多的性能评测从结果看叠加VTensor之后相比Native的实现方式性能基本持平相比使用配置的方式的Kernel在一些场景下会略好一些并在4卡A100上端到端评测了E34B的模型从结果看当叠加VTensor以极低的代价快速集成FlashTension3Kernel之后效果相比VM的PageTension要好不少相比FAR的实现来说也略好一些在LayerKV的评测中我们设定SRO为首次延迟小于1秒生成速度大于每秒10个Token在这种情况下我们评测发现LayerKV相比基线VMQPS可以提升1.3倍如果我们设定SRO为首次延迟小于1秒生成速度大于每秒7个Token在这种情况下我们评测发现LayerKV相比基线VMQPS可以提升2倍以上在生成速度不影响的情况下平均首次延迟可以降低1.8倍而P99首次延迟可以降低5.9倍我们同时也评测了更多的模型在QPS影响在3%以下时首次加速UP2可以到17倍最后谢谢大家今天的聆听谢谢大家