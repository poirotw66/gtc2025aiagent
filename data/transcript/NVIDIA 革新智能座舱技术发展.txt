大家好我是英伟达的解决方案架构师查理今天的内容主要包括三个部分第一部分是全链路加速生成师AI应用的一个开发第二部分由我的同事Jerry来介绍大模型推理框架TRT-LM第三部分由我的同事Amber来介绍端侧的一个部署那么首先我们来看一下全链路的加速那么我们知道生成师AI在智能座舱中应用广泛目前除了像应用最多的智能交互语音控制聊天机器人事实上还能够在自动驾驶系统中做出帮助自动驾驶做出更好的决策那更进一步的在舱内我们还可以通过面部识别情感分析对舱内做出一些个性化的推荐除此之外还有内容创作和生成等娱乐系统在做舱内目前也非常流行那如何基于英伟达的技术站构建生成师AI的一个应用呢英伟达提供了一套叫NEMO的这个平台这个平台是一个端到端的全流程的一个解决方案如图所示这是构建生成师AI的一个流程图我们从最左侧的数据处理开始到分布式训练到模型的定制化再到右侧的模型的推理加速检索增强生成RAG以及模型上线前的安全护栏这样一个全流程的一个方案呢其实是覆盖了开发过程中的各个阶段同时呢我们在每个阶段也都集成了当前业界最为高效的一些优化点能够帮助我们用户和开发者能够更快速的更高效的来迭代自己的大模型下面呢我会针对其中的两点做一个展开一个是Curator一个是FP8那我们首先来看什么是CuratorCurator呢是一个数据处理的一个Pipeline它包含了文本预处理图片预处理还有视频处理那这个图琐事是一个文本预处理的一个Pipeline从最左侧到最右侧我们可以看到首先是会从互联网或者说本地来获取数据拿到数据之后呢我们先会做一个基本的一些数据清洗比如说解决一些特殊字符的编码啊格式等问题接着呢我们会通过一些启发式的规则啊来筛选一些高效质量的数据然后我们会进入到最花时间的一个阶段叫去虫也叫Deduplication那在这个阶段呢我们会引入英伟达的Rapids这样一个组件利用Rapids中的一些数据处理加速库然后引入GPU去加速我们的这个数据去虫去虫完之后我们再经过基于模型的一些过滤将将数据中的一些个人信息像姓名啊身份证号啊做一个去除经过上述一系列的操作呢我们最终会形成一个高质量的数据我们最终会形成一份可以直接使用于训练的这样一个高质量的一个数据机那如果处理图片呢整体的Pipeline也是类似的但是其中会用到一些视觉语言的模型像比如基于Clip的Embedding基于模型的这种安全检查分类还有这个图片美观分类这样的模型当然也有这个图片语意义上的一些去虫同时我们也提供了一些图片的一些caption的能力啊用这个VLM的模型来对图片做一些caption这些处理的过程呢我们也都接入引入了GPU的一些加速像利用Dali和Rapids等等让我们整个图片的预处理速度更快除了刚刚介绍的文本和图片我们还有视频的Pipeline视频的Pipeline呢也更为复杂像视频中可能会涉及一些视频的Split等等这里就不再进一步展开刚刚讲的Curator是准备数据那准备完数据之后呢我们要进行训练下面进行下面就要介绍一下训练中一个很重要的IPIP8训练那为什么要讲IP8或者说IP8的优势是什么一个重要的原因就是我们发现对于计算密集型的算子比如GIM的这种矩阵运算IP8的TensorCore相比BF16或者说IP16能够提供两倍的这样一个算力从而缩短训练时间那么对于仿存类型的一些操作呢IP8的格式呢需要的存储资源也更少同时可以达到节省显存的目的所以说IP8是我们这个训练过程中的一个重要一个重点那什么是IP8我们可能比较熟悉IP16或者BF16的训练它是用16个比特位来表示数据那IP8呢正如字面的意思它就是用8个比特位来表示一个数据目前IP8的支持主要在开源的Transformer Engine库这个里面去支持目前有两种具体格式一个是E4M3一个是E5M2其中E表示指数位M表示尾数位E5M2呢由于有更多的指数位它能表示的数据范围就更大E4M3呢有更多的尾数位它的数值的精度会更好那刚刚提到IP8能够加快我们的训练速度但是呢我们大家也知道就是说IP8也会对精度带来一个挑战那我们先回想一下IP8的这个混合精度是怎么做的IP8呢它其实是有两种模式一个是O1一个是O2O1的话是会针对算子维护一个白名单白名单中的算子呢会以低精度去计算如这个GIM的矩阵称和Count这些算子那么其他的算子呢仍然会用高精度来计算和存储那O2的话我们会把这个weight转成IP8然后一些特殊的操作像Softmax这些还是会用IP32同时呢我们也会保留一份这个IP32的weight来解决这个模型训练后期梯度值过小的这样一个问题那IP8的实现呢我们可以认为是O1和O2的一种混合的一个模式那这如图所示这个图呢就是Transformer Engine中在训练过程中全连接的一个前向加反向的操作其中红色呢表示高精度数据绿色的这些呢表示低精度数据其实无论是前向还是反向啊我们可以看到整体的训练精度呢还是以BF16为准为主但是呢我们会维护一个白名单啊白名单中的这些OP呢会以IP8来计算比如这个LinearLinear中的这个局限程和记录那需要说明的是像FMHA也就是Multiheadattention目前在功能上是支持IP8的啊但是呢在实际训练中我们通常还是会用高精度来做这个FMHA从而保证更好收敛性那在训练过程中呢我们这个前向和反向会采用不同的数据精度啊前向呢我们会用E4M3因为前向它的数值的动态范围呢不太大用E3M3能够提供更好的一个精度反向的话由于它需要更大的一个动态的一个数据表示范围所以我们常常会用E5M2啊这个流程图中呢蓝色的框就表示了这个16位精度到8位的进度的一个转换的一个过程那这个管上转换的过程我们叫CAST啊CAST的过程呢我们为了避免溢出我们常常会采用Tensor Scaling Per Tensor Scaling 的这种方式啊我们通过乘上一个这种缩放的系数啊将16位的数据安全的CAST到这个8位那如何确定这个Skilling的这个因子呢首先我们会得嗯在计算到一个高精度的结果之后呢我们通常会用一个类似于这个Touch.max啊求最大值的这样一个操作来找到Tensor中的一个最大值通过这个最大值呢来计算这个缩放因子然后呢再量化高精度的这个Tensor呢为IP8的输出但是这种问这种方法呢有个问题就是我们Tensor的这个shape通常都比较大啊我们没有办法把这个Tensor呢全部都放到GPU的这个share memory 中去计算所以这个过程我们往往会借助Global memory进行这个数据的一个中转那这就会带来一个额外的一个仿存开销啊那如何解决这个问题呢我们可以这样想啊假如说我们提前知道这个Skilling Factor的值啊那我们量化的时候呢我们就可以直接提前再在share memory 上去计算啊这样的话我们就不需要这个等这个计算最大值的这个过程它是避免这个仿存的开销啊那用这种方法我们把这种方法呢就叫Delay Scaling啊所以说Delay Scaling也是我们Transformer Engine中默认的一种Skilling的一个策略那如图右边这个锁势啊这就是我们Transformer Engine这个Delay Scaling的这个API的一个接口啊用户和开发者可以直接去使用了解了理论部分呢我们简单看一下IP8在实际网络中是如何运算的啊图中左部分左侧部分呢是一个激活的一个OP啊右侧呢是一个右侧是一个TensorCode的一个OP我们先看左侧左侧呢它输入是一个IP16啊输出是一个IP8啊它的右侧的话它的输入是IP8输出是IP16啊这两个OP呢可以类比到Transformer Layer中的LayerNorm和全连接啊我们对左侧这个OP来说呢我们看它的输入和计算呢都是高精度的啊我们当我们拿到一个高精度的结果之后呢我们会做两件事啊第一个是说统计当前Tensor的一个最大值啊我们并把它追加到我们的这个HistoryBuffer中第二个的话我们会从HistoryBuffer中选出一个最大值作为当前Tensor最大值的一个估计然后来计算这个缩放因子进而呢就可以进行一个这个IP16到IP8的一个转换那对于右侧这个TensorCodeOP呢它的这个输入呢已经是IP8了啊然后它的权重呢也已经是使用这个DL Scaling已经量化到IP8的方式了那这样的话这个右侧的这个TensorCodeOP的话它的整个计算都是一个IP8的计算啊所以说它是一个计算得到的一个结果是高精度的啊在计算结果输出之前呢还需要一个反量化的过程那IP8训练的性能怎么样呢性能大概包括两个部分啊首先是收敛性如图所示啊是一个LAMA38B的一个收敛曲线图我们发现基本和BF16是能够保持一致的我们在Pile数据集下训练了300B的Token啊IP8相比BF16的这个Loss的Diff呢在能够小于0.001然后右侧的话我们可以看到是IP8吞吞的一个表现我们看到我们在LAMA370B这样的模型上H20的IP8相比BF616有1.74倍的加速比啊这个加速比还是很不错的那在这种情况下我们可以看到和A100去比基本有10%的左右的性能提升最后一部分呢我简单给大家更新一下Deep6的内容我们知道啊Deep6它是一个671B的一个MOE的这样一个模型它的参数量是很大的啊在这种规模的参数下呢至少需要七八百G这样的一个现存啊目前对于这个H2096G的H20或者说H100啊实际上部署下来都至少需要两台那对于141G的这个H20部署起来就会好很多啊如果我们在加速之间的141G的NASK在NASK app的这种情况下呢基于英伟达NASK的这个框架我们在输入1024输出256的情况下吞吐可以达到700多TOKENS每秒啊输入256输出1024的话吞吐可以达到1477TOKENS每秒好我要介绍的内容就这些好,我要介绍的内容就这些,谢谢大家大家好,我是NVIDIA开发与技术部工程师,实际接下来将由我为大家介绍NVIDIA在数据中心测对大模型推理优化加速的解决方案以下是本次演讲的一个大纲首先我会介绍NVIDIA针对于数据中心大模型推理的解决方案框架Tensor RTLM以下我会简称为TRTLM并为大家介绍TRTLM的一些重要特性和功能在第二部分我会为大家介绍在生产环境当中使用TRTLM部署大模型的具体步骤然后我会为大家介绍两种最为常用的多模态大模型第一个是视觉语言大模型VLM以及扩散模型DIT的模型推理优化以及它们在TRTLM上的性能以下是TRTLM的一个简单介绍TRTLM是英伟达针对大语言模型推理所开发的一个加速框架它有两个比较重要的特点第一个是它领先的性能使用TRTLM能够使得大模型推理在H100平台获得相比于A100平台超过4倍的一个性能提升并且使TCO下降了三倍第二大特点是它简单易用的Python API用户使用短短几行Python代码就可以部署跑通一个大语言模型的demoTRTLM针对大语言模型推理的场景做了非常多的优化以下是具有代表性的几个特性第一个是它的高效RuntimeTRTLM的Runtime整合了高效的Batch Scheduler用于推理的请求打包和P处理还有着非常高效的KVcache设计以提升KVcache读写的效能以及高效的显存管理机制第二个是它深度优化的Colon包括了Attention计算的Colon矩阵层的Colon以及其他自定义的高性能Colon其他自定义的Colon其他自定义的Colon其他自定义的Colon其他高性能Colon第三个是TRTLM支持了非常多不同种类的大模型比如Decoder OnlyEncoder Decoder或者是多摩泰大模型并且这些大模型都十分易于拓展第四个特点这些TRTLM支持了许多自定义的Workflow包括Multi-Lora以及Logic Processing等等第五则是TRTLM支持了非常多高级的量化方法这些量化方法包括FP8FP4SmoothQuantIntel Only的量化等等第六则是TRTLM集成了非常多领先的研究成果包括Speculative Decoding或者说是PD分离在生产环境下使用TRTLM部署大模型主要分为以下几个步骤首先第0步是获得大模型的Checkpoint文件用户可以使用NVIDIA NEMO训练框架训练出的Checkpoint也可以使用从Hugging Face, Pedal Pedal等平台下载下来的模型全种然后用户可以通过Convert Checkpoint这个脚本将准备好的模型全种将准备好的模型全种进行处理转化为适用于TRTLM的全种在转化过程中这些全种主要会被进行两步处理首先根据用户所指定的TP, PP, EP等并行方式TRTLM会将全种切换至更加上去更加上去的模型全种可以使用于带来的模型全种可以使用从Hugging Face, Pedal Pedal等平台下载下来的模型全种然后用户可以通过Convert Checkpoint这个脚本将准备好的模型全种进行处理转化为适用于TRTLM的全种在转化过程中这些全种主要会被进行两步处理首先根据用户所指定的TP,PP,EP等并行方式可以分为多个小块,用于多个GPU上的部署然后TRTLM会根据用户所指定的量化方法对模型全种进行量化转化为更低精度的全种当用户准备好了转化完成的全种后则可以进行第二步,构建引擎用户使用TRTLM Build这个命令可以将处理好的全种编译成为经过TRTLM深度优化后的二进制 Engine最后用户可以通过TRTLM serveTreton或是Python等方式将构建好的引擎部署到多个GPU上以实现端到端的部署这样生产环境的部署就全部完成了用户就可以向服务器发送请求来测试和使用大模型了接下来我将以千问VL这个模型为例为大家介绍多摩泰大模型是如何在TRTLM中进行部署的首先先简单介绍一下千问VL这个模型千问VL是一个非常典型的视觉语言大模型也就是常说到VLM这一类的模型主要用于多摩泰信息的理解比如图片或是视频的理解千问VL这个模型主要有两部分组成第一个部分是视觉编码器VIT这个部分主要是将图片信息转化为Embedding并且通过一个projector将其映射到文字部分的Embedding相同的空间中第二个部分是大模型的主体部分也就是千问大模型大模型主体部分负责将文字与图片的信息一同处理理解这些信息并且输出结果在实际的部署过程当中TRTRM会将这两个部分分开为单独的引擎以供用户使用更加灵活的多卡部署方案这一点我会在后面进行深入讲解对于大模型主体部分由于它与普通的单模态大模型拥有完全一致的结构TRTRM则可以利用上所有在单模态大模型上已经实现好的相关的优化例如高性能的Kernel,KVCache以及量化方式来对多模态大模型进行一样的加速右边的图是端到端推理过程当中完整的一个workflow首先预处理阶段文字会被tokenize图片则会被rescale或者reshape然后图片信息将会被位入到千问VL的视觉编码器所构成的TRT Engine输出的结果与tokenize之后的文字信息一同被位入到千问大模型的主体部分的TRT Engine进行来推理输出的结果通过一些后处理转化为用户可以读的文字信息这就是一个典型的多模态大模型在TRT LM当中的部署workflow了接下来我会介绍视觉语言VLM大模型部署的两个重要的优化点首先第一个是灵活的分布式部署前面提到过TRT LM为多模态部分与大模型部分提供了分离式的部署方案这个部署方案的优势在于用户可以根据实际的场景来调整不同的分布式并行设置比如大家看右边上方第一种部署方式这种方式下视觉部分和大语言模型部分被部署在了同一个GPU上这个方案是对于小型的VLM最优的方案因为它不会带来任何通信的开销但是前提是GPU的显存足够存下整个模型并且有富足的空间存储激活和KVCache然后对于视觉编码器部分非常小并且语言大模型部分比较大的VLM来说可以对语言大模型部分进行一个分布式部署例如右边图上所示的张亮并行方案另外用户还可以使用一个混合的部署方案例如对于体积较小的视觉编码器部分进行一个数据并行体积较大的语言大模型部分进行一个张亮并行这样能够最大化GPU的利用率当然根据具体场景用户甚至可以将视觉编码器和大模型部分完全分开的部署到不同的节点甚至不同类型的GPU上以实现性价比最高的并行方式第二个重要的优化点则是对于VLM大模型本体部分的优化包括各类已经集成进TRTLM的Kernel优化Runtime优化以及可供用户灵活配置的模型的量化等等下面展示了一个多么太大模型Lavanex它是一个典型的VLM模型它在TRTLM上部署的性能数据大家可以看到通过TRTLM各类优化方法的加速用户在H20GPU上对各种体积的Lavanex模型都可以实现比起原生Hugging Face Transformers超过两倍的性能提升而使用Int8 Weight Only对权重进行量化的性能提升比起原生Hugging Face Transformers超过两倍的性能提升而使用Int8 Weight Only对权重进行量化之后则可以进一步提升性能最高能获得超过2.8倍的提升这是因为模型的Int8 Weight Only量化使模型的权重体积缩小了一半在计算过程当中模型的仿存量得以减少让模型的计算仿存比更高获得了加速并且更小的模型体积让更多的GPU显存能够用于激活和KVCache的存在这样也提升了整体的吞吐接下来我会介绍另一种多模态大模型DIT模型的相关优化方法DIT模型全名又称Diffusion Transformers它是一种扩散模型广泛应用于各类生成的场景当中例如图片的生成以及视频的生成对于DIT模型的模型一个重要的优化点就是它的分布式部署方案在TRTRM当中DIT模型有两种分布式并行的策略第一种是TP也就是张亮并行模型将会沿着Hidden Dimension的维度进行切分以实现多卡的并行第二种则是CP也被称为序列并行或是上下纹并行经过实验测试我们发现使用4卡的TP能够为DIT XL这个模型加速2.5倍而改为使用4卡的序列并行加速并行使用4卡的TP能够为DIT XL这个模型加速2.5倍而改为使用4卡的序列并行加速并行加速比能够扩大到3.1倍以上这是因为DIT模型的序列长度往往非常长动辄上万TOKENS尤其是在视频生成的场景当中在序列维度进行切分能够更有效的降低单卡的计算负担并且比起张亮并行序列并行的通信开销更少因此达成了更高的加速比除了序列并行之外另一个对于DIT模型非常重要的优化则是FP8的量化在TRT-LM当中我们支持了对DIT模型的训练后量化处理用户通过一个简单的脚本就可以将FP16的权重量化为FP8的权重不同于INTA-BAR-WIT ONLY的量化不同于INTA-BAR-WIT ONLY的量化FP8量化不仅将模型的LINEAR成权重缩小了一半还将对应层的激活也从16位变成了8位也缩小了一半这样的最重要的优化最重要的优化最重要的优化最重要的优化在TRT-LM当中我们支持了对DIT模型的训练后量化处理用户通过一个简单的脚本就可以将FP16的权重量化为FP8的权重也缩小了一半这样FP8量化不仅减少了模型所需要的GPU显存空间还可以利用上ADA以上架构GPU的FP8 Tensor Core计算单元进一步加速整体的推理如右图所示使用FP8量化相比于FP16可以使DIT XL模型的端到端推理延迟降低35%配合上4卡的序列并行可以达到总计4.4倍以上的性能提升好了这就是NVIDIA对于多模态模型推理在数据中心的相关解决方案那么在车端如何进行大模型的推理优化呢下面请我的同事刘琦来为大家介绍端测的大模型软件部署加速方案Hello大家好我是来自NVIDIA TSE的Amber接下来我将为大家介绍一下我们团队在Thor上针对端测大模型推理加速所做的一些工作首先呢我想简单回顾一下Thor的硬件架构Thor是NVIDIA基于Blackboard架构的最新一代车载芯片那么随着大模型的飞速发展不管是在智能座舱还是智能驾驶的场景下车端对于算力内存以及带宽的需求都在不断的提升那么和上一代Orig相比SOR在大模型方面的能力还是有非常明显的优势的首先在计算方面SOR搭载了第四代Tensor Core和第二代Transformer Engine是它不光拥有更大的算力同时也添加了对于两种新的推理精度FP4和FP8的支持能够进一步的提升计算的吞吐量而在仿存方面呢SOR拥有更高的带宽和更大的L2缓存能够进一步加速数据传输的速度并且减少仿存的延迟那这些特性都为大模型在车端的推理优化提供了非常强大的硬件支持使得大模型在车端能够展现出令人满意的性能那么我们该如何将云端训练好的大模型顺利部署到车端呢和传统模型一样Lidia基与Kuda和Tensor RT为客户提供了一套云端与车端一体化的开发环境正如刚刚我们同事所介绍的在云端的部分客户可以使用Nemo等工具进行大模型的训练微调以及优化那在训练完成以后就可以通过DriveOS和相关的工具链将大模型部署到车端的应用场景当中从而实现从运营到车从训练到部署的一个无缝对接为此我们从Thoric开始为DriveOS专门研发了一套全新的大模型推理框架它的全称叫做DriveOS LLM SDK也可以简称为DriveLM那这个SDK包含三个核心的部分首先在算子层面我们针对大模型中占比最大的两种算子Attention和矩阵程开发了相应的优化加速kernel并且通过Tensor RT Plugin的方式集成到了SDK当中那在中间层我们构建了一个轻量化的大模型推理Runtime能够为模型推理提供高效且稳定的环境最后在应用层我们提供了一些主流模型的端到端推理的呃让客户可以直接使用并且也能够以此为基础来拓展和部署其他的模型那这是DriveOS LLM的架构图我们可以看到在前端模块我们会把Hugging Face或者Pytorch模型导出为Omix合式并且会用到NVIDIA Model App工具进行量化那在中间模型的部分我们将所有的模型分为LM纯语言模型和Multi Model多模态模型两个大类那在每个大类下又细分出多个子模块比如说包含Tensor RT Engine的Builder一些基础的对话应用精度评估以及性能评测相应的脚本那这些不同的模型应用都会调用到我们后端核心的推理模块除了刚刚有提到的Tensor RT Plugin以外还有Decoder Sampler和Tokenizer这几个主要的部分那最终呢以上所有这些不管是模型还是推理的模块在底层都会调用到Tensor RT的API来执行推理任务那么我们为什么要开发一个新的工具呢或者说Drive LM有哪些独特的优势可以分为以下几点首先Drive LM是专为车端量身定制的它基于C++进行开发能够满足我们车规功能安全的相关要求并且和其他更加适用于大卡的工具链相比比如说像Tensor RT LMDrive LM更加的轻量化环境依赖非常的少占用的内存空间也很小其次呢Drive LM的代码对于客户是开源的代码结构也非常的简单比较容易上手尤其是对于之前比较熟悉Tensor RT整个模型部署流程的客户来说同时我们提供了非常丰富的参考模型可以直接使用而对于有更高功能需求的用户也可以很方便的进行二次开发整个框架的灵活性比较高第三呢尽管Drive LM的功能相对比较精简我们依然支持了对于端侧大模型推理比较重要的一些功能比如说低精度的量化投机采样LORA动态Batch等等那这些功能能够覆盖当前绝大多数的车端使用场景最后呢在性能方面Drive LM会针对车端比较常用的一些模型和配置进行专门的优化比如说像0.5B 1.5B 3B这样一些大小的模型那这些模型在针对大卡的一些推理框架上往往是优先级比较低的因此往往也不会被专门的优化那在具体的模型支持方面在我们在2月份发布的Drive LM 0.0.2版本当中呢我们已经支持了5种LM模型以及1种多磨泰模型分别是LAMA3LAMA3.1LAMA3.2千问2千问2.5以及千问2BL那这些所有的模型我们都支持4种推理精度分别是INTS4WIT ONLY量化也就是W4A16NVFP4FP8以及FP16这些推理精度能够帮助客户在不同的精度和性能之间找到一个最佳的平衡那未来呢我们也计划支持更多的模型比如说千问2.5VL千问2.5VLEnternVLDeepSeq蒸馏的模型等等同时呢我们也期望通过不断的完善我们的参考模型文档以及核心功能模块提高整个工具链对于新模型的适配度和灵活度让客户能够自己部署个性化的模型那在具备了丰富的功能和广泛的模型支持的基础上呢大家肯定也十分关心大模型在FOR上的性能究竟如何那这是FORU上面NC4W4A16的一个性能数据我们可以看到在Batch Size等于1的情况下千问27B的模型的Decode速度能够达到54Token每秒而1.5B和0.5B的模型分别可以达到169和329Token每秒那这样一个数字相比叫ORINX是可以有1.3倍到1.5倍的提升效果那由于我们所有的这些数据指标数据指标都是没有量化LMHead的所以说模型越大LMHead的占比越少在FOR上的加速提升也就越明显并且大家知道大模型的Decoding阶段属于仿存密集型而FOR的带宽是ORIN的1.3倍所以说我们能够取得1.3倍以上的一个Decoding性能的提升已经在硬件层面是达到了比较理想的一个状态那如果我们的客户想要进一步的大幅度的提升Decoding性能的话我们会推荐部署投机采样的模型不过在Preview阶段呢当前版本的Intes性能仍然有一定的提升空间我们也在优化当中那除了大家比较熟悉的W4A16以外在FOR上另外一种推理精度更加值得关注那就是NVFP4这也是NVIDIA未来在大模型推理方面会重点推广和支持的一种精度格式那在这里由于时间的关系我简单的介绍一下NVFP4是NVIDIA定义的一种E2M1的FP4精度格式并且它采用W4A4的一个量化和计算方式那为了在W4A4的情况下依然能够有比较高的精度我们做了一些特殊的优化处理首先在量化上我们会进行动态的PerBlock量化并且这个Block Size等于16比较小的一个Block Size呢能够更好的保留精度然后我们会对所有PerBlock的量化参数进行一个静态的二次量化那在计算方面呢NVIDIAF4采用了FP4 MMA和FP3 II累加那这样的设计能够使得NVIDIAF4格式相较于其他的FP4格式拥有更高的精度那在我们对LLM做的精度测试当中NVIDIAF4已经证明了它的精度调点是很小的而在性能方面呢NVIDIAF4的优势是非常大的相较于FP8它能有两倍的带宽和吞吐量提升那相比于INT4NVIDIAF4由于是W4A4的一个计算方式并且它有TensorCore层面硬件加速指令的支持所以说相比INT4的W4A16也是能够有明显的提升尤其是在计算密集型的prefuel阶段提升效果会非常的明显那这张表呢就是我们对比了在Batch Size等于1输入长度是1024的情况下NVIDIAF4和W4A16INT4在F4U上的一个性能表现我们可以非常清楚的看到呢NVIDIAF4在prefuel阶段的性能提升非常大那在0.5B的模型上NVIDIAF4比INT4可以有60%左右的提升那在7B的模型上这个数字可以达到80%左右那虽然说这个其中有一部分的原因是由于我刚刚提到过的我们目前的INT4的prefuel阶段的性能还没有达到最优还在优化当中但是即便是这样的话即便是在优化以后INT4和NVFP4的性能也依然会有非常明显的差距那需要说明的是呢在Decode阶段目前我们NVFP4的性能和INT4还有一定的差距那我们正在实现相关的NVFP4 Kernel预计优化以后可以达到和INT4相当甚至更好所以说总结来说NVFP4在prefuel阶段比INT4更快在Decode阶段我们预期可以达到和INT4相当的性能所以我们非常推荐客户在Fuel上可以对NVFP4的推理做一些尝试尤其是在一些输入长度比较长的然后输出长度比较短这样的一些场景上面prefuel阶段的加速优化就变得更加的重要最后呢我来简单介绍一下DRIVE LM部署大模型的步骤非常简单只有三步第一步把模型转换为ONIX第二步生成Tensor RT Engine第三步使用DRIVE LM进行推理就可以在Fuel上成功运行起来了那这个流程和我们部署一些传统CN和N模型是非常相似的那么从下个版本开始呢DRIVE LM会正式的集成到DRIVE OS当中在未来大模型无疑是我们在Fuel上会重点支持客户发展的方向我们会持续的迭代系统和工具链也希望让大模型在车端跑得更加的快更加的精准同时也更加的便捷那我今天的分享就到这里谢谢大家