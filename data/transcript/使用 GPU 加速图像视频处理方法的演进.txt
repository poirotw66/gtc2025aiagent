大家好,我是来自火山引擎视频云的朱一凡今天我给大家带来的主题是使用GPU加速图像视频处理方法的引进今天的主题分为以下几个章节使用CUDA加速传统算法使用Tensor RT加速基于CNN的神经网络最大化GPU的使用率调度器执行模式以及优化VLM视觉大模型背景介绍火山引擎视频云提供了多种视频处理能力主要包括视频转码视频增强视频分析支持了抖音今日头条西瓜视频番茄小说等社门应用在这里火山引擎视频云多媒体实验室提供了各种视频图片的增强以及分析算法这里有基于CV的传统算法也有基于CNN的神经网络算法我们使用英伟达GPU来加速部署这些算法为上层应用提供支持在这里我们使用CV来加速图像处理传统算法使用Tensor RT来加速CNN神经网络模型使用硬件编辑码来加速视频编码的整体Pipelan在调度层面我们使用了异步处理执行器处理以及分块处理来满足不同业务的需求接下来就为大家展开介绍使用CUDA来加速传统算法传统视频处理算法例如色彩空间转换图像缩放边缘检测HDR等算法仍在广泛应用我们使用了CUDA加速了上百个此类传统算法最少有40倍的性能提升这些被CUDA加速的传统算法在实时转码点播视频增强直播视频处理图片处理等场景中得到了广泛应用我们除了直接应用在视频图像处理过程中还有一部分作为Tensor RT的插件例如高丝模糊边缘检测等算法构成了全GPU流程的处理链路避免了主机端到设备端的数据传输提高了计算效率这里是一个HDR算法的样例该算法需要根据直方图进行色彩映射该过程属于单线程操作因此这里最开始使用了CUDA动态并行特性在CUDA 9.0后我们将其升级为CG调度在T4 GPU上整体算法速度相比CPU而言提升了200倍左右另外要不额外补充的一点是本次主题中所有的图片均由我亲自拍摄不存在版权问题这张图照片拍摄于甘肃甘南藏族自治州也欢迎大家交流摄影信德这里展示一个GPU实施水印转码的案例该方案用于抖音视频下载场景当用户提交下载请求时服务端实时地提供带有水印的下载视频我们使用GPU的硬件编辑码能力对视频进行解码再将视频从硬编码输出的NV12格式转换到RGB色彩空间根据视频的大小调整水印的大小和贴图的位置并将最后的结果交给GPU进行硬编码该方案在T4GPU上处理1080P视频速度可达500fps以上常见的抖音短视频均在几秒内可以完成转码并输出使用GPU实施转码节省了大量的预处理计算资源每年节省的储存资源高达若干PB接下来我们来讲使用Tensor RT加速增强模型这里我们主要会讲到一些模型转换的心得来减少显存的开销和性能提升火山引擎视频云多媒体实验室针对不同场景不同质量的视频图像研发了上百个相关的增强类模型将算法模型部署到生产环境为了减少计算成本需要使用Tensor RT进行推理优化从PyTouch模型转换到Tensor RT我们使用了两条路径第一是导出Onix将其转换成Tensor RT另一种是基于开源项目Touch2T RT我们扩展了该项目开发了几十个自定义插件将PyTouch模型转换到Tensor RT转换后的模型精度主要集中在Int8,FP16和TF32根据统计模型平均提速高达2.5倍这里我们展示一个超分辨率模型的优化过程我们使用EDVR超分辨率算法目标是将1080P视频处理至4K分辨率该算法在FP32精度下直接运行需要消耗大约50G左右的显存远远超出了当时推理卡的显存上限即使转换到半精度下也需要接近30G左右的显存仍然远超了T4的16G如图所示我们将该模型拆成三个部分分别转换成Tensor RT模型将这三个子模型共享工作区显存成功地把模型显存开销降到14G左右并部署在T4GPU上EDVR算法的第二个优化点紫图融合左边的代码来自于EDVR的一部分其计算图如右图所示我们将整个过程使用KUDA重写融合为一个KUDA kernel作为Tensor RT的插件相比于Tensor RT除层优化的实现我们的融合算子获得了大约4倍的性能提升显存开销也从2700M左右下降到800M左右半精度下显存开销进一步下降到400M左右取得了明显的性能和显存的重大收益接下来我们是讲一下最大化GPU的利用率对于基于CNN的神经网络增强算法来说普遍分为前处理及图像到张量GPU处理及推理过程以及后处理即张量到图像三个过程前后处理均由CPU执行基于单线程的同步调度会导致GPU经常处于闲置状态在视频增强的场景下同步模式的GPU利用率一般不会超过70%有较严重的资源浪费因此我们需要一步调度通过减少GPU的闲置时间来提高GPU的利用率一步调度过程我们将之前提到的三个算法过程部署在不同的线程上使用对列进行任务管理满足了CPU的一步调度GPU处理线程只负负责提交GPU相关任务所有GPU任务的提交均使用一步接口配合Cuda Stream提交任务所有任务提交完成后再使用Cuda Event进行记录然后将整个任务打包传递给后处理线程即可事件等待放在后处理线程中不阻塞GPU任务的提交这样GPU利用率在运行状态时可以稳定在100%左下角是整个集群在任务高峰期的利用率统计图可以看到集群平均利用率接近到100%没有完全达到满负荷的运行的状态原因一般是由于以下两个原因第一任务分发不饱和有GPU处于闲置状态第二CPU平静GPU速度太快导致相应的CPU任务例如视频编码阻塞了整体流程从而导致GPU利用率下降这一点是更上层的应用我们暂时无法进行深度优化接下来我们讲一下GPU的执行器调动模式随着增强算法的逐步迭代单一模式的增强方法逐渐过多到内容自适应增强模式如图所示算法根据输入图像的画质和噪声指标来明确具体的增强链路由于条件判断在CPU上确定导致整个链路变成了同步模式根据上文提到的内容GPU利用率不会很高统计显示这种模式下GPU利用率在40%左右随着自适应增强算法的演进接入的业务越来越多相应的算法模板数量一直在上涨截止到目前为止算法模板这已经超过20多个单个模板用到的增强模型最大数量也超过了10个除了前面提到的利用率不高的问题之外这带来了两个新的问题第一模型数量的上涨导致了巨大的显存开销第二算法模板数量太多导致了维护相当困难因此我们提出了GPU执行器调动模式来同时解决这三个问题关于显存开销巨大的问题我们使用了两种方式来规避第一Tensor RT的显存开销分为权重和工作区一般情况下每个Tensor RT模型是独立管理自己的权重空间和工作区空间的在这里我们使用共享的工作区空间所有模型共享使用同一块显存作为工作区这样可以几大减少显存开销第二工作区大小是和输入的张量分辨率呈正相关输入的分辨率越大工作区占用的显存越多在日常场景中尤其是VR场景输入分辨率可能会高达数十亿像素即使是目前显存最大的GPU也无法直接进行处理针对这一点我们采取了分块处理来解决工作区过大的问题为了避免分块后卷积在RAW盘顶带来的边缘问题各个区块在边缘处会有一部分的重叠来避免经过这两步的显存优化显存占用明显减少显存开销节省了50%以上关于第二个问题多种业务带来的高复杂度我们的解决方案是GPU执行模式将业务上用到的所有模型集合在一个服务中再根据不同的配置同时支持所有的业务经过之前的显存优化我们可以将所有的GPU分析增强模型注册到GPU执行线程中GPU执行线程专注负责GPU任务提交使用Cuda Stream和Cuda Event来完成一步处理如图所示增强算法的前后处理均由CPU线程池来进行计算并将处理结果通过Future编程范式来进行线程任务传递GPU调动模式使得单一服务支持了已接入的所有算法只需要维护一个单一服务即可满足各种业务的复杂需求在GPU执行器模式中为了使GPU利用率最大化我们启动了多个任务线程来提交来自不同业务的增强任务经过多线程任务提交优化后单GPU的利用率可以维持在97%以上目前在火山引擎视频云的图片服务中单A10GPU上已经部署了高达60多个增强模型充分利用了GPU资源来满足业务的各种复杂需求接下来介绍一些关于视觉大模型的一些优化工作这里我们介绍一下关于LAVA相关的优化工作LAVA是业内知名的开源项目大体框架可以分为VIT和大语言模型后端两个部分VIT将输入的图片编码加上相应的提示词送到大语言模型后端最终得到最后的输出优化LAVA的第一步工作将VIT部分使用TensorT进行加速再将大语言模型使用TensorT LLM进行加速使用7B的大语言模型时在Python环境下我们在A10GPU上取得了大约5倍左右的提速但是根据N-Sighted System的分析来看多图处理时GPU处于间隔运行的状态空前时间较多利用率无法一直处于满载状态这是由于Python的开发流程较难实现异步处理工作导致GPU处于同步运行模式下因此GPU利用率无法一直处于满载状态在这里我们使用前面提到的执行G模式使用C++重新优化了该流程减少了GPU的空前时间根据N-Sighted System的结果来看获得了大约20%的性能收益该项目已经在Github开源欢迎大家提出意见和建议最后来到了总结部分GPU在视频图像处理场景得到了广泛应用第一GPU拥有无与伦比的处理性能传统算法和CNN神经网络均以支持并且提供了高质量高性能的硬件变加码能力第二GPU易于使用其拥有成熟的Cuda生态以及Tensor RTTensor RT LLM高性能软件同时知名的开源项目FMPEG也接入了GPU的硬件变加码能力最后感谢大家的观看谢谢