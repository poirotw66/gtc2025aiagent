 Welcome everyone, my name is Heitor and I am the leader of the language modeling team at the Language Technologies Lab of the Barcelona Supercompeting Center. Today I am excited to introduce you to Salamandra, the first and biggest national language model in Spain, which plays a crucial role in empowering AI sovereignty not only in Spain but also across the European Union. The Salamandra family includes models with 2 billion, 7 billion and 14 billion parameters, all trained with up to 13 trillion tokens. This achievement was made possible through our collaboration with NVIDIA. Together we have contributed to shaping NEMO into the powerful framework it is today, which serves as the backbone of Salamandra. One of the key aspects of Salamandra is its commitment to openness. We just don't provide the model weight, we also share the configuration files and execution scripts. This transparency ensures that our model can be freely accessed and utilized by researchers and developers alike, fostering innovation in the AI community. Moreover, Salamandra is fully compliant with European AI regulations, thanks to our close collaboration with AESIA, the Spanish agency responsible for AI oversight. This partnership ensures that we adhere to the highest standards of ethical AI practices. We have made significant efforts to ensure that the model does not contain or generate biased content, aligned with European standards and culture. This commitment reinforces our dedication to developing responsible AI. Additionally, although we have not yet released the dataset, we guarantee lawful and legal access to the data. We have meticulously described the dataset with all necessary details, ensuring it is completely auditable by regulatory bodies. This transparency further reinforces our commitment to responsible AI development. All of this aims to enable the use of Salamandra by the industry, providing legal assurance that the models comply with regulations. The Salamandra model family has been trained by the Language Modeling Team, which is part of the Language Technologies Laboratory at the Barcelona Supercompeting Center. The team has seen significant growth in recent months, now comprising 24 out of 60 members of the lab. Here you can see one of the most recent pictures of the lab. As you can imagine, training language models of this size requires funding. In the case of Salamandra models, the funding is entirely public. It comes from the Government of Catalonia through the ILENIA project, and from the ALIA project under the Secretary of State for Digitalization and Artificial Intelligence. Additionally, it has received support from ILENIA, a coordinated project that supports regional initiatives, which includes the University of Santiago, the ITS center from the University of the Basque Country, the Zenith in Alicante, and also the University of Jaén. Therefore, the Salamandra models are the result of a joint effort by leading institutions in the country. Building on our commitment to responsible AI, we face several challenges in developing language models. We face the same challenges as well as the data scarcity. Key issues include language and domain coverage, which affects our ability to capture linguistic diversity in morphology, syntax, and genetic systems. Also, licensing restrictions can limit data access, and biases in multilingual corpora can skew model representations. Tokenization also presents challenges, requiring us to accommodate language-specific structures while balancing vocabulary size and efficiency. Despite these challenges, they also provide opportunities for innovation, helping us create more inclusive and representative language models. And now let's move on to present a crucial topic in the training of a large language model, the training corpus. One of the main objectives of the project has been the collection of data from non-internet sources. The team has spent years gathering data from institutions and companies. The team has been the collection of data transfer agreements. The team has been the collection of data transfer agreements. And here you can see some of them, such as SCELO, DIALNET, Radio Televisión Española, the Spanish radio and television, and the Parliaments of the Parliaments of Catalonia and Valencia. In addition to these periodic downloads from internet sources and from our data providers, we have also operationalized the extraction of certain sources, such as Wikipedia, the official gazette of the Government of Catalonia, and open licensed content from YouTube, ensuring that this content is always up to date. We continue to work on operationalizing the extraction of additional sources. Once the data is extracted, we use the CURATE tool developed by our team to process the languages of our peninsula, Spanish, Catalog, Galician, Portuguese and MSC. CURATE has been highly optimized for this set of languages. For other languages, we use the Angoliat pipeline from the OSCAR project. Our final corpus is composed of all the official languages of the European Union, plus the official languages of Spain, Catalan, Galician and Basque. Despite the tremendous effort in the data collection, two-thirds of the corpus still comes from Colossal OSCAR. The percentages of languages and language families are shown in the table on the left. Here, the languages of Spain were oversampled by a factor of two. Nevertheless, English remains the most abundant language in our corpus, while Spanish accounts for a notable 60% of the total. Fain Wepedu was introduced while we were already training the models, allowing us to gradually replace OSCAR at different stages. Now I will briefly explain the architecture of the Salamandra models. The architecture of Salamandra is quite standard, based on Lama 2, which is also fairly standard. It includes several improvements that have been gradually introduced by subsequent works. For the 2B model, we decided to use multi-hat attention to maximize performance. For the larger models, we employed group query attention with 8 groups. All models were trained with BF16, as FP8 was considered risky. We will discuss this further later when I present the infrastructure used for training the models. While the architecture is quite standard, we put effort and attention into the tokenizer. Our goal was to improve the fertility of the languages present in the corpus. Based on previous experiments, where we tested five different distributions, we decided to use a uniform distribution for the tokenizer's training corpus. The final size of our vocabulary was 256,000 tokens, and it was trained with 3.3 billion words. This graph shows the fertility of the languages in our corpus. The orange color represents the fertility resulting from containing the tokenizer with sun sampling based on the corpus proportions. The blue color shows the fertility of the tokenizer trained with a uniform distribution. The horizontal lines above the bars represent the fertility of a monolingual tokenizer with a vocabulary of 50,000, which we use as a baseline. As we can see, the fertility of the most languages is significantly reduced, leading to benefits in efficiency, as the necessary computation is lower, and that the meanings of words are broken down into figure tokens. The downside is that for some languages, fertility increases slightly, for example, in English and Spanish. However, we believe that the tradeoff is worthwhile. So far, we have presented the team, the training framework, and the corpus. However, training a large language model requires computational power. A lot of computational power. Next, I will present the Marnestrom 5. The Salamandra models were trained on the Marnestrom 5, the supercomputer of the Barcelona Supercomputing Center. The BSC is not only a center for high-performance computing, it is also a research center. Marnestrom 5 is part of the European Supercomputing Network, and is open to all researchers in Europe, subject to competitive project approval. The accelerated partition of Marnestrom 5 consists of 1,120 nodes, each one equipped with 4 H100 GPUs, with a total of 4,418 GPUs. These H100s have a unique characteristic, they come only with 64 GB of RAM, which prevents us from using configurations optimized for the standard version with 80 GB. Marnestrom 5 entered full production in July 2024. But one month earlier, it went through a pre-production phase, during which the center had to ensure the system met all the requirements and was ready to serve researchers. The Salamandra project was selected to push Marnestrom 5 to its limit during this period. Our team had an incredible opportunity, but also a tremendous challenge, to train the models within a single month on a system still in testing, without prior large-scale trials. It is important to emphasize that Marnestrom 4 had no GPUs, meaning our previous experiments were limited to very small-scale tests, with significantly less data. Now, let's move on to explain the pre-training process. As a summary, I will first present these tables. The Salamandra model family has been trained with NEMO, and comes into three different sizes, two 7 and 40 billion parameters. The two smaller models were trained with nearly 13 trillion tokens, while the 40 billion model was trained with 9 trillion tokens. This later model was trained using up to 2,048 GPUs. All models are available in base, instruction tuned, and their quantized versions. The scalability during the training was not ideal. The larger the model, the better the scalability achieved. In the case of the 7B model, increasing from 64 nodes to 128 resulted in an 86% throughput increase, but scaling up to 192 nodes reduced this increment to 68%. In the case of the 40B model, scaling from 256 to 512 provided a substantial increase of 73%. Despite the opportunity to advance in one month what we would typically achieve in a year, we decided to use 64 nodes for the 2B model, 128 nodes for the 7B model, and 512 nodes for the 40B model, using a total of 704 nodes out of 1,120 nodes, and leaving the remaining nodes available for evaluations or for other researchers. So, what you see here is the training loss of the 7B model. The training process was quite smooth, except for some spikes that are noticeable here in the chart. Almost all of these spikes in the smaller models were caused by faulty GPUs. But in the case of the 40B model, it tended to diverge when using FT8, which is why we decided to use BF16 to stabilize the training. Since the computer was in a testing phase, errors were fairly common, and each color here represents our start of the training. Only the runs that reached checkpointing are shown, so if I included all the restarts, it would be hard to see anything. Fortunately, the BSD systems team managed to resolve these issues some time ago. In addition to the base models, we also released, instructed, and multimodal versions of Salamandra, with the goal of showcasing the model's capabilities. The Salamandra Instructed model has been trained in 6 languages, the 5 main languages of the Iberian Peninsula and English. Despite starting from a highly multilingual-based model, we focused our extraction-tuning efforts on Catalan, Spanish and English. Some experimental models were fine-tuned with all openly available data for all languages included in the pre-tuning phase, but this approach was discontinued as it led to a derogation in results for the target languages. This approach also limited capacity to properly curate the data and subsequently carried out an exhaustive evaluation for other languages. However, we also included instruction data for other closely related Iberian languages as we observed a positive impact on the languages of interest. The final mixture was evaluated by a group of native speakers. This model is still in experimental phase, but we expect to release an extended model with all languages of the corpus by the end of March. The final model, which will also include the alignment phase, is planned for release by the end of the year. But this doesn't end here. We know that we live in a multisensory world, and we believe many real-world applications will heavily rely on the multimodal capabilities of the models. Images, audio, video, sensor data, 3D data… These are just a few examples of the modalities the model will need to support. In this regard, we are already working with images and videos. This was achieved using late-fusion techniques, which involve integrating a pretend encoder, a base large language model, and a projector. The training process primarily focuses on aligning the image embedding from the encoder with the large language model, enabling the model to understand the new modality. In our case, the model consists of Siglio as a pretend encoder, our Salamandra extract in 7b as the large language model, and a two-layer Preservitome as the projector. Here we can see how the model is capable of correctly solving various image-related tasks, both in Spanish and English. In the first example, the model is asked to describe the image in as much detail as possible. It successfully describes the city and identifies a bridge spanning a large body of water. It doesn't call it a river. The picture itself doesn't make that entirely clear. The model recognizes correctly the bridge arcs, the mountain slope, and even the reflection of the city and mountain in the water. In the second example, the model is tasked with naming each vehicle in the image and identifying their position. It accurately detects the yellow taxi, the green bus, the red truck, and the blue train, but overlooks the remaining vehicles. This suggests that such tasks require more specialized training data. However, the model correctly identifies the positions of the vehicles it does recognize. And in the third example, we tried to evaluate the model's ability to recognize text, and the result was satisfactory. Since Visual Salamandra is still in an experimental phase, we are optimistic about further improvements in future releases. Now we will discuss the evaluation of the model, as well as the characteristics it presents regarding bias, safety, and ethics. Regarding Salamandra's evaluation results, we are quite satisfied with its performance on various tasks. The model ranks among the top four for each size, and stands as the best among non-corporate models for the languages of the Iberian Peninsula. Salamandra excels in generative tasks, producing rich, grammatically correct, and coherent text. This strength is reflected in its high scores for generation tasks, particularly for translation tasks. While we didn't surpass models like GEMA2 in English and Spanish, we did outperform it in Catalan and Basque. This is a significant achievement, considering that GEMA2 is a distilled model, probably from Gemini, and has 9 billion parameters. On the downside, mathematical reasoning remains a weak point. We did not include enough high-quality mathematical content in the corpus, and due to the limited training time of one month, we failed to identify this issue early on. And now I will introduce a dataset designed to measure bias in question answering tasks. The goal is to assess how strongly the model reflects social biases when responding to ambiguous context. It also evaluates whether bias can overwrite a correct answer in disavigated contexts, where the right choice is clear. This dataset was developed at BSE by a team with diverse backgrounds, and to create it we conducted a large-scale survey to collect the stereotypes present in Spanish society. This slide presents two scores. QBSE measures how well the model performs in a given task, while the bias score indicates the extent to which the model relies on social biases to make decisions. A higher positive bias score means the model is more influenced by biases, while a negative score suggests alignment with counter-stereotypes. Overall, we are satisfied with the bias levels in the BSE models, which perform well across categories except for physical appearance. However, we observe that instruction tunes models tend to have a higher bias score than the BSE models. This is expected as they have not yet undergone the alignment phase. Here we present our analysis of cognitive biases that the models might have learned from the data. Cognitive biases can cause a model to answer based on factors such as the position of the response, the format, or the frequency of words. The presence of these biases means that accuracy doesn't necessarily indicate that the model knows the correct answer, but rather other factors are influencing the choice. In the case of the Salamandra models, we see that most of them perform well. The exception is the QBSE model, which shows a tendency to choose the first answer. We also observed that the instructed models exhibit better behavior. We also evaluated Salamandra's resistance to attacks. A red-temin pipeline simulates malicious usage, and we measure whether the model resists the attack or not. In this pipeline, the model receives prompts in English, Spanish, and Catalan, then generates three responses. A moderated model classifies the type of attack and decides if the attack was successful. The lower the success rate, the more secure the model is. We are currently developing our own moderation model to substitute LAMAGARD. To improve the user experience with Salamandra models, we have conducted the first alignment test. We applied DPO with PKU safety dataset to the models, resulting in a significant increase in the resistance to attacks. The evaluation shown on this slide presents the results from the HH reformer-related dataset for simplicity, but the results are similar across other datasets. The current versions of the instructed models are proof-of-commit sets, as we are still collecting and generating our instruction datasets with permissive licenses. One of the challenges that arises when using available instruction datasets generated by other initiatives is that the model's identity is sometimes incorrect. For example, in the early versions of the model, it would respond as if it were Open Assistant, which is one of the largest datasets that was used for the instruction phase. To solve this, we crafted a dataset with questions about its identity successfully resolving the issue. We also ran tests using the system prompt. In the graph on the right, we see the percentage of correct answers, and the odd columns show the result with a system prompt, and the event columns show the results without a system prompt. And finally, let's move on to a few key conclusions. So, we have introduced the Salamandra family of models, the first and biggest national large language models, a major step in promoting AI sovereignty, not just for in Spain, but also for the European Union. This milestone was made possible thanks to our collaboration with NVIDIA. Salamandra stands out for its openness. By providing a full transparency into the training process, we ensure the model is accessible to researchers and developers, encouraging continuous innovation in the artificial intelligence field. Also, the model is fully aligned with the European AI regulations, ensuring compliance at all levels. We have worked hard to eliminate biases and to align Salamandra with European culture and ethical standards, reinforcing our commitment to fairness and inclusivity. And also, to ensure full accountability, we guarantee lawful access to the training data, offering complete transparency for regulatory review. But the ultimate goal is for small and medium enterprises in Spain and Europe to be able to use the Salamandra models within their own infrastructures. So, Salamandra models comply with European regulations providing businesses with the legal as soon as they did, while also reducing their dependency on proprietary models. Also, looking ahead, we will continue scaling up the model sizes and releasing new models with improved capabilities. For example, we are planning to release a mixture of experts models. Also, to address the computational gap, we are working on federated learning to harness the computing power of the European HPC network. Additionally, we will be incorporating new modalities because we know that businesses will also need these capabilities. Lastly, we have been working hard to overcome the ethical challenges involved in developing large language models. And this concludes the presentation. Thank you all for attending and I will be available in the chat to answer any questions. And I will definitely give you an understanding of the total knowledge that the studio – I´ll decrease... Until next time. As you will give us a decision about the怎么样 we can provide the behavior of the association which is somebody is using aişla as an example. So, you can also, it doesn't look like the Aislauva of the potentialех predictable thinking that the combination of the initial equilibrium effect is very difficult for us. Summa señor hasn't done this.