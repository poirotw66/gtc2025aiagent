 Okay, great. So yeah, so exactly. The thing that I want to tell you about today is the work that we've been doing with our polymathic AI group based in New York on trying to build multidisciplinary scientific foundation models. So foundation models, you probably already know what those are. But what we want to do is try to find ways of integrating those models within scientific workflows. And even beyond that, we have this vision of being able to build broadly applicable models that will be able to be applied in many different science domain and disciplines without specifically needed to develop domain specific applications. So to tell you the story about what we're doing and kind of the challenges that we have to be, that we're dealing with, I wanted to start by telling you a little bit about how deep learning is currently behaving in my domain of application, which is astrophysics. And you would find a similar story in many other domains. So this graph is just showing you the numbers of papers out there that are using deep learning at various levels of the analysis of an astrophysics workflow. And you can see that since 2016, 2017, these use of these methods have kind of exploded. And this is something you see in many domains. But one thing that is not so obvious at first sight is that the vast majority of all deep learning that is applied in this particular domain relies on supervised learning. And more importantly, relies on networks that are trained from scratch for very specific use cases. And what I want to show you here with this particular domain is that that has intrinsic limitations in the speed at which we can do scientific research. The first two examples that I want to show you here is that in my field, for instance, we are in situations where we have very low data regime. We have very few examples to train on. So this is an example of such a situation where trying to find what's called strong gravitational lenses. They are essentially a background galaxy that is being completely lanced by the presence of a foreground galaxy. This is the arcs that you can see here. And those we only know about a thousand of them on the sky. So that's not enough to train a deep learning model, right? And the second situation is something that is becoming more and more common in scientific domain. It's called simulation based inference. And it's the idea of relying on a simulator to infer parameters of a particular simulation system or physical system. And again, the way that you use deep learning here is that you need to have access to enough simulations that you can train these modules that will infer for you the parameters of the system. And so that can be very costly if I need to retrain those neural networks for every new problem that I'm confronted with. And then the second point that I want to make is, as I said, so far, every problem has been targeted with particular neural networks. And there is no crosstalk between problems, which means that if I come into the field, I want to apply on some new data or do some new task. I need to start from scratch. And in practice, this means that if you're interested in doing something in astrophysics, it means that you need to invest something like a PhD year before you can actually get to the science. And that's the thing we're trying to solve. So meanwhile, while this has been happening in astrophysics and in other domains of applications, in computer science, we've seen the emergence of those foundation models. So you've probably heard a lot about them. But because if you ask two different people, they will give you two different definitions. Here is my definition of what I mean by foundation model. The first important point is that these are going to be large scale models that we will pre-train without a particular task that we want them to solve at first. And so the important notion here is that we do not need supervision to build those kind of models. And because we don't need supervision, we don't need labels. It means that we can handle a lot more data to train neural networks in. So in an image domain, the way that you would do this, for instance, is to train a vision model to predict missing patches, from an input image where you have removed some patches. That's a very trivial task. It's not targeted to a particular application in principle. But because the task is non-trivial, it forces your model to discover good representation of your data such that it can solve it. And then the second important point is that once I've discovered that representation, I can adapt it to solve a bunch of new problems that rely on this existing foundation. So that's the idea. And it has been exceptionally successful in broader computer science, in language and in vision. And this is an example of vision where before this reliance on pre-trained large scale unsupervised foundation models, you were restricted by the number of labels that you knew, for instance, for image classification tasks. Which means that if I only have access to a particular size of data, I cannot train a very large model, which is what you see here on the right. And a very large model would be able to reach better performance. So this is an example of classification tasks. And then the last important point is that what will be important for scientists is that we need a way to adapt this foundation model to the particular task at hand. And so there are different flavors of how you can do this. But one thing, one behavior that I want to point out and that we're going to be looking for is the ability to adapt such models with very few examples of the particular application that I'm interested in. So here is an example of adapting an existing vision model by just training a linear layer on top of the model. And you can see that with, let's say, five or ten examples per classes, you already kind of saturate the performance at 85% classification accuracy, which is near the best state of the art. So that's what we're looking for. And the mission of our team has been, can we try to unlock a similar shift toward foundation models in scientific applications? And what I'm going to show you afterwards is that it's not that simple. And I'm going to walk you through the different stages of challenges that you need to solve. But before that, I just want to say a few words about our team. We're based in New York, but we have collaborators from different places. And we're funded by the Simons Foundation and Schmidt Sciences. And another important aspect of our team is that we are really interdisciplinary. So myself, for instance, I'm an application domain expert. I'm also an expert in deep learning. But we have people like Alberto Bietti or Kinocho, who are pure computer science researchers. And bringing all of that expertise together is the only way that we can actually make progress here. So let me tell you a little bit more about the challenges that we are going to be confronted with. And I'll say two challenges. The first one is the scientific data creation challenge. So one factor that has enabled the development of the large scale models in broader computer science is the availability of very large data sets of images and texts. So this is an example from this Leon 5B paper where they make available billions of examples of text and images. And that allows you to build very good image generation models, very good language vision models. But now if I'm trying to do the same thing in a particular scientific domain, I'm confronted with new considerations compared to traditional images or texts. So the first one that I want to highlight here is that I don't only care about the data. I also care about the metadata that comes with it. So this is an illustration of a patch of the sky that is taken by three different telescopes. So they have different properties. And if I do not know about the telescope that takes the image, I could be fooled into thinking, for instance, that this object is very blurry or not very bright or something like that. So I need to know about the observing conditions, about the instruments to properly interpret the data, which is not a problem that you would have in classical natural images. And then the second challenge is that we have to deal with a lot more different types of data than just images and text. So in astrophysics, for instance, we have images. We also have spectra. So the decomposition of the light that you see from different objects. We also have hyperspectral images. We also have time series. And we also have a whole bunch of different measurements that come from different measurement pipelines. So in principle, we need to have strategies to handle that data diversity. And the last point, which is maybe the most important point, is that although there is a vast amount of data available out there, it also requires a huge amount of domain expertise to know what data I need to access. How should I compile it? And then how should I use it for a particular problem? And so those are actually the main challenges to transpose existing methodologies to scientific domains. And then the next point that I want to make is even though if I manage to solve that data challenge, I'm confronted with the next challenge, which is being able to build neural networks that will be able to handle all of that data diversity. So to think about it this way, on the left hand of this axis, this is the traditional way of doing deep learning and machine learning where you would develop neural networks for specific types of data and specific kind of problems. And this is what we've been doing for a long time. So I have papers, for instance, where we do this for images. I have colleagues that do this for spectra or time series. But if I want to dream a little bit and think about what I would like to have, not that I have it today, but that's my goal. I'd like to be in this other regime where I have a single model that is able to handle any input data that I can throw at it. And one solution for thinking about this would be a model that acts directly at the byte level representation of your data. So that sounds like a little bit of science fiction, but this already is achievable in limited examples. So this is a paper, for instance, where they show that you can build a transformer model directly at the byte level to classify JPEG or PNG images. And in practice for us, it would be a little bit challenging to scale this up because then you need very big context sizes of your transformers. But that's kind of the notion of where we would want to be at some point. And so I'm going to show you our solutions to the different challenges, starting with the data challenge that I mentioned. So the first project that I want to show you here is a project that we call the multimodal universe that we presented just a few months ago at NeurIPS, where we try to solve that question of how do we aggregate this data at scale for foundation models in this particular domain. And here it's astrophysics because I come from astrophysics. But the important point that we were experimenting with here is what is the strategy to do that? Try it out with a particular community. And then once we know how to do it, we can apply it in another community. So here the project is to assemble a large scale data set of different types of data in consistent fields. So we can apply machine learning to it. And it relies on three main pillars. The first one is to interact with a broad community of experts. That is always going to be the case because of this notion that I mentioned, that expertise is very limited in a particular type of data. I'm expert on some data, but I'm not expert on some other data. So you need the experts to be part of the loop. Then the second pillar is agreeing on some way of standardizing the data in a format that will be machine learning friendly and accessible. And the last consideration that we had with this project is that we wanted to make sure that we were targeting many different types of data, but also data relevant to many different sub areas of astrophysics. So for instance, we have images here. We have deep images from JEPPS web telescope. We have hyperspectral images. We also have time series and such. And all of these are used for slightly different sciences. And we want to show that in principle, we can build a model that would be applicable throughout those different subfields. So just to tell you a little bit about the concrete strategies that we adopted to reach that goal here. We followed the following schema for how to collect data and integrate it into a consistent format. So the main piece is this download script, which is our interface between experts and our framework. So here, essentially, we make it pretty easy, very easy. We go see the experts. We tell them, write a script that will download all the data you have in a particular survey, in a particular database. You know how to do it. Just do it. And then we collect the result and we aggregate it at scale on our own compute in a way that will then be accessible to everyone. And we standardize format that way. We also provide utilities to cross match different data sets together. So for instance, if I'm interested in building multimodal models, I have a strategy to extract all the different types of observation I have for a particular object on the sky. And so at the end of the day, we were able to deploy this at pretty large scale. This is not yet at the scale of the largest data sets that you have in computer science, but it's still at the hundreds of millions of images of time series and spectra and such. And we make it readily available and feel free to go take a look at that if you're interested in it. But this is this was to tell you a little bit of our concrete strategy, but we applied a very similar method in a different domain, which is also a domain in which we're working of surrogate model for computational fluid dynamics. So the example that I show you here are a subset of a collection of data that we call the well that we worked on with a collection collection of experts that have contributed simulation models and data from the simulation models to a consistent pile of data. So the main point here is that this is one of the first data sets, probably the first to reach again, very large scale. So image net scale data sets for fluid dynamics, and it represents a very large ensemble of simulations and different settings from 18 different subsets that span astro bio aerospace atmospheric science and such. And the last point is that we make it really easy to use, very easy to download is documented and anyone can use it for for the science. And again, that has required a lot of interaction with expert. This is not something we could have done on our own. All right. So now that we have solutions for how we aggregate the data. The next challenge is what kind of architectures we can think about to bridge this gap between different types of data or potentially different types of applications you're looking for. So I'll pull up again this diagram here and we'll go back to the left with the first example of such method that we developed, which we call AstroClip and which was released last year. So the idea behind this is not yet to have an architecture that will work across all your type of data. Best is a first step toward being able to have models that understand strongly different types of data. And I'll show you how. So clip, we didn't come up with it. This is something that's been well known for a while. And in particular, it rose to success by the efforts from OpenAI to build models that can link together text and images. So here we're not working with text or images. We're working with images and spectra, which are two different modalities. But they tell you about the same physical underlying objects. And what we come up with here, we have a machine learning model and encoder that acts on spectra and one that acts on images. And we just train jointly those two encoders such that if an example of an image and spectrum come from the same object, the embeddings that you get, the vector embeddings you get should align strongly in that embedding space. And if they are not associated with each other, they should not be aligned together. This is essentially what this loss function tells you. And that is a pretty simple thing to implement, but it has very interesting properties for us. So the first thing is that the space that you discover through this embedding process as a notion of similarity and proximity in that embedding space that also aligns with the notion of physical intrinsic physical similarity. So here is what I'm an example to show this. This is a galaxy that I can embed into this space. And then I can look for all the surrounding objects that are very similar embeddings. And that's what you that's what you see. So this is very similar to what happens in text embeddings with rag and such. But here it works at the level of of data. The important point is that those look similar. But as I will show you just after, they are also physically intrinsically similar. And the the property that we're actually interested in. And the second point that I want to make is this type of models, they are directly usable without any additional fine tuning or without any additional machine learning to build prediction models for some properties that you might be interested in that it was not specifically trained for solving. So one way to illustrate this here, this is what's called zero shot prediction in many instances. We have again the embedding of an object that we know nothing about. We only have access to its image. And what we can do is to look for the spectrum that aligns the most with this particular object and spectra are things that we can analyze from a physical standpoint. And we can, for instance, extract the distance of that object or the mass of this galaxy from knowledge of spectra. So now if I want to build a mass predictor model for this galaxy, I just need to find the closest neighbor in that space. And I can grab the known mass or distance from that neighbor. And I return this as an estimator for those properties. And what we found is that this is pretty much the best thing you can do without performing even supervised methods on this example. So here is a more quantitative way of saying this. What I show you here is the performance of trying to predict a particular property of galaxies, which has to do with how far away from us they are from having access to the image. So the important plot here is this one where we train a dedicated deep learning model. So this will be the standard, the very standard approach of doing deep learning. And you can see that we reach a pretty good accuracy, but it's, there is a bunch of scatter. And it requires a dedicated model to train this. What we obtain here on the left is something that is less scattered and is obtained simply by using the pre-trained clip model and just using nearest neighbors as a way of doing zero shot prediction. And it outperforms a dedicated supervised model for this. This is not the only example. We tried it on a bunch of other applications. Here it's the same thing again that we see with different properties that we want to infer. And the important point is that the scatter that you get from doing this prediction is going to be lower than a dedicated model that you've trained specifically for this. So they are very good and like mathematical underpinning for what this would happen. I won't go into all the details. The important thing is that this loss function actually promotes extracting information, physical information that is shared between the different modalities. And so in our case, this has this eventually results in extracting information about the physical properties of these particular galaxies. I'm going to skip over the details. But I wanted to say also why this is so interesting for us. The first point is that this type of embedding models are extremely easy to use. And for us, it means that we would never have to retrain our models from scratch. We can grab one from a given library. No matter what is the task that I want to solve, I can just adapt or do a simple search in that embedding space given this pre-trained model. The second point is because the operation you need to do in this embedding space are very simple, I can do them even with very few number of examples. So if I want to predict, let's say, the mass of an object here, I need to only know a few known examples in the neighborhood of the point I'm trying to predict for. And I can apply at scale pretty fancy deep learning without having access to a lot of training data. And the last point is that if we make available this type of embedding tools to scientists, they can then do their science with essentially psychic learn, doing a nearest neighbor search or doing a linear regression, instead of having to deploy very powerful deep learning directly from the data. So that was our first method, our first foray in that area. And now I want to tell you in a slightly different field about another project that we call MPP for multiple physics pre-training. Again, it's a slightly different topics, but it has the same issue of needing to find a particular neural architecture solution that will work across different domains or different situation. So that's a work that we presented at NREPS at the AI for Science workshop in 2023 and again in 2024 when it got accepted to the main conference. And the premise here is that we want to find neural network architectures that are going to be able to work on many different types of fluid simulations, whether they are 1D, 2D, 3D, whether you have pressure field, temperature field, just velocity field and such. So the background for this, we are using PD bench, which is previously to the well that we developed ourselves. This was one of the most common type of collection of such simulations that were available at there. And on that collection of simulations, essentially we want to develop a neural network model that will be flexible to any simulation setting that you have to some extent. So the key aspect of this, this is a transformer architecture. I won't go into all the detail, but I want to highlight the solution that we have to have a single network capable of handling different physical fields. So the way that it works is that at the input of the network, we have a way of telling the network about which fields it's looking at. So if it's a pressure field, a velocity field, we encode that information at the input of the model, which means that now we have a model capable of acting on any input fields and thus applicable in any situation that you are interested in. And just to show you what happens at the end, this is a result from the train model where you have at the bottom here, I'll restart it, and then you can see that a few shots of what a real simulation would give you. And then what you see at the top is just unrolling this model forward in time, and it's just simulating forward in time this system even beyond the limits of the original simulation. So again, yes. And so why are we specifically interested in this is, again, with this idea that now we have one system is broadly applicable across many different situations. And it also has this nice property of being adjustable, transferable, with few number of examples to a new system that it was not specifically trained for. So to highlight this feature, let's concentrate here on the plot on the left. What I show you is the evaluation, let's say, accuracy that you can obtain from a model trained from scratch on a new system that it was not trained for. So this is what you see here in the green. And in blue, this is what we can achieve with this pre-trained model that was not specifically trained on this particular type of data. But you can see that you reach orders of magnitude better performance at much lower regime of samples. And for the end user, what it means is that it is a lot more applicable for them. They don't need to rely on very expensive and large training sets. And last point I want to make is that I will direct you to Shirley Stoke that will happen in the next room at 4 p.m. And she will say a little bit more about MPP and its applications. And the final point I want to make is the latest version of models that we've been working on, which goes more towards the modern techniques that are currently used in language vision models. And those are called early fusion multimodal models. So this particular model that we call Aon1 for Astronomical Omnimodal Network is the first instance, as far as we know, of a scientific large-scale multimodal data model. There is no language in it. And I'll show you exactly how it works. This is something that we trained on the Jean-Zé supercomputer near Paris in France as part of a grand challenge that was run just last year. All right. So first of all, what goes inside this model? Our idea here is that we want to demonstrate that we can come up with a single system, a single model capable of handling many different types of data in a usable way. So we collected different subsets of this multimodal universe that I told you about. The first one is a set of astronomical images that is very good for studying galaxies. The second one is, again, astronomical images, but from a different instrument. It has different pixel size, different precision, different resolution. And it's also fairly smaller in size. But beyond the images, we also collected data from this experiment, which is a spectroscopic instrument. It's going to look at the spectra for many objects on the sky. So it's no longer an image. It's a 1D spectrum. And interestingly, this also is interesting if you want to study stars, so beyond just galaxies. We collected similar data from, again, a different instrument to show that we can be agnostic to the particular details of the instrument. And we even went further and grabbed data from a satellite that is, again, a spectral instrument, but with much lower resolution. It gives you access to a lot more examples. So that's our data pile, let's say. And an important feature that went into deciding how we would build it is that it can be used, data from this sample of data, can be used for many different scientific applications within astrophysics, whether it's for cosmology, galaxy formation, astrophysics, and such. So it covers our idea of being able to translate between different subfields and subdisciplines. So how do we go about making a system that can act on all of this data? The first stage, very similar to what is done in modern vision language models, is to go through tokenization. So here the idea is that we are going to come up with encoder-decoder models that will be very good at representing each modality that we have. But in the middle of that bottleneck that you have here, we are compressing the data in the form of tokens. And those tokens are essentially just discrete numbers. And so at the end of the day, this is a system that turns any type of data into a series of discrete tokens. I will skip a little bit of those details, but we also take particular care of building those tokenizers such that they are applicable for many different types of instruments within a modality category. So for instance, here, we are using the same strategy as you saw in MPP for telling the model about which field I'm looking at. But we are using this to tell the model which telescope I'm actually using and which particular wavelengths I'm looking at through that telescope. So that's, again, with the idea of being able to use the metadata that comes with the data. And in the end, we deploy this strategy for 39 different modalities. Some of them are images. Some of them are spectra. But we also have physical parameters like the flux or the size of an object. So just numbers, essentially. As well as additional data that looks like the sample of tasks that we might be interested in solving with these models, like segmentation maps. And at the end of the day, once we have the strategy for tokenizing data, we can now think about training a large model on top of the tokenized data. So the first aspect here is that we want to imbue in the model the notion of understanding how the same object is going to look like through different lenses, through different type of instruments. So the way we do this is by finding pairs of objects seen, same objects seen in different datasets. And then we collect all the pairs of a given matching object. And we'll get the tokens for these measurements. And we are going to feed a random fraction of these tokens as an input of an encoder decoder transformer. And request at the output of the transformer the missing tokens that we didn't feed as an input. So this is a pretty simple strategy for training this kind of model. But it's very nice because it's extremely scalable to the numbers of modalities you have. You don't have to think too much about it. As long as you randomly sample all different possible mixtures of input and output tokens, you have a model that will be able to take any inputs and generate any outputs if you desire. And so this is a strategy that was also used by a model from Apple last year, which in a more classical setting, but we essentially use the same thing. And at the end of the day, what it means is that you have a probabilistic model. It's a generative model that has learned to model all conditional and joint distribution of all the different types of data that you've trained it with. All right. So last point is that training this kind of model at scale is not something you can run on your laptop. So we were very lucky to be selected as part of this grand challenge on the John Zay supercomputer, where we were able to develop and then train a series of foundation models of different sizes from 300 million parameters to 3 billion. And what I show you here is a loss function for this kind of model as a function of the size of the model. You can see that for our extra large model here, you do get much better performance. So we have evidence of scaling with model size. And now to conclude, I'll just show you the kind of things that we can do with such a model. So the first examples here, they are not something I would directly use for science, but just to show you that our model actually understands a lot about what's going on in the data. The example that you see on the left is an example of super resolution, where I put as an input an image, a low resolution image from a low resolution instrument. And I request out of the model to generate a high resolution image that I could have observed with a better instrument. And this is why you see this kind of like zooming in and out. The lower resolution image gets turned into a higher resolution image where you have better separation between objects. So that shows that the model kind of understands what's going on in this image and can generate observation from a different instrument. And on the right, it's a similar thing, except this time it's on spectra. So 1D spectra where we feed the model with those red lines here. So very low resolution spectra. And we ask it to generate what this particular object would look like with a much better instrument. It's called DESI here. And what the model will output is are those blue lines that you may be able to see. And the black line is the truth that we know for this object. So you can see that even though it doesn't see any of those absorption lines here, it will want to put them in the reconstructions because it understands that stars have absorption lines as those wind lengths. And that's what it's showing you here. And even beyond this, we were extremely glad to find examples of emergent multimodal understanding. And what I'm showing you here is, again, an example of survey translation. This time it's no longer within one modality. It's across two different modalities. And I'm sending in an image of these galaxies here, left and right. And I'm asking, okay, what would be the spectrum for that particular galaxy through this instrument that takes spectra? And what you see here in blue is what the model outputs. In black is the truth that we know of for this particular object. The main important thing that I want to mention here is that at no point did we train it specifically to translate between those two specific surveys. But essentially, it has understood through all the other combinations that we showed it how to go between different instruments and even between different modalities, even in cases that it was not specifically trained for. All right. And to conclude, so this is nice, but the main question is how can I use these kind of models to accelerate in practice concretely the way that I do science in different fields? So to show you the difference, let me walk you through the traditional way that I would use deep learning. And this is not new. This is something that I've been personally doing since 2016. The first stage is I need to assemble a large training set of very realistic data. It can be very costly because I need the data to be very realistic in order for my model to perform well on real world. Then I need to come up with a neural network that will act well on my data. These days, there are common choices, but by then it was a lot more difficult. I need to deal with a lot of issues with normalization. I need to train this thing on some compute that I may or may not have readily available. I apply it to my problem. And then at the end of the day, I throw the network away and it's never to be reused again. And that is the sad part that we are trying to get rid of. And so now if I change this vision by how a foundation model could be used in that workflow, now I don't need to have a very large training set. As I'll show you, I can get away with just a few examples to specify a new task. And I can adapt an existing model in a matter of minutes for my new problem. So because I'm running out of time, I'll skip some of the details. Let's just say that we have some strategies to take the pre-trained model and adapt it to new tasks by just linear projection of the embeddings that you get out of the model. And we show examples of solving a range of particular tasks in this domain from classification of objects, from segmenting different physical structures on those images, inferring physical parameters for these objects, or simply building good embeddings that are very good choices for doing retrieval and similarity search in very large data sets. And I'm going to stop here with just the last word. What is coming next for Polymathic? Go see Shirley's talk afterwards if you want to know more. But essentially, we're going to scale up on more domains, more data, larger models, and always with the idea of trying to build these tools within the workflows of concrete scientists. Thank you very much and happy to take any questions. Hello. Thanks. Hi there. Thank you for the talk. So I noticed on the astronomy example that the accuracy was pushing like 80% or so in the beginning. Like this one? No. Very close to the beginning. It was like you tried one thing and it was like 0.73 and then you tried another thing and it was 0.79. So it's not an example from us. It was an example from a paper showing that you can do adaptation of existing models. Is this the one? I think so. When you were showing the results later. But in other case, the question is still relevant where I'm curious about the accuracy of these models in real applications so far. Oh, yeah. So, okay, maybe I see what you were referring to. It is better than state of the art is what I can tell you. So there are two types of state of the art. There are the standard techniques where people have data sets of usually tens of thousands of examples. We are way better than that no matter what. And then you have for very specific problems, people have trained models on hundreds of thousands of examples and millions. And so those specific models, they are still slightly better. But we are very close to there. Then are there certain domains that you're very excited about in terms of the accuracy that you think you can get out of them? Thank you. So I would say we always want to push the accuracy. But our first and foremost consideration is the ease of use. So if you are 99% there in a fraction of the time that it would take you to have a more specialized solution, that has a lot of value. And then we can do full fine-tuning and reach state-of-the-art performance. But it's how fast can we go from the data to the science that we want to improve. Just a quick question off the riff of the way you just answered. Sure. There's very specific models with, let's say, 1 million data inputs versus yours at 100,000 or 10,000. Are you able to stitch those very specific models into this overall? We could. So the way that they work is that they have access to labels that we don't assume generally. So one thing we can do is in a framework like the last one that I showed you, if we have a million labels for a particular task, we can include it as one new modality within our model. And so then we benefit from that additional information that those people are using. Okay. I guess perhaps last question. Yeah, very nice work. So I know in your domain you're dealing with images that have some kind of standardized properties, like everything I saw was on a unit square. Any ideas of moving past that? Yeah, exactly. So this is kind of legacy from the way that we've been doing machine learning so far. Everything is what we call postage steps of objects. This doesn't have to be the case. So we now have technologies where we can have an entire tile or whatever geometry, and we can tell the model that we're interested specifically in one location on that tile. And so the next generation of model that we're building will be in this more flexible fashion, capable of handling different input sizes, and having a specific way of telling the model, okay, I want to know the property of this object in that big tile. Thank you so much. Thank you for being part of the session.