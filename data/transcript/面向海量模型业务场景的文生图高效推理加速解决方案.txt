大家好,我是来自阿里云的李克森今天我和我的同事吴征标将为大家带来主题分享我们分享的题目是面向海量模型业务场景的文生图高效推理加速解决方案在正式开始之前,给大家展示一下我们的推理方案能够获得的性能提升还有业务成本方面的收益首先,在性能上可以获得最高1.9倍的性能提速其次,将文生图模型的编译时间从原本所需要的30分钟优化到了5秒钟大幅增加了模型适时优化的能力另外,针对不同的英语达硬件进行了性能建模从端到端能够带来额外的5%到10%的性能收益在业务场景,我们的方案可以大幅缩减文生图的时间改善用户的体验,能够减少GPU资源的闲置与浪费最终体现在成本上能够实现30%到50%的业务成本下降我们的分享主要分为背景介绍、挑战、解决方案还有总结四个部分接下来,由我的同事吴正彪为大家带来背景介绍大家好,我是阿里云智东集团的工程师吴正彪我来给大家讲解一下文生图模型的一些基本的概况包括文生图模型的历史以及它的模型原理还有与我们推理相关的Stable Diffusion的结构为了给一部熟悉文生图推理过程或者模型结构的听众提供一些基础的认知我们先介绍一下文生图模型的历史发展如果对此部分不感兴趣的听众可以先跳过在2018年之前文生图模型基本使用上都是基于生成对抗网络的方法生成对抗网络就是Generative Adversarial Networks简称为GANSGANS的基本思想是用一个生成器和一个判别器生成器和判驳器都是神经网络结构GANS的核心在于生成器和判别器之间的对抗训练机制生成器用于试图创建能够欺骗判别器的数据样本而判别器则试图区分真实数据和生成器生成的假数据放在文生图网络上就是文生图模型上就是生成器接收文本信息并尝试生成相应的图像然后判别器评估输入图像的真实性并同时考虑对应的文本描述是否符合该图像判断该图像是由生成器生成的还是来自真实数据集也就是说判别器不仅需要判断图像的真实性还需要验证图像与提供文本描述之间是否匹配这个思路就是slide上面显示的stackGAN以及后面后续系列模型的基本思路不过GANs现在已经用的比较少了在18年之前对后面文生图影响较大的模型反而是最下面这个VQVAE全称为向量量化变分自动编码器VQVAE采用了一种向量量化的方法来将连续的潜在变量转化为Lisan的代码本就是英文叫做Codebook向量这个Codebook机制直到现在的Stable Diffusion模型仍在使用这个Codebook可以类比为NLP领域自然语言领域中的Word Embedding部分就是每个Codebook向量代表图像特征的一种特定的模式或元素本质上是一个包含多个固定大小向量的集合这些向量可以看作是潜在空间中的一个LP中的词点或者词汇表在VQVAE中输入的图像输入的数据比如说是图像吧首先通过编码器转换为一系列的特征向量然后这些特征向量被量化为最接近的Codebook向量意味着每个特征向量最后都会替换为在Codebook中与其最相似的一个向量完成了一个从连续空间到离散空间的转换在21年之后随着GANS逐渐被替代文生图模型进入了使用自回归方法生成或者Diffusions模型就是扩散模型的时代基于自回归生成的方法它是将图像视为一个序列这通常是通过某种方式将二维图像转换为一为序列的这个转换的过程来实现然后使用类似于自然源处理中的某些这个像Transformer之类的序列处理技术来生成图像这种方法的核心在于Transformers这个架构它能够有效地处理长序列数据并且很擅长捕捉序列内部的依赖关系自回归模型是一种利用序列数据中的历史信息来预测未来值方的方法这就是自回归的意思它得通过过往的一些输出来预测未来的输出在图像生成领域中自回归模型通常将图像视为一系列的像素或者称为Token的序列并使用这些序列来进行预测比如在OpenAI在2021年发布的这个Doree图像中Doree模型中图像首先通过VQVAE被分解为一系列的离散Token然后这些Token与输入的文本Token一起被送进那个Transformer结构的模型中这个模型学习如何根据前面的Token序列预测下一个Token从而逐步构建出完整的图像Doree出版是基于自回归的架构之后的Doree2就转向扩散模型了包括Doree3也是扩散模型的思路此外除了这个DoreeCogView也是自回归的结构在2021年底之后的文生图模型除了这个slide上提到的Party模型是使用自回归结构外其他大多数都是走的这个扩散模型思路了包括Doree2 3还有那个我们待会要讲到的重点StableDiffusion系列我们接下来简称为SDSD系列利用OpenAI的Clip编码器作为文版编码器整个系列的模型都使用了Attention的机制到SD3还提出了这个叫做MDIT的结构这是比较新的结构随着这个时间越来越靠近2024年可以看到的一个趋势就是扩散模型统一了这个基本统一了文生图的这个领域包括Attention的比重也是越来越大这张slide我将介绍一下扩散模型的原理还有一些概念让一些对文生图领域不是很了解的听众有对这个推理的过程有一定的了解扩散模型的原理的精妙之处再与其模拟了物理世界中的扩散现象类比于现实世界就是类似于一滴墨水在纯净水中的扩散与复原的过程扩散分为正向扩散和逆向扩散过程正向扩散过程呢整个文生图系统通过逐步添加少量的随机扰动这个随机扰动放在模型里一般是随机的高斯噪声来逐渐破坏原始图像的信息经过多个step的迭代后原本的图像就会变成完全随机的高斯噪声图像每一个过程添加的高斯噪声量都是由一个调度器scheduler去完成调度的是为了确保这个破坏的过程必须是平稳的必须是渐进的是模型可预测的正向扩散的目的是为了让图片的那个伤增反向过程反向扩散过程就是在训练神经网络如何去对抗这一伤增的过程扩散模型必须学习去预测当前step下的噪声分量整个反向扩散推理过程呢就是不断地用模型去预测当前step的噪声然后根据采样sampler的那个策略比如DDPM或者DDIM来还原出下一步骤图像重复多个step就能得到清晰的图像具体可以看一下这个图片上的这个这是一个逆向扩散的过程每一个step都会在减去对应的噪声分量在最终的step就获得一个清晰的图像可以想到在逆向的扩散过程中反向扩散过程生成一张照片需要大概20到100个step每一个step都要调用一次模型的整个的forward函数每次forward都要走一遍这个正向的这个推理流程这大大增加了计算量stable diffusion模型可以成为实用的文生图模型的一个重要原因是它引入了latent space这个概念引入了潜在空间减少了计算量它把这个图像从pixel space转化为这个latent space将生成速度提高了7倍这一张幻灯片上展示的就是SD模型的结构这个模型SD用一个VAE用的是VAE将一个图片从像素空间转为潜在空间在潜在空间上完成了这个推理的计算在潜在空间中预测噪声减少了整个计算量不需要在整个的像素上进行计算这部分主要给大家讲解SD模型中噪声调度器与采样器的核心思想噪声调度器就是在逆向扩散的每一个step中当然也包括正向扩散控制添加或者去除噪声的比例采样器就是在反向扩散的过程中每一步会预测这个噪声的方向减去对应分量的噪声噪声调度器和采样器本身都是普通的数值计算流程并不是一个神经网络它是使用特定的算法实现对噪声水平的精准控制和去除噪声会在第一一般来说会在第一个反向扩散的step中达到最大最后一个step中需要减去的噪声最小在SD模型的推理中主要有三个部分构成了现在的整个推理流程一个是文本编码器在SD1.5中用的是Clip一个是将前空间输出的最后的前空间图像翻译为这个输出的图像用的是VAE用VAE中间呢一般就是我们多个step的推理过程用的模型现在SD1.5用的是UNET里面是自己改进过的UNET用了很多的这个注意力机制Transformer BlockUNET过程一般占据95%以上总计算时间文本编码Clip和用于前空间转换的VAE模型一般不会超过5%的时间花销所以一般我们的这个优化都是针对预测噪声模型UNET的优化在SD1或者SD1.5中这个模型都是使用了这个注意力机制的UNET结构这里slide上展示了右边的一个图片是用Control NET辅助生成的SD结构因为后面呢我们在展示我们的数据中会涉及到Control NET所以现在先给这个听众讲解一下什么是Control NETControl NET是被设计用来在预训链的扩散模型比如Stable Diffusion的基础上添加某些额外的条件控制从而使生成的图像更具灵活性Control NET结合到SD推理的时候它一般会是每一层的输出或者对应层相对应层的输出会和SD模型的输出直接相加在向量那个层面上相加然后加回到SD模型的特征上最后来影响这个生成的效果这一张幻灯片给大家展示一下现在常用的文生图平台网站海逸CRS和LibLibAI是常用的文生图平台他们都将我们提到的文生图模型用在了自己的这个实践环境中除了刚才举例提到的海逸与LibLibAI之外Mailia相机和会挖也是两个比较成功的商业案例他们将文生图模型用于自己的业务场景之中基于客户服务比如Mailia给用户提供了AI换脸服务会挖则用于生成他们的AI模特除此之外他们还提供一些例如广告生成还有游戏角色生成这样的服务接下来是第二部分文生图领域所面临的一些挑战左侧图表展示了SD1.5到FLAX模型的尺寸变化随着参数量的增加模型对于算力和显存的要求也在增加右侧图表展示了主流推理显卡的算力与显存容量的配比所以我们面临的第一个挑战是如何在有限的硬件资源下实现文生图应用的高效部署右侧图片展示了文生图服务的部署架构不同用户可以通过网络接口上传定制化的模型并且进行图片的生成GPU服务器会根据收到的用户请求对显存中的模型进行切换或者是加载然后执行文生图的计算任务这样的架构会面临三个挑战第一个挑战是如何对海量的定制化模型进行实时的变异优化第二个挑战是如何在多样化的用户出图尺寸请求下保证GPU性能的最优第三个挑战是如何减少频繁的模型切换所导致的GPU资源的闲置与浪费针对这些挑战目前已经有一些主流的解决方案它们在出图效率框架的灵活性方面各有优势但总的来说出图效率低模型编译优化时间长对ControNet和Laura这两种扩展的支持角弱仍然是阻碍文生图应用大幅落地的主要因素为了解决这些问题我们结合实际的业务场景做了大量的性能分析和优化工作接下来为大家详细的介绍首先介绍我们的算子优化工作我们使用英伟达提供的Nset System软件对文生图流程进行性能分析以SD1.5为例左侧展示了文生图不同组件的耗时占比其中Text Encoder占比大约1%Unit占比大约95%Image Decoder占比大约4%因此Unit是文生图中的性能瓶颈我们进一步对Unit进行拆解之后可以得到大约四类的算子这四类算子每一种有一个自己的耗时占比其中矩阵程卷积和注意力算子属于是计算密集算子它们的占比大约是60%GroupNorm LayerNorm这些算子是仿存密集算子它们的占比大约在40%基于以上拆解我们针对性的进行了算子性能的优化对于矩阵程和卷积算子我们使用调整矩阵分块大小片上数据附用多级流水线和分块调度这些手段进行性能优化单算子大约有20%到最高3倍的性能提升端到端取得了20%的性能收益对于注意力算子我们在社区融合优化的基础之上利用注意力头的特别的尺寸信息对这个算子进行定制优化单算子大概有10%到40%的性能提升多大端带来了5%的性能收益对于group normlay norm等仿存密集算子我们主要采用算子融合single path reduce向量读写等方法提行优化平均单算子的性能提升在2倍以上端到端取得了30%的性能收益我们可以看到仿存密集算子的优化是整网优化收益的最重要的来源这个收益占比达到了50%以上同时也可以观察到在一化完成后矩阵程卷积和注意力等计算密集算子的占比达到了90%这也说明文生图是一个典型的算力密集型的任务负载算子性能的优化带来非常显著的性能收益多到端可以提升50%左右然而用户随时可能会上传新的模型要想让这些新的模型快速的能够把我们算子融化的能力用起来必须要经过模型编译调用的流程在SD场景这个流程长达30分钟非常影响用户的使用体验因此呢模型编译时长的优化对于算子性能的发挥至关重要我们对现有的编译优化流程进行了一个耗时分析从左边这张图可以看到编译优化的绝大多部分时间都在进行算子调优其中矩阵程和卷积的算子调优耗时大约是27分钟占比90%数据重排逐元素等仿存密集算子的调优耗时大概是3分钟占比9%还剩下大约1分钟的时间主要进行计算图IR的转换以及序列化反序列化等操作我们使用两种方法对编译速度进行优化第一种方法呢是对矩阵程和卷积进行提前调优的动作也就是预调优我们调优之后把这个调优的结果保存在文件里面在推理服务进行启动时将预调优的结果从文件读入内存然后通过单例对象保存起来这样在模型编译优化的阶段我们就可以直接复用预调优的结果从而节省了矩阵程和卷积的调优开销经过这样的优化模型编译时间可以从30分钟下降到5分钟以内第二种方法呢是对模型结构进行缓存我们研发了一套权重追踪机制可以在不影响模型性能的前提下快速地对权重模型进行切换因此呢我们会对特定的模型进行预调优预编译然后对其结构进行哈希缓存后续再遇到类似的结构或者是一样的结构我们直接把权重提取出来进行替换即可通过这种方法我们可以把模型编译的时间从5分钟下降到5秒也就是说用户实际上并感知不到编译仪化的过程从GPU的角度来看只是完成了一次模型切换的过程而对于实际线上业务来说实际上则是完成了一次新模型的线上优化从理论上来说做完上述的优化工作就已经足够进行线上服务了可以取得比较不错的推理性能然后在实际业务场景中出图尺寸是不断变化的我们观察到一个现象模型在经过编译仪化之后在宽1024像素高1024像素的尺寸下模型的算力利用率指标MFU可以达到70%这是一个非常理想的数字但是在用户切换出图尺寸到1536像素时大部分的矩阵程或者是卷积的实现效率不再是最优的它们的分块策略也不再是最优的因此模型整体的MFU指标会出现明显的下滑要想在任意的尺寸下都达到比较高的MFU指标我们需要对用户输入的任意尺寸所对应的矩阵程和卷积进行预调U那随之而来我们会面临一个问题出图尺寸的组合是无限的不可能一一没举如何对搜索空间进行减枝同时不影响端到端的性能呢为了解决这个问题我们对不同显卡的硬件进行了性能建模利用理论模型确定矩阵程和卷积尺寸搜索的边界从而实现在短时间内完成对SD模型中所有矩阵程和卷积的所有尺寸的便利调U通过这种方法我们的MFU在任意尺寸下都能够保持在较高的水位在随机尺寸出图场景下这个方法能够带来大约5%到10%的多大端的性能收益这张图展示了我们推理框架的基本结构大致可以分为四层最上层是计算图表示与转换层主要负责计算图的解析与转换第二层是图优化层这里对注意力、规一化等操作进行计算图层面的融合优化第三层是运行实层主要负责显存的管理还有推理流程的调用第四层是高性能算子层主要包括算子调U、算子性能建模、手写模板或者是CUDN、CUDUS等算子调用的逻辑我们很大程度上借鉴了英伟达开源的Touch to Tensha RT框架的设计与实现并在此基础上根据业务需要加入了一些额外的功能基于前面所述工作这套方案在各种实际业务场景下都取得了不错的性能收益这张图展示了SD1.5在不同显卡、不同尺寸、还有功能组合下取得的性能收益横坐标注明了测试的机型、尺寸、还有功能组合测试的迭代次数是20次左色的纵坐标表示生涂时间、单位10秒蓝色柱子表示社区开源PiTouch的性能是我们的性能对比基准橙色柱子表示我们优化后的性能柱子的高度越小,代表性能越好绿色的数据点表示我们相比于PiTouch的加速比可以看到在SD1.5场景下在各种各样的分辨率下我们的加速比最低是1.23最高可以达到1.66平均来说我们的性能收益在30%到40%之间另外我们还可以观察到在同样的测试条件下是否引入CtrlNet对于我们的加速比没有显著的影响这得益于我们的方案对于CtrlNet等扩展的明火支持也就是说我们不仅对SD基础模型进行了加速我们还可以对CtrlNet模型进行加速这是一个非常重要的特性这张图片展示了我们在SDXL模型上的性能表现SDXL模型的参数量是SD1.5的3倍结构上也有所调整这为性能优化创造了更加有利的条件因此在SD模型上在SDXL模型上我们的加速比最高达到了1.94平均的性能收益在40%到50%之间也就是说SDXL模型的性能收益整体上要好于SD1.5这张图展示了我们在Flux模型上的性能收益Flux模型的参数量达到了11B对显存和精度的要求比较高同时Flux模型的主体从UNet转向了以矩针程和注意力为主的DIT性能优化空间要小于SD系列的模型尽管如此我们在Flux模型上的加速比依然可以达到27%到45%之间此外我们同时还支持了BF16和IP8两种精度从而能够适应不同的业务场景的需要以上就是我们在文生图推理优化测所做的主要的工作最后我们进行一个简单的总结与回顾我们针对文生图场景提出了一套推理优化方案这套方案在性能提升和业务成本下降方面取得了不错的成果首先性能上可以获得最高1.9倍的性能提速可以大大提升用户的出图体验其次将文生图模型的编译优化时长从原本所需要的30分钟优化到了5秒钟大大增强了模型适时优化的能力让用户感知不到编译的过程另外针对不同的音味达硬件我们进行了性能建模端到端可以带来5%到10%的额外的性能提升在实际业务场景中我们的方案可以大幅缩减文生图的时长减少因模型编译和切换所导致的GPU资源的闲置和浪费最终实现30%到50%的业务成本的下降未来我们将聚焦于以下几个方向首先是文生视频文生视频是AIGC领域下一个主流的方向目前我们已经在文生视频场景开展了一些工作并且初步获得了20%左右的性能收益其次我们会对文生图和文生视频的模型进行调度优化从而达到显存和算力的充分利用第三我们会进行分布式推理框架的实现并且对计算和通信进行重叠优化在这方面我们也已经开展了部分工作并且取得了37%的性能收益最后我们会持续的适配最新的架构从而能够在最新GPU上实现更低的出图延迟我们的分享到此结束感谢大家的观看期待有机会和大家进行交流再见再见再见再见