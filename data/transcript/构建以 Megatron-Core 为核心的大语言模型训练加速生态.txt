尊敬的各位同行,大家好,我叫黄俊,来自阿里云人工智能平台派团队,非常荣幸有这个机会给大家分享最近我们和英伟达Magatron Core团队在大语言模型及多么太大模型方面的训练实践和训练优化相关的合作工作。本次的分享主要包括以下几个方面,首先简单回顾一下PiMagatron Patch的核心定位和相关功能,然后分别从性能优化,功能扩展两个方面介绍我们在分布式优化器卸载,多么太大模型分布式训练的支持,以及DeepSeq V3相关的训练实践这三个主要的更新,最后对下一步的工作做个简单的展望。首先简单回顾一下PiMagatron Patch,我们知道英伟达研发的Magatron Core是一个成熟的大模型,大规模分布式训练框架,包含了训练超大规模的大语言模型所需的核心的关键技术,比如说各类模型并行的支持,算子优化,通信优化,显存优化,混合精度训练,以及混合专家训练等等。它也是业界训练大模型最常用的技术引擎。虽然Magatron Core的功能非常强大,它在实际用户使用的友好性方面,还有一定的提升空间。比如说,模型命名空间和Huggen Face不一样,那要快速拉起一个Huggen Face的开源模型,需要进行精细的模型转换和精度对齐。另外,它也缺乏显存非常有限情况下的训练拉起的支持,以及对文图多模态大模型的训练支持也不够全面等等。为了帮助用户可以更方便地使用Magatron Core框架,我们研发了Pi Magatron Patch这样一个工具库,提供了丰富的Huggen Face到Magatron的双向模型全重转换工具。在保证精度无损的同时,也提供了丰富的开源大模型,包括Deep Seek,Lama,千问等系列的训练脚本。支持了Dense,MOE,多莫泰等不同类型的大模型训练。在此基础上,我们在Pi凌峻智认平台上提供了从数据处理,模型预训练,后训练,到模型部署的全流程模型开发工具,以及最佳实践。接下来,我分别从显存优化,多模态大模型的支持,以及Deep Seek V3的训练实践三个方面,介绍我们最近在Pi Magatron Patch上的一些工作。我们先来看看显存优化。这里我们主要介绍分布式优化器CPU卸载方案。很多用户在实际训练大模型过程中面临的问题是,我没有那么多显卡,但是我又希望去微调一个大模型。那我们知道CPU Offloading是一种比较成熟的降显存的技术。简单的来说就是以时间换空间,通过将训练过程中的部分参数计算从GPU转移到CPU上进行,以实现降显存的目的。而优化器的CPU卸载,就是指把优化器状态和参数更新计算放置到CPU上进行。这样大概能节省多少显存呢?我们可以做一个简单的估算。假设模型的参数量是M,那么计算采用BF16精度的话,使用ADAM优化器。那参数的存储和前项计算分别需要4M和2M,那T度需要4M。ADAM的状态需要4加4等于8M的存储。这时候如果不算其他中间显存的占用的话,一共是18M。这里面优化器状态的更新需要的显存是12M。我们可以直观感受下在GPU受限的情况下,不同类型模型不同部分的单卡显存占用情况。这张图从左到右分别代表了Lama3 3.1 8B在单卡上,70B在8卡上,405B在32卡上,以及Mistro 18x7B的模型在4卡上,8x22B的模型在16卡情况下的每张卡的显存占用情况。绿色的部分代表全精度参数和优化器状态,橙色的部分代表激活值的内存,蓝色部分代表其他的中间结果。中间这条虚线代表80G显存的这个位置。可以看到如果绿色和橙色的部分都放置到CPU上进行存储,那么在上述GPU卡数限制下,对应的模型理论上都可以拉起来。我们和英伟达Macron Core团队一起基于FSDP加上激活检查点,以及优化器卸载技术,做了一系列的实际的实验,来探索拉起不同规模的模型最少需要多少张显卡。假设Sequence Lens固定为8192,Micro Batch Size固定为1,在这种情况下,对于LAMA 3.1 8B的模型,如果只开3D并行,则需要4张80G显存的显卡才能拉起。如果打开FSDP加上激活检查点的话,则最少需要两张显卡。如果再加上优化器卸载,则用一张卡就可以拉起。那对于LAMA 3.1 405B的模型,如果只开3D并行,则128张卡是无法拉起的。如果FSDP加上激活检查点,那128张卡是可以拉起的。这时候如果进一步打开优化器卸载的话,则用最少32张80G显存的显卡,既可以拉起训练。其他规模的模型也验证了优化器卸载在显存优化方面的有效性。为此,我们非常有必要在Macron Core当中去支持这项技术。下面简单介绍下我们在Macron Core上实现的优化器卸载技术方案。第一个是核心的工程设计。首先在顶层API方面,我们沿用了Pytosch风格的优化器接口。其次引入了主优化器和主优化器分离的设计,实现了优化器卸载加载的统一调度,提升了效率和灵活性。那么在此基础上,我们进一步提供了卸载的比例变化等灵活的能力。另外,CPU卸载技术本质上是一个用时间换空间的技术。大量的计算放到CPU执行,以及CPU之间的读写,使得计算性能会有大幅的下降。那为了减缓这种性能损失呢,我们也进一步的设计了,我们叫Naive的Optimizer Overlapping技术。下面会进行详细的阐述。首先我们来看一下混合设备优化器,这里我们简称为HDO的设计。与支持单一设备张量的优化器不一样。为了实现位于不同设备上的多个张量的参数灵活更新,以及性能优化,HDO被实现为多个子优化器的集合。就是这个图里的SubOpt,可以理解为按张量的维度进行均匀的切分。每个子优化器负责单一设备上数个张量的参数更新,而HDO相当于一个调度器。主要用于实现主优化器与子优化器之间的优化状态同步,参数和梯度的传递,以及由此引发的CUDA同步问题。在优化器初始阶段,相对于普通的ADEM优化器,HDO额外要求用户指定CPU和CUDA上的子优化器类型,以及相应的负载比例。随后基于这一个比例,对参数进行拆分,再重新构造对应设备上的子优化器。那么在训练阶段,由于LRSchedule等外部因素可能会对优化器参数产生影响,那么在子优化器参数更新前,HDO会将所有的状态同步到子优化器。如果子优化器在CPU上更新,也会将GPU梯度复制到CPU上。那由于子优化器也可能修改参数组的状态,在更新结束以后,每个子优化器也会将这些状态同步到主优化器,从而保证与非优化器卸载训练收敛的一致性。除了支持完整的优化器保存读取特性,HDO同时也支持加载优化器时切换,优化器卸载比例。具有更大的灵活性。如前文所述,HDO内部由多个子优化器负责实际的参数更新,在每个参数更新阶段开始前或结束后,分别进行HDO及子优化器间的参数同步。在保存优化器状态时,由于HDO内的数据与子优化器一致,那么HDO的状态字典即包含恢复优化器所需的全部信息。那么在加载权重时,由于子优化器需要基于FP32的参数进行更新,我们引入了状态字典加载前以及加载后的hook。将HDO的状态中的半精度参数数据临时用保存的FP32替换后同步到子优化器,然后再替换回来。然后再将每个子优化器的状态移动到其对应的设备。通过在运行时更新子优化器的对应设备,我们能够将在一个负载比例下保存的模型在另一个优化器卸载配置上拉起训练。前面提到HDO初始化时会额外要求用户指定相应的卸载比例。我们在更早的PyMagicTron Patch的版本中,实现了一版自动调整卸载预值的功能,以简化用户的使用成本。我们把显存的占用划分成可根据模型参数估算的部分,简称MD,以及不可估算的非模型数据部分,简称NMD。那么在模型训练过程中,疑似迭代可分为前向后向以及参数更新这两部分,分别具有不同的非模型数据大小。通常前向反向的非模型数据会多于参数更新部,在更新参数的时候,根据当前显存使用情况,更新各个Trunk的设备。在MagicTron Core里面,模型数据包括不可移动的Fixed MD,即DDP内置的参数T2的Buffer,以及可移动的Trunk,在这个图里面就是指代表,用C1到C5来代表。当观察到最大的非模型数据,加上已使用的模型数据大于安全预值时,就将显存上的部分Trunk移动到内存中去做更新。反之,就移动更多的Trunk到Cuda设备上。我们采用贪心策略,尽可能地让GPU拥有更多的Trunk。在实际的训练中,由于显存碎片的存在,第二个迭代的非模型数据往往大于第一个迭代的值,但小于第一次迭代前向反向,以及参数更新步的值之和。在参数更新结束后,我们采用WarmUp。当本次迭代非模型数据增加时,使用当前迭代各步骤的非模型数据之和,作为下一个迭代非模型数据的估计值。CPU Offloading虽然能够大幅度地降低训练的GPU需求,但本质上是用时间换空间。大量的CPU计算以及参数的拷贝导致训练耗时会变长。与QDA上的参数更新相比,基于CPU的参数更新,性能会成为优化器卸载技术的是否可用的一个关键因素。通常优化器卸载的时间包含三个部分,将QDA梯度同步到CPU,就是这里的D2H。第二个是在CPU上进行参数更新,也就是C步。然后最后是将更新后的CPU参数同步到QDA,这里我们把它简称为H2D。为了尽可能的提升性能,我们提出了一项naive optimizer overlapping的技术,通过利用Pytorch的QDA Stream机制,实现通信与计算的重叠,将CPUQDA间的数据拷贝的时间,尽可能的掩盖在CPU的计算之下。在naive optimizer overlapping中,我们的一个主要的改进,是将CPU子优化器进一步的打散,将一个CPU参数对应一个CPU优化器,同时令所有的拷贝非阻塞化。这个改进允许HDO在单个梯度张量同步完成后,立即调用CPU的参数更新,无需等待所有的梯度完成拷贝。也允许了CPU参数更新完成后,立刻非阻塞地拷贝到GPU,从而实现重叠。为了保证多个Cuda Stream以及CPU之间正确的数据同步关系,同时尽可能地避免影响性能,我们仅在关键位置引入Cuda Event,来避免额外的流通步带来的开销。下图表示了一个理想的优化器重叠场景,其中每个参数张量的更新时间,与两次数据传输接近。当流水线热身结束后,也就是这里面的Time Step 2,可以看到HDO在两个Cuda Stream上进行双向的数据拷贝的同时,CPU也在同时进行参数更新,这个方式最大化地掩盖了冗长的数据传输时间。在我们测试Lama2 70B的训练中,我们观察到该优化能够减少约10%的端到端的时间。我们简单通过一些实验验证,验证了基于CPU负载、卸载技术带来的显存收益和性能的均衡。比如在Lama3 8B的模型上,不做模型切分,只开Offload的情况下,在某80G显存的A卡上,单卡就可以拉起训练。而不开Offload的话,需要打开TP使用两卡才能拉起。另外在吞吐方面,单卡开Offload和两卡不开相比,吞吐的损失仅仅损失了7.5%,就是这个表当中的两个红色的数字。同样我们在更大的模型,也就是70B大模型,在长上下文的情况下,在相同的卡形上也进行了对比。打开Optimizer Offloading,可以在32张卡上拉起16K上下文的模型训练。也是80G显存的A卡。如果打开激活检查点和CPU Offloading,可以在32张卡上拉起64K上下文的模型训练。除了重命模型以外,我们在MOE模型上也做了相关的测试。这里我们以千万二57B A14B为例,也就是一个总参数量为570亿参数的一个MOE模型。每次推理会用到14B的参数。那么在TP2,PP2和EP4的这样一个分布式的情况下,我们只打开激活检查点,显存尚不足以支持2048序列长度的训练。但如果打开Optimizer Offloading,则可以支持4096长度上下文的模型训练。好,接下来介绍下我们在多摩泰大模型上做的相关的功能扩展。这里我们以千万二VL多摩泰大模型为例。千万二VL采用了专门设计的多摩泰编码器架构,可以同时接受文本、图像以及视频作为模型输入,并将它们转换成统一形式的表征向量,然后送入LIM解码器。这种设计使得模型能够在同一个空间内,有效地捕捉到跨模态的信息关联。与Lava等多摩泰大模型不同。千万二VL的视觉输入不需要缩放到固定的分辨率大小,而是根据图像、视频数据的原始分辨率,动态的生成不同长度的视觉表征向量。这一特性被称为动态分辨率。在支持千万二VL的过程中,主要需要解决四个方面的主要挑战,包括一个是并行的模型权重转换,第二个是多摩泰数据的高效加载和动态分辨率的支持,第三个是视觉编码器和大语言模型解码器的计算负载均衡,以及长序列情况下的显存消耗问题。下面我们分别从这几个角度进行详细的阐述。首先我们来看一下并行的模型权重转换问题。上图表示了一个Huggen Face实现的GQA权重转换,到Megatron的正确和错误的流程对比,上半部分表示错误的实现,下半部分是正确的实现。其中每个块表示一个head的权重,边框和内部的颜色分别代表这个head对应的类型,以及被Megatron理解的成的类型。那么在Megatron Transformer库中,Huggen Face采用QProject,KProject,VProject这三个算子来计算QKV的Status,随后再调用函数,将同一个Query Group内的KVStatus数量与Query对齐,进而调用Attention函数。在Megatron中,由于张量并行的存在,Attention Module采用了Linear的QKV对QKV进行统一的计算。为了降低节点间的通信量,这个并行的Linear将QKV均匀地切分到各个Node上,使每个Node内就能计算一部分Attention的结果,而不需要对QKVStatus进行同步。那在这个case下,如果直接拼接QKV的权重张量,就是图的上半部分,在进行TP的切分,可以明显地看到大部分的head位置出现了问题,Q和KV会被分配到不同的节点上,但是Megatron仍会按照QKV的比例进行读取,造成结果错误。因此我们需要将QKV权重进行一定的转换,按照Query Group的顺序进行拼接,如图的下半部分,拼接完后再切分,才能保证Attention的正确性。具体的说,在进行TP等于2的切分时,PiMegatronPatch先将QKV和Value这三个Tensor分别reshape成一个4D的Tensor,接着沿着第二个维度拼接到一起,然后再沿着第一个维度进行算子的拆分,才能确保并行加载Checkpoint后的起步的Loss值的误差偏移,在合理的范围。为了验证PiMegatronPatch内多模态模型转换的正确性,我们采用VLM EVO Kit对转换前后的千万RVL7B模型在多个数据集上的表现进行评估,结果如下图所示。其中Reference是VLM EVO Kit提供的在Seed Bench以及MMBench上的评估分数。Official是官方的权重在测试环境内的实测分数。可以看到,转换前后的模型评估分数完全一致,且均与开源的Leadboard指标接近,有效说明了转换代码的正确性。然后来介绍一下多模态数据的加载。当前的Megatron在多模态数据加载方面存在几个主要的问题。一是支持的数据加载格式简单,例如仅支持一条样本,至多包含一张图或者一个视频,应用场景有限。其次,现有的Task Encoder输出格式固定,灵活性不大,难以高效地支持模型的训练。在实际的开发中,千万2VL同时遇到了上述两类障碍。千万2VL基于ChatML的格式设计输入,应当支持单个样本中多张图片、视频、多轮对话等复杂功能。但这类数据无法使用当前应伟达的AnagonCOO的代码进行读取。此外,现有的Task Encoder仅支持将图片缩放到固定大小,再传入模型训练。因此,难以支持千万2VL独有的动态分辨率特性。为了解决上述问题,我们首先对内置的DataLoader代码做了大量的扩展,使其能够基于输入的原始多摩态数据的数据构造用于动态分辨率训练的图像切片序列以及位置信息。同时支持了可自定义prompt多轮对话,包含任意数量的图像或视频的对话样本的数据处理。在此基础上,我们设计了一套自动化的运行脚本,能够基于用户给定的Sharp GPT格式,Sharp GPT格式的数据集路径自动转换成Anagon可读取的Web Dataset数据集,绕过当前的Anagon数据集准备的交互流程,加快数据转换效率,同时降低用户的学习成本。此外,对于数量不定的图像,为了能够在Anagon测自动的解码,Web Dataset支持了将其用Numpy Array的形式保存。然而由于JPG格式到NDArray之间的数据格式的差异,在实际的测试中,我们发现这会造成最终的数据集文件,体积出现几倍到几十倍的增加。为了解决这个问题,我们在运行时,像Web Dataset的编辑码器模块增加了构次函数,使其能够支持图像文件列表的自动编辑码。实现在按照原始二进制数据保存的同时,能够被Anagon自动转换为图像张量的需求。通过上述优化,在Pi-Magicron Patch当中,经过转换的Share GPT格式的数据,相对于其原始的总大小几乎没有变化。除了解决多么泰数据加载的问题,我们对视觉特征的处理流程也做了优化。与LM相比,多么泰大模型先对视觉数据进行编码,在于文本特征拼接后送入大语言模型进行推理。与基于静态分辨率的多么泰大模型不同,复杂的多么泰数据格式以及动态分辨率共同使得千分二VL这一过程变得更加复杂。不仅显著影响了训练的效率,也与能否应用比如说张量并行,以及通信和通信的overlap等技术相关。与静态分辨率使用固定数量视觉的token不同,动态分辨率技术允许模型依据输入图像的大小,将其编码成不定数量的视觉特征。这一改进使得模型对于高分辨率图像的细节捕捉能力获得了提升,同时也优化了低分辨率图像的推理性能。左图是一个千分二VL的训练数据的一个简单示例,针对不同长度的视觉token,最终它们会被嵌入到文本序列中,与同一个batch的数据拼合后送入大语言模型的解码器。在这个过程中主要存在两个难点,一个是如何对同一个批次内的多个视觉输入高效的获得特征表示,二是如何拼合数据以支持原生性能的优化开关。由于视觉输入的分辨率具有很大的不确定性,为了将同一批次的不同长度视觉数据转换成推理所需的特征表示,常见的做法是将每个视觉数据填充到相同长度后,统一送入到视觉模型。尽管这一方式实现简洁,但填充的token不仅造成了显存的浪费,同时也影响了视觉编码器的吞吐。当同一个批次数据中,同时包含高分辨率视频以及低分辨率图像,由于视觉token数量的显著差异,这将造成极为显著的性能浪费。对此,PyMectonPatch借鉴了sequence packing的做法,将同一批次内的所有视觉输入打包后调用变长的attention,来避免填充操作带来的显存和性能损耗。在生存视觉特征表示后,基于预先构造好的眼膜张量,视觉编码器的输出会被依次填入文本表示的对应位置,替换掉默认的站位符。在不使用任何性能优化技术的情况下,这一实现并不会带来什么问题。然而,当开启序列并行以后,Language Embedding的模块的输出会自动的reduce scatter到各个TP rank,导致基于原始输入构造的完整眼膜张量无法使用。为了解决这一问题,同时避免冗余通信,PyMecton Patch针对性的修改了Mecton Core中实现的Language Embedding模块,在输入序列拆分前增加利用眼膜张量替换文本表示的操作。不干扰序列并行特征的特性的正常运行,在实际序列并行机制的基础上,通过进一步的应用TP Communication Overlap的特性,对于CM2VL70B,我们在4G32卡的某型号A卡上观察到了接近6%的性能提升。除了数据加载,多模态模型在训练过程中会因为负载均衡的问题导致吞吐的降低,主要包括两个方面。首先,由于模型结构不一致,视觉编码器与大语言模型解码器的前向速度有差异,导致MicroBatch内各个GPU间存在负载不均衡。二,由于视觉编码器支持动态分辨率特性,对每个MicroBatch实际的前向的Token数并不一致,使得每个MicroBatch的计算量略有差异,也会出现各个GPU间存在负载不均衡的情况。为了解决上述的负载不均衡问题,进一步的提升训练吞吐,在原生的MicroBatch的基础上,我们从两个方面对现有的框架进行了改进尝试。一是支持基于非均匀切分策略间的负载均衡,二是拓展模型以模型实现,以支持虚拟流水线并行特性。具体来说,基于非均匀切分策略间的负载均衡是指,在模型总参数量较小或者PP较大的并行配置下,PPRank0包含的视觉编码器带来的额外计算,它的计算量相对于原有的大语言模型解码器的计算量,比例会有显著的提升。由于在流水线当中最慢的步骤决定了整体的性能,那么我们应用Megro内的非均匀切分特性,将部分计算量转移到其他的PPRank上,能有效的提升整体的训练性能。为了支持基于非均匀切分的继续预计量或微调功能,我们同步更新了千万二位遥的转换模型。用户可以通过控制MPPP0Layers的环境变量,获得非均匀切分的模型权重文件。虚拟流水线并行是MegroChun在流水线并行实现上的一个重要改进,它通过将同一个PPStage内的Transformer层分配到多个GPU,进一步的打碎了计算,通过增大通信量的方式使得负载更加均衡,降低了GlobalBatch内的Bubble的比例。然而对于英伟达内置的所有的多摩胎模型,MegroChun Core其实是不支持启用虚拟流水线并行的。考虑到视觉编码器的整体计算量不需要独立的GPU,我们进一步改进了千万二VL的实现,移除了Encode流水线并行的支持,并将千万二VL的模型类型设置为大语言模型的Encode or Decode,这样一来就可以启用虚拟流水线并行的选项。这里简单展示一下虚拟流水线并行带来的优化效果。我们通过打开SP和TP通信Overlap的开关,模型可以采用TP4、PP4的配置进行训练。那通过调整虚拟流水线并行,相对于TP8、PP4的最佳性能有额外15%的性能提升。由于各个GPU的机内、机间的带宽差异,那么在不同的GPU上,最佳的优化方案其实是会有所不一样。最后来看一下我们在DeepSeek V3训练实践方面的一些尝试。早在DeepSeek V2阶段,Macron就进行了相关的支持。那V3和V2的差异主要表现在几个方面。首先在模型方面支持了MTP,也就是Multi-Token Prediction。在MOE的Lot Balancing方面,V3使用的是Jopolis的负载均衡。在DeepSeek V2阶段,我们基于Macron Patch和Macron Core,已经完成了基本功能的支持和精度验证。包括HagenFace到Macron Core的模型格式转换。MLA的实现,MOE layer的frequency的支持。细粒度的和Share的专家的支持。负载均衡的Loss实现。同时我们也支持了包括预训练、微调、评估的完整流程。参考事例可以参考如下开源代码库。在V2的基础上,我们进一步在算法功能和AI Infra方面和V3进行了对齐。目前在算法方面的优化包括MOE相关的负载均衡方法。MTP我们已经支持了。在微调完整流程的优化我们也在进行中。在AI Infra方面,FP8的混合精度训练功能上已经完成了。只是性能和收敛性还在进一步的优化当中。另外我们也在尝试其他一些降显存和加速技术。比如FSDP加CPU Offloading的降显存技术,以及高性能的AutoAuto通信算子等等。但后面高性能AutoAuto通信算子DeepSeq自己也开源了一些相关的库。我们后面也会参考。那么除了V3的训练支持,我们接下来会进一步的探索深度推理模型的训练过程。这里面主要包括和强化学习框架的一些对接,以及完整的SFT和强化学习的训练流程。同时也会及时更新到社区进行期待。好,我的报告到此结束。谢谢大家,欢迎进行交流。请不吝点赞 订阅 订阅 转发 打赏支持明镜与点点栏目。