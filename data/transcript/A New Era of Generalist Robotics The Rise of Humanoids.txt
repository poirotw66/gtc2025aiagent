 Hello, hello. Wow, this crowd looks amazing. As Madison just said, my name is Tiffany Janssen, and I'm going to be your moderator today. A little bit about me, I am the founder of TiffinTech, and I don't know about you, but I have been counting down the days for this panel. Humanoids have been in the, we've seen a lot of advancements with them very recently, and to be able to sit down with some of the leaders in this space and hear from them is truly incredible, not only to learn where we are now, but where we are headed. Let's start with a round of introductions. I'll start with Bernd. Sure. So my name is Bernd Barnick. I'm the founder and CEO of OneX, and we're on a mission to create an abundance of labor through these safe, intelligent humanoids, and we really believe that to truly get to intelligence, these robots, they need to live and learn among us. And that's why we think consumer has to happen first, to really be able to experience all of the nuances that is kind of human life, and then use that intelligence to be able to do useful labor in all of the verticals down the line, right? Hospitals, elderly care, retail factories, logistics. It's exciting. Yeah. Hi. I'm Deepak Pathak, and I'm CEO and co-founder of Skilled AI. At Skilled, what we are doing is we are building a general brain for robotics. Our thesis is that we can have a single shared model, because robotics is a field which is anyway scarce of data. We might as well use everything that we are available from any platform, any task, any scenarios. So think of it as like a large scale foundation model you can use for any robot, any hardware, any task, any scenarios. Pras Feligapudi, the CTO at Agility Robotics. And at Agility, our humanoid digit is made for work, and we're bringing it out to manufacturing and logistics use cases today. We feel that the best way to get the technology out there and learn from it is to be able to get real customers and real deployments doing work, and that's what we've been focused on, in getting our robot out there and in the workforce. Aaron Saunders, CTO of Boston Amics. I've been working on humanoids before they were cool. And at Boston Amics, you know, our mission has been the same for a long time. It's to make robots real. We've shipped a couple thousand robots. The humanoid is the latest kind of announcement for us. And we're really wanting to bring a product to market that can do real useful work, right? So to do work that removes people from dirty, dull, dangerous things. And that's the thing that we've been up to for a long time. And I think there's more work to do yet, but we're pretty excited about where we're going. Hi, everyone. I'm Jim Fan. I'm the co-lead of NVIDIA Gear Lab and also Project ISA Groot. Groot is NVIDIA's moonshot initiative at building the foundation model, the robot brain, for humanoid robots. And Groot also represents our strategy for the next generation computing platform for physical AI. And we're also on a mission to democratize physical AI. In fact, yesterday at Jensen's keynote, we announced open sourcing of the Groot N1 model, which is the world's first open humanoid robot foundation model. It's only two billion parameters, and it punches above its weights. You are basically holding the world's state-of-the-art autonomous humanoid intelligence in the palm of your hand. And I would also like to say that, just like everyone else on this panel, I started working robotics before it was sexy. And today I see the full house here. So I'm just really, really glad that it becomes sexy today. So thank you all so much. All of you made my day. Thank you all for being here. I know we're all excited. We had a call before our panel, obviously. And in that conversation, I can't recall who exactly it was, but someone shared on the call that robotics is the oldest application of AI. And historically, it's moved the slowest. Not the case anymore, I would say. And what has changed? I think I raised that question in that call. Yeah. So, well, I think the biggest change is Jensen now is paying attention to robotics. Jensen's got a Midas touch, right? Any field he puts his finger on is going to scale exponentially, and we call Jensen's scaling law. So jokes aside, I think robotics, one of the oldest field as long as AI itself existed. And the reason that robotics is so hard is because of the Moravec's paradox. So what this paradox says is the things that are easy for humans are actually really hard for machines, and vice versa, right? The things that we kind of find extremely hard, like creative writing, may actually not be that hard for machines. And that's why the LOMs, like the NLP, like computer vision, is being solved much better than robotics these days. So we are facing this paradox right now. And now, what has changed? I would say a couple of aspects. One is on the model side. Because of large foundation models like LOMs, the chat GPT moment, now we have models that can do reasoning. And we have multimodal models that understands computer vision, right? Open vocabulary, understanding of the 3D visual world much, much better than what we had before. And these are necessary but insufficient conditions to solve robotics. Like you've got to solve vision to have a really good vision system before you even talk about having a general purpose robot. So I think the rest of the models are becoming really, really good that we can start to tackle robotics a lot more systematically. So that's number one. And number two, what has changed is on the data side. So, you know, unlike LOMs, and I'm quoting Ilya Suskever here, he says the internet is the fossil fuel, right, for AI. Well, robotics doesn't even have the fossil fuel. At least for LOMs, you can download text. You can scrape text from Wikipedia. Where do we scrape motor control? Where do we scrape all of those, like, robot trajectories? From the internet. You can't find it anywhere. So we've got to generate data. We've got to collect data at scale. And I think the advent of simulation, of GPU accelerated simulation, really makes these problems more tractable. Because now you can generate, like, 10 years worth of training data in maybe three hours worth of compute time. So that really takes us beyond this data paradox. So that's number two. And number three is on the hardware side. And we have some of the most extraordinary founders on some of the best robot hardware we have ever seen. And I think hardware has become a lot better and also a lot cheaper, right? Like, this year, we have seen hardware in the range of maybe 40K. That's the price of a car. And back in 2001, NASA built Robonaut, one of the first major humanoids, and it's like $1.5 million in 2001 USD, right? So it's finally becoming affordable that it will become mainstream very quickly. Aaron, I'd love to hear from you on this. In your intro, you mentioned you were in robotics before it was cool. So what do you think has changed? Yeah, I think that was a lot of terrain. So let me try to pick away at some pieces there. I do think the kind of closure of the sim to real gap is a big deal, right? So I think for a long time, the robotics community kind of struggled with creating a simulation environment that represented physics properly, and that was computationally efficient, right? So we could create very complex models that did a very good job of representing the physical world, but we couldn't run them at real time or faster than real time. So I think for me, the biggest change has probably been the ability to represent the physics of the real world in greater than real time, which lets you accelerate how many simulations you can explore and how you can use those simulations to develop new AI. And then, you know, the commoditization of so many bits and pieces. So I think we probably need to give a huge amount of credit to some adjacent industries, right? So consumer electronics have developed batteries and cameras, like, you know, the technology for perception, for seeing the world, for computing. When I look back, you know, even 10, 15 years, you know, most of the robot was full of PCBAs and wires and had a very small amount of battery capacity, and that's completely changed, right? We can put a huge amount of compute in, we can put tiny sensors in, they're power efficient. So I think the commoditization of the components, and I don't think it's so much about low cost. I know that's a big focus right now, but I think the reason we're seeing the era of the robotic startup is because there's a global supply chain full of really important pieces that you can put together as a puzzle piece. And so we've elevated the robotics community from the people trying to design every cog to people who can put those things together as a puzzle and basically be operating at a higher level. So now we have companies that are operating at the intelligence level that are developing applications, rather than spending all that same amount of capital and energy just making a physical machine stand. Yeah, I can add something to, I think, what Jim's point initially about, I think you put it very well about all the differences that have changed. I want to add that AI was not just the first application, sorry, robotics was not the first application of AI. It is what AI is. Like, if you look at the original documents of Turing, when he talked about AI, it was for robotics. He was like, you should make something that instead of building like an adult, make something that learns like a child. And then it can grow, you can put the same robot in a classroom, it will grow to be adult over time. It's a fascinating thought. And he had this thought in 1950s, right? Because language, vision, all of these things are cool. But if you look at the nature, they come much later in timeline compared to physical action. Like, for instance, LLMs, we are training on the data, it's from maybe last 100 years, 200 years, let's say 1000. We are not training more than 1000 years of data. Humans have been around for more than 1000 years. So it's not that language led to intelligence. It was the infrastructure already existed, our brain actually existed, which came through physical reasoning, which is why the impact is so big. Like if you don't have to explain a person to any person what robotics is, you can feel it because you do physical tasks every other day, every company gets impacted by robotics. And that's the main deal. So what has changed apart from all the factors that Jim mentioned, these are technical details, that is yes. But what has changed is completely how we approach robotics. Robotics so far has been the field of controls. Controls was driving robotics until I would say even three, four years ago, right? Now for those who have been in this area for a long time, they would know controls was not designed for robotics. Like control had its really limelight during World War II for like flying planes, missiles and everything. And then you had the craze for robotics began because of because of doing. And then people are what do we use? Well, let's use controls, which were biz, which was built for that. And that stuck around that stuck around for decades for 70 years. But it's not in the same spirit. It's not childlike learning in child you are not teaching them calculus first to learn how to walk, like figure out your joint movements and then learn what you are learning by experience. So learning by experience is the main change that has happened. Now we are seeing all the shift happening. And I mean, BD just released a video today, learning by experience. So that is, I would say, one major change has happened. Instead of programming experience, from that we have gone to learning by experience, which is a major shift how we think about robotics. I want to actually double click on that. So I think for me, right, and I've also been in the field for long enough to be in the classical controls regime. But a large part of what happened was the internet, right? If you think about it, like it's this enormous human experiment of like 30, close to 30 years of everyone around the world contributing to creating this enormous source of data, so we can train an AI, which is completely magical. And now what we're going to do is we're just going to ask all of you to do the same again in the next 30 years, just go around and be robots. And no, we're not going to do that. But we had that data. And therefore, that is what actually moved AI forwards, even though it started in robotics. And to me now, what it's all about is how do we use that data that exists to bootstrap to where your robot can do something useful? Because at that point, you can start to learn in the real world, where intelligence really comes from, right? But you have to get to where it is to a certain extent useful. So when I say, go grab me a Coke in the fridge, if the robot is able to do so half the time, then we have a feasible path to getting there. Because now we just need to say, okay, that worked, that didn't work. And we need to run it enough times. And it'll get really good at getting me a Coke in the fridge. And I think that is what we're seeing now with the advent of all of these multimodal LLMs, is that you can't really solve robotics or intelligence in general, I even think, through this approach. But you can get to where your system is useful enough that you can create a very efficient data flywheel at scale that doesn't require you to teleoperate the robots for everything it does. And that is probably the path to, if not AGI at least, very, very useful machines. And maybe also to AGI. We'll see down the road. I feel like we've got to circle back to the last part you just said there. But in the meantime, Paraz, I'm curious to hear your take. Yeah, I think echoing some of Aaron's comments here, you know, why robotics is kind of like coming back in. You know, AI started with robotics and then kind of progressed to all these other areas and then sort of circled back. Well, robotics is challenging for two reasons. One, hardware is hard and the world is unstructured, right? And if you look at how AI has evolved, right, and how robotics has evolved, a huge portion of robotics has been dealing with the hardware is hard problem and miniaturizing sensors like MEMS, building on actuator and drive technology, energy storage technology, all of that stuff had to be solved. Even platforms like Arduino democratized people's ability to just get things to move in the real world at all and bring that to people so they weren't reinventing the wheel every single time. On the AI side, we've been basically blocking our way through going from solving structured to increasingly unstructured problems. Solving problems of queries and prompts to APIs to simplified world models to now unstructured world models, where every piece of this puzzle has been sort of up-leveling the AI platforms, finding new ways to ingest data, taking the best practices of the previous sort of structure and how that worked, and then taking it to this next step of, okay, what if we remove some of these training wheels and now you're just looking at video coming from a self-driving car, or now you're just looking at egocentric video being captured by a robot's cameras, and what is going to happen next in this world. So I think there's a bit more of this progression and unlock that's been happening behind the scenes, and we're just seeing the culmination of that finally reach this tipping point where now, okay, we can go after this full problem of just interacting with the world in unstructured ways. I think the last thing you say there is so important, though, and talking about what has happened in hardware, and maybe one of the biggest things happening in the last few years is the robustness of hardware and the ability to make hardware that doesn't break when it interacts with the real world, because all of us have worked long in robotics, right? Experiments take quite a long time when you need to rebuild the robot every time you run it, but we're also now really at the point in hardware where we can have something learn in the real world and safely interact with the world without damaging itself for the world. And that's also a necessary condition, really, for this to progress. And that took a long time. That's a pretty hard problem. You know, even listening to you all and your thoughts around this, it brings up an interesting question. I think you all have very exciting and unique strategies and approaches. I'm really curious to hear your take on your strategies and approaches for things such as the role of AI moving from specialist to generalist models, or how you think through things such as the explosion of foundation models. Yeah, I can perhaps talk a bit about Groot's strategy, right? We're solving a really, really hard problem to build a general purpose brain for all kinds of different human robots, not just one. We also hope to get what we call crossing embodiment. So how do we tackle this problem? I would say there are two main principles. Principle number one is that the model itself, we want it to be as simple as possible. We want it to be as end-to-end as possible, to the point that it's basically photons to actions, right? You take pixels in from like a video camera, and then you directly output continuous floating point numbers, which are essentially the control values on the motor. And that is the model, end-to-end. There is no intermediate steps, as simple as possible. And why is this good? Because if we look at the field of NLP, and by the way, NLP is an extremely, perhaps the most successful field so far, that's been solved by AI. And I think as roboticists, we should copy homework, copy homework from someone that already worked. So for chatGPT, right? Before chatGPT, the field of NLP is kind of a mess, right? You have text summarization, machine translation, code generation that's using completely different data pipeline and training protocols and model architectures, sometimes not just one model. And then chatGPT came and just blew everything out of the waters, because it's simple, right? It maps any text to any other text, and that's it. Underneath is a transformer that maps a sequence of integers to another sequence of integers. And because it's so simple, you can unify all of the data, all of the problems into one model. And I think that's where robotics should copy homework from, to make the model as simple as possible. And the second principle is the data pipeline will actually be very complicated. Like all the things that surround the model will be very complicated. And that is because for robotics, as I said at the beginning, data is a huge problem. You cannot download motor control data from YouTube, from Wikipedia. You can't find it anywhere. So for Groot, our data strategy can organize into a pyramid, right? Now close your eyes, visualize a pyramid, right? At the top, you have real robot data that's going to be highest quality, because there is no domain gap, right? You're collected using teleoperation in the real world. But that's got to be quite limited, not very scalable, because we're limited by the fundamental physics limit of 24 hours per robot per day, right? That's it. And it's really hard to scale in the world of atoms. And in the middle of the pyramid, that's where simulation comes in, where we rely heavily on physics engines, like ISAC, for example, to scale lots and lots of data. And this data can be generated based on the real world collected data, or through learning from experience, as Deepak mentioned, right? So that would be simulation data. And just remember, before NVIDIA was an AI company, it was a graphics company. And what are graphics engines great for? Physics, right? Rendering. So that's our simulation strategy. And on the bottom of the pyramid, right, we still need all of those multimodal data from the internet. But this time, we use it a little bit differently, right? We use it to train vision language models that can become the foundation for vision language action models. And the VOMs are trained from lots and lots of internet, text, images, audio, you name it. And then recently, there's also video generation models that become so good that they can be neural simulations of the world. So the last layer of the pyramid is really the neural simulation, right, that goes beyond traditional graphics engines. And these neural simulations, you can prompt a video generation model and ask for things like, you know, hallucinate a new trajectory, a new robot trajectory for me. And the video models learn physics so well, because it's trained on hundreds of millions of videos online. It trains, it learns physics so well, that it's able to give you physically accurate trajectories in pixels. And then you can run algorithms, which we propose in GrootN1, something called latent action, to extract back those actions from the hallucinated, what we call the dreams of the robot, right? Like, do humanoid robots dream of electric sheep, right? It's dreaming, and you collect those latent actions from it, and you put it back into this data pyramid. And with all of these very complicated data strategies, we compress them, we compress them into this one clean artifact from photons to actions, right? A two billion model suffices for a wide range of tasks. So that's an overview of Groot's strategy. I think that paints a really kind of great future picture, right? So we have a simple big model, it's not even that big, that kind of solves everything, pixels to motion, right? But I think along the way, we also need to pay attention to all the things that we have to own delivering products out into the real world that require determinism, right? So when you need to deliver something to a customer, you need to understand what it's going to do in unexpected conditions. You need to think about functional safety, you need to think about how it's going to regress if you add new capabilities on top of existing ones. So I think you pointed out a really important thing, which is the complexity gets pushed into the data, right? And the data you gather. And I think we're at the very beginning of the journey of building that data set. And so I think, you know, maybe I would say a piece of strategy that we think is important is to make sure that you don't throw the whole toolbox out in pursuit of this potentially very powerful end state, because we have a lot of things to do as a community along the way. And one of them is to maintain the trust of the customers that are buying robots. We have to be able to do that by applying all of the tools we have. So I think there's a lot of exciting new capabilities, things that we think will totally change the landscape of robotics. They already are. But at the same time, we need to also be realistic that there's a big toolbox of robotics tools going back 70 years. And some of those are also the right tool to apply to solving real world problems, problems, especially when you're doing things with large, powerful robots that can hurt somebody, or doing things around a person where you want to maintain that trust, because as soon as you break it, you never get it back, right? So I think maybe I just say there's a big toolbox that we need to apply. Yeah, I mean, I second you very much, Jim. And we are very much in that camp where we're making like one simple model. We don't know exactly how it's going to look yet, so I wouldn't call it so simple, but relatively simple model, and it's all about the data, right? And if we want to take the lessons learned from early LLMs and late LLMs in that case, I think one of the things that often gets underestimated is the importance of diversity. So in the beginning of like this history of LLMs, right, there's a lot of companies who tried to train, let's say, a very good model to create poems. So they would train on all of the best poems in the world, and it doesn't really work. Because unless you train on this very diverse data that has nothing to do with writing poems, then you're not going to get intelligence, because intelligence comes from that diversity. And what we're seeing now, at least in our models, is that this is obviously also true for robotics. And even at the very small scale we are now in the beginning with these tiny data sets, we're actually more limited by diversity than we are by the scale of data. So it's about like how do you get as many tasks as possible in as many different environments as possible, preferably with some much noise and dynamic stuff going on as possible, so that you can understand what an actual task is. My favorite example is opening a washing machine. And when we come in like and we see a washing machine, we see like, okay, we're going to put the clothes into the round hole there, so we're going to try to open it, and we try to find a handle, and if it doesn't open, maybe there's a latch somewhere. If not, maybe we turn the dial back to zero. But we have this great understanding of like, how does the washing machine actually work, right? So we can figure out how to use a new one. And machines today don't have that at all. You're kind of like learning how to repeat a motion. And this is why we really think the thing is so important about getting the robots out there in volume, and really getting that diverse data. And I guess here's our very contrarian view, which I think is very interesting to discuss, because that's why we mean like this has to happen among people. It has to happen in a home. And safety has to be an intrinsic thing to the machine, right? How you ensure that the energy in the machine is not so big that it's dangerous? And then thinking about how can we combine this with the classical toolboxes? And yeah. I think one thing I would like to add here is unlike LLMs or vision, when you say robotics, what's the approach? It's always two things. What's the approach for hardware? What's the approach for software? Nobody asked that for language. What's the approach for GPUs? Because we have that covered by Jensen. So it is like, but there are two different things, right? And this is a major question, like, should there be only one robot? Like, should there be one X robot? Should there be one X robot? Like, which robot are we deploying? And then if you're deploying all the robots, then is the brain shared across them? And this is where I think the inside, there are two things here. One is humans. Like, anyone can come up from the audience, and you can give them a VR suit, like a tracking suit, or some gloves, or VR headset. They can control any robot. Any robot, and they don't need to know the motor details. They don't need to know how the motors work. This is already an evidence that a brain can exist that can control any robot. So that's the first aspect. So you can use data from anywhere. Now, the second thing is, there is no data out there. Everybody knows that, right? But we are missing one special robot that is out there, and we have tons of that data. And that those robots are humans. We are not mechanical robots. We are not designed by electricity. We are biological robots. But at the end of the day, similar principles guide us. Like, you have motor neurons. Like, they are called motor neurons. Sensory neurons. Sensory neurons carry signals from your sensors to your brain, and motor neurons from brain to your motors. So if we are agreeing that a brain can exist that controls all hardware, why should we exclude the biological hardware? And if you don't exclude that, you can actually use human video data of human activity. Like, we may not have a 1x robot, let's say, doing something, picking up, opening a fridge. But humans open fridge every other day, like, ten times a day. There are trillions of videos out there of humans doing it. So this is, at least our belief, is that this is one of very critical data for robotics. Like, how human limbs operate, how this. So you can actually use that knowledge to go towards, in addition to simulation, of course, it's not complete without that. Because you cannot just watch and play. So, but these things can combine together. I think this is very, oh, so very quickly, I think this, I think we very much agree. Yeah. I mean, all that data is incredibly useful. And we use it too. Like, these are not usually exclusive. That data is needed. I just want to try to clarify the point that this point was getting mixed in two things. Yeah. No, it's good. And I can tell Pras has some strong thoughts on this too. Well, as someone who's tele-operated a lot of robots, I can say that, sure, the human brain is great at tele-operating a variety of platforms. But I can tell you from experience, not at the same level of performance. The hardware can definitely make a difference. And definitely, I mean, I've tele-operated a 1X robot, right? And it's a great experience, right? I've tele-operated some industrial robots, not a great experience. The hardware can matter a lot in this and does define some of the characteristics of performance. And I think that's important to note, that there will be differences. And there is some amount of building the right hardware to make it be controllable, to have the right sensing, to have the right inertial properties, to make it effective in the real world. I mean, we have Aaron here who has, like, wowed the world with that for the last 10 years, right? That the dynamics of the machine matters. And you can really see it. It moves in a different way. One example is, like, we are missing here, which is not here, like, DaVinci robot. People use that robot for doing surgery. Like, that's already a $100 billion plus company. And all they do is tele-operation through this. Like, this is amazing. Like, so, which means that nobody is disagreeing on the fact that human brain is very powerful. And hardware, so, like, these questions are kind of, that's why robotics is always these two things. Like, approach can be different. In the end, they all have to come together. So, it's not like one hardware or the other. Like, it's like, but real world data, human data simulation, and scaling from all these things. I think it's also a bit about, like, bottom up and tops down, right? Because now we're talking very much top down on the control architecture. But I think it's also very interesting with the bottom up of, like, how do you learn dexterity, for example, right? And at least we're experiencing that learning, like, in-hand quick dexterity in Tallyop, we don't know how to do it. Like, we don't know how to build a teleoperation system to do so. That's fast and good enough and really gives you tactile feedback and all these things. But the robot can actually learn it really well. So, if you just give it a bunch of objects to play with, this is learnable. And then it becomes a question of how do you kind of lift the interface in a, like, add an abstraction layer, basically, on your teleoperation interface. So, you're no longer saying, like, hey, I'm going to pinch grasp like this. You're more like guiding the machine for, like, what tasks to be done and allowing the system to actually learn dexterity. And, yeah. Yeah, I think there's one thing that I think we tend to skip when we try to separate the brain from the hardware. And that's the task you're trying to do. So, if you're thinking about a whole set of tasks where the objects are small, they're inertially irrelevant. Yeah, you can separate a lot of the brain from the body. But I think the reality is that mostly what we want to build these machines for extends beyond the simple tabletop tasks that I think a lot of people start with. If you want to be lifting big, heavy, complex objects, or you want to be touching sharp sheet metal parts, or you want to be working with something hot because you can remove a person from a manufacturing environment and take them away from the hazard and replace that with a robot, then I do think hardware really does matter. And I think it has to co-evolve. I think the idea that we can kind of have a complete disconnection between, you know, a good hardware platform with an API and any software brain, I think sometimes, you know, those two things need to co-evolve. Understanding the kind of quality of your actuator, how much friction it has in it can matter a lot for how well you can represent it in simulation, for example. And I think we're going to need to have more time before I think we fully understand how a model like Groot, for example, deploys on a robot that's of type A and a robot that's of type B. Because I don't think we have enough data points yet to say that one model will deploy across all of these different kinds of robots and there won't be significant differences in the resulting behavior. And if I'm trying to pick a bag of chips and move them and drop them, I don't think it matters. But if I'm trying to pick, you know, a high precision part and assemble it in another high precision hole, it might matter a lot. So I think there's the jury's out for me on whether you can really separate those two things. And it really depends on what I think what application... It could be the other way around too. Like one hardware has lots of brains. Yeah. Like the Nvidia. One hardware has lots of companies with building brains. So it will... I think Aaron actually touched on a very interesting topic and a very difficult challenge of cross embodiment. Yeah. Right. Like what does cross embodiment mean for a model? So let's maybe think a second about ourselves. I think actually humans are great at crossing embodiment. Like any time you open up a video game and you start playing it, you're actually doing a cross embodiment, right? Like if you are, let's say, you know, driving a car in a game or like playing some like weird character, sometimes like non-human character. And then after a while, right, after you play with a joystick a little bit, you'll get a feel of how you control that body inside the virtual game. And after a while, you can play it super well. So actually the human brain is great at crossing embodiment. So it... I think it's a solvable problem. We just need to find that set of parameters to enable this. And I agree with Aaron that for now it's too early. It's an exciting problem. It's still quite early to talk about like full zero shot cross embodiment, meaning that you bring a robot and the model just magically works. I don't think so, right? We're not there yet. But someday we will. And I think like one way to have that is to have lots and lots of different robot hardware and even more different robot hardware in simulation. So previously our research group had a very interesting work, but I would say it's still like a kind of toy, you know, exploratory work called Metamorph. So what we did is in simulation, we procedurally generated a lot of simple robots with like, you know, different kind of joint connectivity. It can look like a snake, look like a spider, really weird, but we generate thousands of them. And then we use a robot grammar to tokenize, to tokenize the body of the robot, right? Essentially converting the embodiment itself to a sequence of integers. And once we see a sequence of integers, right, then we see transformers. Attention is all you need, right? We see transformers, we apply transformers to this whole set of thousands of embodiments, and we find that you can actually able to generalize to the thousand first embodiment. But again, it's a very toy experiment, super early. But I do believe that if we're able to have like a universal description language, and we have lots of different types of real robot and simulation robot, and we can tokenize them, we can generate lots of data from them, then all the embodiments become this kind of university space, vector space of embodiment, and perhaps a new robot will be within distribution. And I also want to add that this is not just an intellectual curiosity. It's becoming a very real problem, right? So I think all of the hardware company founders here have this issue where you have different generations of a robot, and the data you collected on a previous generation and a model you trained on that data, it doesn't generalize or it degrades significantly even to your own company's V2 and V3 robot. Actually, like even forget that, we're seeing the same version of the robot because of manufacturing, because of all the little defects, it's the physical world, it's messy, right? Because of all the messiness, different robots don't even always replicate the same model perfectly, right? You have cross embodiment issue even within one generation of the robot, let alone across generations, let alone across different company and designs. So it's becoming a real problem, and I think we're just scratching the surface. Yeah, there's not a lot of diversity right now, honestly. And if you look in the humanoid space, it's, we're all pretty much working with a fairly similar thing. It's the replication of our body. At Boston Amics, we decided to only use three fingers for our gripper. And you know, it was bucking the trend of having a fully anthropomorphic hand. And we found like, you know, humans are so good at mapping themselves even to three fingers, right? So you can have a tele operator operate a three finger gripper. And within a couple hours of training in the tele rig, they're doing kind of everything you do with five fingers. So I think there's a lot of space to explore here. I think because everybody's trying to get a foundation built right now, we're not being very brave. But I think what's going to happen as soon as you see these generalizations start to show up in our models is you'll see, you'll see people break away from these a little bit. And that might be good or might be bad. I think, you know, we may end up with robots that look just far enough away from humans that it's scary. But I think just inside of the manipulator alone, there's such a rich space of opportunities there. I think agility has got, you know, a completely different gripper than anything that you see on these other humanoids. And they're still able to do some of these same tasks. So I think that's going to be an exciting topic in the coming years. Yeah, Aaron, you give me a thousand different atlas. I'll solve it for you. All right. All right. Deal. Well, I feel like you already answered my next question, which was specifically around hardware. So thank you all. But I want to continue on that because it is a really interesting challenge that you all have such really insight into and from unique perspectives. Would you say what you were just talking about? I mean, even you, Jim, when you were mentioning more so about the same robot that's manufactured, it might perform differently. It just depends. Is that, would you say, the biggest challenge right now when it comes to hardware? I think that's definitely one of the challenges. And that also prompted us to kind of study this line of research on crossing embodiment, on how we can bridge some of these gaps. But I will defer this question to all the hard experts here. This is back to where I think you'll find that this is where the rest of the toolbox matters, right? So if you make a robot that has really good calibration methods, if you make a robot that you understand how to characterize, if you do a lot of the good work on the joint level control, right, the stuff that sits way below the AI, then I think some of these things aren't as big a deal. So I think when you have a robot that you can't characterize, that you haven't calibrated, that has a lot of variability from copy to copy, and you just kind of throw a controller at it, whether it's an AI policy or whether it's something else, I think you find a lot of variability in the output. But I think you can do a lot of work to minimize that gap right now. And I think you probably have some thoughts here as well. Yeah, I think another aspect to this is just, you know, getting robots out there in the real world and doing manufacturing and seeing what variability you have, you do get a lot of learnings that do feed back into the pipeline you built. So a great example of this is, you know, Digit has a recovery behavior that's fully learned, right? And we've been deploying it out in the real world. It's on our production systems. And the domain randomization and diversity of data that we use to train that comes from, it's fed back from what we experienced in the real world and the variation in all of the digits that we've got in our fleet. It turned out that we were doing so much of this domain randomization and hardening the policy so much that when we transferred the policy to our new robot, which we just debuted, which is like 10 kilograms heavier, it's like a much larger frame. The policy actually just one shot transferred to this totally new robot, slightly different kinematics, heavier payload, everything. And it's because we had been spending all of this time like hardening and robustifying like all of the sim to real transfer, really understanding like all the details of things like foot contact and all of these pieces. So I do think that with experience, you get better at this cross embodiment. And it isn't just you're always doomed to need to like look at the manufacturing serial number of the robot super carefully. There's some amount of as you do it and as you get experience with the real world, you understand more about what the levers are that you need to capture in the training pipeline. I think you're confronted by it. When you get from, when you go from hundreds to thousands of robots, it's not a choice. You can't be tuning your software stack per robot when you have thousands or hundreds of thousands of robots. So I think it's just something that has to happen. I kind of agree with both of you, but like I agree a lot here with like calibration matters. It matters a lot. But I think it's very interesting actually, and maybe it's a bit too deep, but like when you do domain randomization, what you're actually teaching your system, right, is to be conservative. You're teaching your system to like, oh, if I don't know what will happen, if I do this, I'm safe anyway. And this kind of masks your dynamics. So it really depends on what you're trying to achieve. Like you won't get the same performance out of the system if you domain randomize, but of course you will get something that's very robust. So if you do really good calibration, you can kind of like get more out of your system. So it will matter in the long term. And then I think there's some incredibly exciting work going on right now with adding the robot history to the context of the model. So you, for every single individual robot, you take some of that robot's runtime and you put it into the history, into the context of the actual model. And then it learns kind of its own dynamics in context, which actually works surprisingly well. And this is really cool. And that's kind of like getting... That's the work we released. It's called RMA, Rapid Motor Rappation. So that's this idea. But I want to give a slightly different flavor to this thing. That it's a big problem that you cannot change your model across versions. And it's very hard to expect that there will be only one robot, one company, all the robots in the world. It's nowhere there. Like in cars, there are so many car companies. In mobile phones, so many mobile phone companies, right? So, but the thing is for them, and even for like, for every other application, there's so many GPUs NVIDIA creates. But then you have CUDA layer, which abstracts you away from it, right? For this, too, for operating system. What is the equivalent for robotics when it's come to solving robotics? So here, I would say the slightly different take here. For every other field, since we are always abstracted away from the hardware, whether it's vision, language, like it does that. Like if a new company has to come in, let's say AMD or any other company, they have to make sure that everybody else can seamlessly run their NVIDIA code or their code that runs on NVIDIA GPUs on their GPUs. It's their burden. It's not the software burden. For the analogy for AI is, the brain for the robot that we are building, we shouldn't be building the brain that just works on the robot, but rather a brain that adapts on the robot. And that's the major difference. Like what humans have is not a system which can do many things. It's a system which can learn to do many things. What we are carrying in our head is a learning engine. Like it can learn on the fly. Like whatever you hear, you are learning on the fly and adapting on the fly. And that's what will be the major difference between how AI has been done for everything else and for robotics. Like for robotics, what we will be deploying really are these mini learning engines. And they are, because many things happen. Like for instance, forget about other humans and other cars, etc. Even basic thing, your own body. If I go to do workout, one after one hour of workout, my hands are sore, and I have to pick up a toothbrush or even a bottle. I have a different body now. Because my body now requires a lot more torque to get the same output than it did before workout. So our brain is adapting on the fly to these changes happening in every microseconds to minutes to long hours. And this is what the main difference I believe should be or will be when these AI models get out of the robot compared to how they have been applied to anywhere else. Like anywhere else, their study has been simple. Train, deploy. Train, deploy. You don't have to worry about adaptation, no changing, because NVIDIA is taking care of you. As the GPUs get better, you are taken care of. Any company comes in, you are taken care of. But in robotics, that will be the difference. You will be deploying learning engines, which is why this is a much very different application of AI than anything that we have seen so far. But I think in general, this distinction between like robotics AI and other digital AI, I also think that will go away, right? So I think we ask the question, what can AI do for robotics way too much these days? And we don't ask the question, what can AI, sorry, what can robotics do for AI? Because the data that you get when you're actually taking actions in the real world, and you have a hypothesis, you take an action, you observe the result, and you are learning. That's how we learn, right? Yes. And we see a lot of things in reasoning models lately, for example, being incredibly good at math, being incredibly good at code, because it's verifiable. You can go and see, like, did I get it right? Well, robots can't allow you to do that for everything. That's how we learn. And in that sense, I think... I can actually agree. Another example is hallucination. Yeah. Like, hallucination is a big problem in LLMs. Have you ever heard robots hallucinating? Like, this is not a discussion topic we discuss. Why? Because robots cannot hallucinate. Because if I have to hallucinate, what will happen if I push this bottle from here to here? I can just try it, it will drop, I can see. I don't have to... I learn by interaction. So since I interact, interaction is the enemy of hallucination. Because as you interact, hallucination goes away. While when you're learning from passive data, where data is coming from Wikipedia, you can't go and verify everything. Unless it's math or coding, where hallucination is less of a problem, because you can actually verify the answer. Yeah. So what happens is we get way more data. Like you said, we flipped the pyramid. Isn't that what Yuki says? We flipped the pyramid and now we get this robot data being way bigger than the internet. And we can solve a lot of the problems we have today. And all we need is a lot more GPUs. I think we absolutely... That's always the answer. Yeah. Sorry, we're all here, right? I think you absolutely can have hallucination, right? It manifests in a different way, which is a deviation in the robot's expected outcome from what happens in the real world. Now, it's verifiable in the same way that code generation hallucinations are verifiable when they don't compile, right? But it manifests in a robot doing a trajectory that's infeasible or generating out... What I mean is it can go away since you can interact. Well, if you do not have the ability to interact, it can never go away. Like, for instance, you can never know... Like, if you cannot... I don't know, like, do you live at this location? If I cannot verify, I can never correct my hallucination. But in robotics, you can mostly correct because of interaction. I have a very good practical example, actually, because we did this. That was actually last year where we had the problem of no one putting down the toilet seat in the office. And we had one of our previous robots, the Eve, some wheels, but it's still very mobile. So we had it autonomously go in and see if the toilet seat is up or down. And we ran GPT-4-0 on this, right? And it was 50% up or down. Like, it had no idea. It was random, right? It couldn't tell if the seat was up or down. It's kind of an edge case because it's usually pretty good at these things. But we had the robot go and close the toilet seat. This is an autonomous policy. So it would go around and, like, check the bathroom and put the toilet seat down if the toilet seat was up. And that was really fun. And we had a lot of fun. And we laughed about it. But it's actually closing the loop in the real world, right? So now the model can get the feedback of, seat is down. I know the seat is down. I closed it. I know it's down. And you told me it was up. You were wrong. And it's similar to closing the loop in other places where we're using AI to interact with, for example, APIs or compilers or things like that, where you have it emit some result and then you put it through a verification phase that you can feed back into the context of the system. It's just, in this case, the loop closure is a little bit slower because it's going through the physical world. Yeah. The problem right now is that we don't know how to do this in a general case, right? We can architect one specific thing, like the toilet seat. And now the question is, how do you come up with some formulation of this problem where you're grounding everything in the real world? And no one knows how to do that yet. The rate at learning in the real world is going to be painfully slow, right? So we can learn these things in the real world because there are consequences to dropping something. Gravity makes it fall. You can tell something bad happened, right? But the rate at which we can explore with a physical robot, I mean, that's back to the blend of data, right? I mean, you can do these really exciting small things, but how many thousands or millions of those things do you need to do before you have enough data? And so I think that the question is really still, can we afford to produce the real world data? But you have simulation too. So simulation is also interactive. So I think there is interaction data you can have both. Yeah, I agree. And simulation also needs more GPUs. All right. I know we're getting close to time and I really wanted to end with this question. So I'm very curious about this. In the next two to five years, where do you see this heading? And I'm going to leave that as a vague question. Answer it how you will. I'd love for you to start, Bernd. Okay. So two to five years, that's a pretty big range, given the current velocity of the field. Okay. So let me like say, I'm going to cheat and I'm going to start by saying, I think it's going to take 10 years before this fully pans out. And it's very easy to say, where will this be in 10 years? I think then we're going to have the same kind of change in society that we had a few hundred years ago with electricity, where we now just take it for granted when you flip the light switch in the morning. That's going to happen to labor across digital and physical. And man, is it an interesting time to be alive. And I think we can really get to focus on what makes us human in that society we're creating. Five years. I hope we're there. I think that's ambitious. We're going to push for it. I don't think anyone knows at this point. I think it really depends on how fast society adopts robots and how fast we can scale manufacturing. We're at kind of like the cusp where it's useful, right? So I would say the product we have, just as an example, is currently useful in a home. It's not perfect. It's not like you don't need to do anything yourself, but it's useful and it's fun. And then you can kind of start accelerating from there. And hopefully it's not an autonomous car and it doesn't take a decade longer than we think. But I do think like three to five years, it is pretty much out there amongst most people. And even if not everyone has a robot, people know someone who has a robot and they're generally part of society. Across everything from consumers and homes into factories, logistics, everything else. Yeah, I can go next. So there is a saying that people often overestimate the progress in the short term, but they often underestimate the progress in the long run. And I think this is probably by Bill Gates or someone. And so I cannot, I mean, this is a disclaimer, but I think that one thing unique about robotics AI that makes it different from LLMs or VLMs is that LLM has to really solve the problem almost to the completion to be really useful. Like, whether it's coding, whether it's general writing or anything, it has to be really, really good. Like, NLP had good systems earlier, but until you reach like really high in performance, these are not useful. But that's not quite true for robotics AI, because we don't have to solve robotics fully for robotics to be useful. And this is just to say, like, even today, there are like hundreds of thousands of millions of robots deployed already out there. Many of our things are made by robots today. Right? So they are already out there. They're already there. So what is the key part here? The key thing in robotics is that there is distinction of tasks. The robot that solves all tasks everywhere, that may be farther, so I'll not make any prediction for that. But we will start seeing robots that maybe solve few tasks or one task or two tasks or task specialist. And even they are super useful, because there are several tasks for which it's very difficult to either find labor or hire. I was talking to some company today. They are unretiring people from retirement because this is a shortage of labor for their specific users. Specialized robots will come much quicker. And generalist one will be farther. But the useful lesson starts from day one in robotics, unlike language. That's so true. And if autonomous cars weren't dangerous, the problem was solved in 2015. You could get into a car that drove you around in 2015 and it was doing pretty well. Solved in one way, as in it's not the human way solved. Yeah, I think that's part of the challenge is that adoption is not just a technological problem. It's also things like safety, things like societal adoption also play a factor. And so in three to five years, what we might see is that there's a lot more robots in certain areas and a lot fewer robots in certain areas than we expected. And I think the important thing, though, is we are really seeing the culmination of robots really going from being historically very single purpose to this notion that it's almost expected that they could be multi-purpose. Maybe not general purpose, but multi-purpose. That's becoming kind of people's expectation and what we're able to show with these new AI-based platforms is that, hey, a piece of hardware can do more than one thing effectively. And I think that's the piece that will hold over the next three to five years is this expectation as the new waterline that people are building towards. And all of you are now seeing this and buying into it, which is great, right? Because now you're going to be sort of carrying that expectation socioculturally forward and saying like, hey, why can't I have a robot that does three or four things in a home or in my case, five or ten things in a warehouse or logistics facility? Like that should be how it is. And I think that's what really drives it is people wanting those things really drives investment and focus into getting us those things. Yeah. When people ask this question, they're looking really concrete, specific, like I'm going to have a robot at this date, right? And it's going to do all these things. And I think the real problem with this is there's no level setting on what expectations are for everybody, right? So the question I usually ask is, you know, when will we have a humanoid robot that's as valuable to us as our car? I have no idea, right? Our car works every day in the most extreme weathers. It costs hardly anything given the amount of material and effort that goes into putting into it. And even the car by itself isn't quite touching what a humanoid robot might add to the value of our lives. So I think, you know, I'm also in the like 10 years or longer. And I think that's the typical like technologist answer. If you ask a founder, they're going to say next year. If you ask a technologist, they're going to say, it's about 10 years, right? And 10 years just means it's really hard for us to quantify specifically what you're going to have. I think the thing we should be focusing on is the rate of progress and where the beachheads are, right? Each one of the groups up here is establishing a meaningful beachhead in a different area. And over time, those things are going to grow. And that space is going to go from a bunch of dots, right? Where, you know, agility solving problems in a warehouse, we have robots in people's homes. You know, we're going to be working on automotive factories. And I think you're going to see from each one of these beachheads, you're going to see a growth, right? And it's not going to be an over the night thing. I don't think anybody here can predict five years into the future and say exactly where we're going to be. But I think we're going to see this growth. And pretty soon, all those things are going to start overlapping. And one day, we're going to have an autonomous car. And when you look back at that history of that market, you know, there was a lot of shade thrown out on how badly they predicted, you know, when we would have an autonomous car. And I think a lot of that came from statements that some elements in the community were making on how quickly it would come. But I'm pretty grateful that my car does autonomous lane assist and, you know, it doesn't hit the car in front of me and prevents me from backing into something. All that magical stuff came out of the dream of having an autonomous car. And oh, by the way, you can get an autonomous taxi right now. So yeah, it took a little longer. And so will humanoid robots. And I think as long as the community is excited, leans in and realizes this is a long game, right, to get to specialist robots that are delivering value in a commercial setting, I think we're going to have that in the next one or two years. Agility is already delivering robots into this space. When we get those robots doing 10, 15, 20 tasks, that's going to be in that next five year terrain. But when we're going to solve all of the problems we imagine across all of the industries, I think we need to keep dreaming and we keep working. And this industry is going to have to keep putting energy in for many more decades, I think, until we've solved all those edge cases. I really like Deepak saying that, you know, people tend to overestimate the short term and underestimate the long term. So let me break it down into short term and long term. I think the next two to five years, from a technical perspective, we will be able to study fully the embodied scaling law. So I think the biggest moments in large language models is the original Chinchilla scaling law, basically that exponential curve where you're putting more compute, you scale the amount of data, you scale the number of parameters, and you will just see intelligence just going up, right, exponentially. I don't think we have anything like that for robotics yet, because the scaling law is so complicated for robotics, right. You can scale across model, you can scale across the hardware fleet, right, the real robot data. And how about the simulation data scaling law? How about the internet data scaling law? How about the neural simulation, the neural dreams, right, scaling law as you are generating lots and lots of videos? So we will be able to study all of these things so that perhaps, you know, five years from now or sooner from then, we'll have that plot on the screen that you know exactly how many GPUs you buy and how much better your robot will be. So we answer that question quantitatively very soon in the short run. Now, let's talk about what's going to happen in 20 years. You know, every time I stay up late at the lab, the robots break doing something weird. I'm like, so frustrated. Let me think about what's going to happen in 20 years, and then I can carry on, right. So 20 years from now, there are a couple of things that I'm super excited about that I think is just not that much far away. One is robotics accelerating science, right. So I have some friends in biomed, and it's just so time consuming to do one experiment and so laborious. Like all those PhD students need to be in the lab, right, attending to those like mouse, right, all those like, you know, dishes of cells. How about we automate all of that, right, automating science, that maybe all the medical research will not cost a billion dollars to do. They will be scaled up because we get this API to accelerate the physical world, right, using intelligence. Perhaps that will be Groot version 10 or something, I hope, right. So that is one thing I'm super excited about. And the other thing is robotics automating robotics itself, right. So why don't we have robots fixing each other, right. So we see all of those big factories making the robots, but how about the robots themselves assembling the next generation of the robots. And I don't think this science fiction at all, because actually in the LOM community, again, they are ahead of us, unfortunately. But in the LOM community, people are studying auto ML, meaning that can we prompt these LOMs to do deep research, right, to find the next best transformer, to find the next best architecture for intelligence itself. And people are actively doing this as we speak. And probably LOM is going to solve that first. And then we're going to copy homework, and we'll have the physical world doing this recursive self-improvement as we go. And I think that's going to happen, right, not in 100 years, only in 20 years. That's definitely going to happen. So I'm going to end on a bright note. I think we, as a generation, all of us, we're born too late to explore the Earth. We're born too early to travel to other galaxies. We're born just in time to solve robotics. And everything that moves will be autonomous. I love that. I mean, I think that's the best note to end on. Thank you all so much to our panelists for coming, sharing your thoughts, not only on where we are now, but where we are headed. Just a note before everyone heads out, we aren't doing a traditional Q&A, but we are going to get, remove our mics, come back here. And for anyone who's interested, feel free to come up to the stage, and you can ask your questions directly to the panelists, for anyone interested. So we're just going to go off stage, get our mics removed. We'll be back. For any questions, come up to the stage.