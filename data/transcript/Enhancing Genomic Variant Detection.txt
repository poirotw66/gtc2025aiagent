 hello everyone i hope you're having a great time at the gtc i am i'm garot rapathi co-founder and uh ctu at innoplexus it's an honor to be back here at gtc to talk about how gpus are changing the world of course gpu acceleration especially with nvidia power brakes how it's transforming genomics analysis and precision medicine we're going to explore how artificial intelligence machine learning and high performance gpus are enabling rapid variant detection reducing the computational cost and unlocking the insights and biomarker discovery and drug development now as you all may have realized we are really in the middle of two big revolutions one is on the artificial intelligence and second it's on the whole synthetic biology biology and genomics since last year we have seen some major breakthroughs happening in generative ai and now what we are seeing is how it's transforming healthcare we are already seeing ai models generating protein structures drug candidates and even predicting disease risks with unprecedented accuracy so all of it that we are doing it's really leading to in the end these this is impacting real lives on the ground uh real patients where we are either uh helping accelerating uh the cure or we are helping find better treatments or we are helping uh really tailor uh specific uh treatments for specific sets of patients and yes in the end it is is really about uh saving uh people's lives whatever that you will see here today uh the the impact of it has been where we have saved uh a few lives at least but in all of it uh the catch is when we talk about ai and uh machine learning and all of it all of these models they they still need uh massive data sets and uh enhance uh lightning fast computations if we look at just genomic sequencing uh it's it's producing data at a scale that is catching up uh with uh probably only next just next just next to astronomy and uh and social media in terms of the scale so the uh the real question is is is is not how fast uh or you know how how much data can we really have it's how fast can we turn it into actionable insights and that is where uh we have leveraged uh nvidia's uh gpu and the gpu acceleration now if we look at the whole uh genetic revolution we are looking at uh uh so drug discovery as a process the whole process uh at at each of the uh at each of the steps we are now looking at uh that how these steps are further broken down into different problems and in all of these problems um uh computation is is playing big role ai is playing big role and if we look at just uh genomics it's it's really uh transforming the drug discovery and the journey from simply understanding a disease to delivering a personalized treatment all of it in the end depends on just one key factor data uh it's all about data if you look at the human genome um the extent of data just to give you the idea of what we are talking about human genome consists of about uh 3 billion base pairs which translates to about 180 gigabytes of data that is a whole genome sequence but uh we don't necessarily uh work with the whole genome genome sequence uh because it contains a lot of repetitive information most of which it's still difficult to interpret and hence what uh practically we end up working with is just a subset of it it's called the whole exome uh and and this is the the part where uh uh where it just focuses on we are picking up the parts which are just focused on protein uh coding uh regions and uh that's what is uh the functional part which is called the exome and that's a the term about the whole exome sequencing this is just about one to two percent of the whole genome sequence and uh hence we are able to narrow down from 180 gigabytes to about roughly about 13 gigabytes per individual but overall if we are looking at uh genomic data uh it's it is definitely exploding as the cost for sequencing are coming down we are going to see uh the volume of potato that is being generated it is going to continuously rise and by by some matrix we are we are looking at this probably a rush you know in hybridasi 지� or so whereas in ever that's gettingOps in reality processing or this could be a general's intelligence turbo gathering so to one of podiput used to achieve basic logo coverage and so one of the pop-ups is having силь structured thing in media so one of those things can be really important but if we see a new network it's going to again as i mentioned maybe at the scale of astronomy and social media in total volume but if you look at the market size uh by 2030 the market size of genomics market itself is roughly going to be 95 billion dollars genomic data analysis and uh just the interpretation market size again it's also growing uh rapidly and uh both of these are are growing uh really rapidly and that's where we see uh the demand for uh for such tools and demand for more and more acceleration it's it's going to grow again and that's when we see uh where the need uh for speed is you know if we if we look at accelerating biomarker discovery so here the problem is that we are looking at uh discovering biomarker from large scale genomic data and here let's see the challenge that that we looked at was for a neurodegenerative disease so there was a trial around a neurodegenerative disease generated five terabytes of sequencing data that was from about 500 patients if we look at the traditional um processing methods we we typically would be looking at months of uh really it would need uh getting all of this data analyzing all of it it will it will need months and when we really think from perspective of life saving research months is is really a lot for a lot of the patients that's really life and death and here uh the challenge was how do we how do we get it uh to so this large scale biomarker discovery how we are able to analyze it really fast and that's where we uh came up with the solution that yes let's leverage the nvidia gpus and the parabrix uh framework that we are able to accelerate it and uh the final impact uh really was that all of it the entire uh project we were able to just wrap it up within within two months from our partners and they were they were actually looking at uh almost uh 10 to 12 months earlier and from there if we if we have to take this into the context uh this time really means uh means it's a question of life and death for for certain patients who and and there we see really that if you are able to extend the the lives of patients even by a few few days or a few months that's a huge impact and that is what the ultimately the impact of what we did the case study here that i'm talking about uh that's where we had uh taken the samples from a randomized control trial uh which was which was about a drug that was for a neurodegenerative disorder so uh typical clinical trial where you have the you know one one side you have the treatment and another um with the placebo um and uh this is the uh there's the order uh of things so we started with the uh sequencing then the quality uh qc workflow and with nvidia gpus uh um powering the variant calling workflow and ultimately uh leveraging ai and ml for uh biomarker discoveries the whole line and if you look at the study design like how the study work designed so this is a phase two clinical trial with about 500 patients with early cognitive impairment the primary objective here was to identify the snips or as it is called the single nucleotide polymorphism which can predict drug response we had the biological samples the blood samples collected from all of these 500 parts pins and in terms of the genomic analysis ws was performed on all the samples with about 100 x depth of sequencing that's what resulted in the five terabytes of the raw sequencing data now this itself this data set of course it was a it was a powerful resource from where we aim to discover the genetic biomarkers should be associated with the uh with the drug response and uh that's what so the that's what we had uh limited gpu now before jumping on the gpus i i'll tell you about how how how is the typical workflow the whole ws pipeline workflow how how does it look like what are the steps so it's uh if you look at the journey of sequencing from from right from biological sample to ending towards where we are ultimately we have the necessary insights generated so we start with uh dna extraction and uh library preparation once we have the biological sample we are extracting dna from cells um which is more like you know extracting the blueprint from uh from a complex machine that's yeah dna is really is our our blueprint of organisms when we look at uh uh library presentation then that's what it means is where we are um uh fragmenting the dna and then tagging with unique identifiers where we are creating a specific library that can then be sent ahead for uh sequencing in sequencing of course we have uh very uh sophisticated expensive you know machines where which read the whole dna sequences and uh it's able to generate literally it's like what's what's in there uh representing them in a in a series of sequence of uh letters which is again uh in itself a massive data that is uh generated uh this raw data that's where it gets translated into this the digital sequence of 80 c's and g's which is just just the building block of dna once that step is uh done then we are looking at uh alignment so read alignment where uh going from the fast uh q files to bam files these sequenced reads are then aligned to a reference genome like piecing together uh how you would try to put together pieces in a jigsaw puzzle and this where we are creating the bam files which contain the aligned reads and information about the quality moving on we go to the what's called the variant calling um moving this is where we move from the bam files to the um the step where we generate the uh vsa file so here we look at uh identifying the differences between the sequenced dna and the reference genome right and these differences where where it is uh where it is differing uh between uh the sequenced uh genome and the reference d genome that's wherever these this difference the variation those are called uh the variants and these variants are stored in what we call the vcf files the variant uh call format uh files so that's where the files are generating and once these files are generated uh the files are analyzed to identify meaningful patterns associations and potential biomarkers you know this is this pretty much um like you know how we have so we have we have we are really converting in a whole complex biological problem into something as it sounds simpler it's simpler like a pattern matching problem where we are just looking for some patterns right which are occurring or what kind of patterns that we can uncover from it and finally uh this all uh analysis leading to where we are generating such insights wherein we are able to understand that there are certain patterns there are certain uh associations and hence resulting in certain types of potential biomarkers which will ultimately lead to that we are able to get to a faster diagnostics or personalized medicine now in all of it doing in all of it the real computational challenge is the step from read alignment to analysis these are the steps which are computationally a lot intensive um requiring significant processing power and memory and this is where gpus come in and again this is uh this is from our end i know you know very much i'm not saying that dna sequencing in itself is uh uh is not computationally intensive definitely it is and that's where again gpus are being used but from our context from our perspective uh we have uh leveraged uh on on on this side of so from real alignment to analysis and that is where we have been able to accelerate all of these uh steps resulting in a much uh faster genomic analysis now next uh slides we are uh showing uh very important uh in aspect of it that yes uh we are uh we have uh generated uh uh the uh the files and eventually got to the inside and so on but then uh the the key uh here is what about quality and that is where uh the equality control uh itself so uh before we uh dive deeper into analysis let's let's uh talk about quality control it is uh if we if we so here on this slide we have uh we have uh looked at or the traditional uh quality control pipeline and what we did at enoplexis where we optimized the uh the quality control so here we uh if you look at the traditional quality control approach um it's a manual inspection where scientists traditionally use tools like fast qc to manually check the quality of uh sequencing data then instance uh selection where it's about choosing the right cloud computing instance for analysis now this this again uh it may sound simple but uh figuring out that given a particular kind of a load what kind of an instance should you choose and how you should choose that also takes a lot of time now eventually uh if you look at it since these are all manual steps eventually all of this time all of these hours add up into days and weeks and that's where we lose time and uh definitely the overall turnaround time is high and there definitely is a potential for human error and it happens a lot given at different stages there are different team members involved in there are there's high probability for human error and that's where when we started looking at quality control is that how can we make it more uh how can we make it smarter and more efficient so that's where uh our approach uh was all about aware uh in automated quality control or ai powered pipeline it automates the quality checks at at each level intelligently filtering and processing the data and uh in terms of the cloud instances as well the pipeline automatically is able to select and configure the most cost effective cloud instances on on platforms like aws or gcp right any of these so it is able to automatically uh suggest and recommend like what what kind of instances would we needed for running this kind of a workload and then um the system is able to dynamically adjust the processing resources based on uh the data needs uh to maximize the efficiency and overall if we look at uh this the approach of the conventional versus uh our approach and automating all of it here this uh this automation itself we were able to reduce the sequencing uh qc time by as much as almost 98 percent so we were able to do uh we are able to reduce it by one one fiftieth you know of that time uh so in this slide as as you see yeah so how we have uh so we have you know compared like what what are the different uh different kind of server instances and the comparison between fast qc and the innoplex is fast qc approach here you see uh so the last bar which is just highlighted with uh with red now that is what comes as an auto suggest uh from our pipeline itself so it's already based on all of the uh all of the data and the workload it's able to already see that which uh option should it really go for what kind of an instance should it be running on and we can see the see the difference and next again uh if we look at uh from a cost perspective as well this uh again for uh if we are using inoplex's pipeline and the comparison between uh the cost for for the approach again here you see the last uh the last bar which is highlighted in in red this again you know comes up if if if it is uh if we choose with if we choose to go with the recommended option again you see the difference in the in the cost which is uh quite uh quite a bit you know especially if this matters when uh you know such kind of workloads are uh are run let's say in in um in hospital research institutions or you know elsewhere there definitely is the grants or in all of these these are these are again you have uh you you have limits on how much money and how much resources can actually be spent and that's when we see that with this approach we can bring down um bring down the costs as well so finally we see that uh if we use gpus and ai speed and cost efficiency it's not it's not usually a trade-off because typically what we have seen uh one of the things that people say that uh yeah gpus definitely can accelerate but it comes with a hefty price tag and here we show really that that it's not while using gpus and ai you are actually able to do it at lower cost than uh than increasing the cost of the project and that's where we see that how they can work seamlessly and they can accelerate the entire genomic workflow without compromising the quality now we look at uh the comparison uh between if we if we are doing it with uh a conventional uh we are running it on conventional cpus versus what happens when we are running it on uh gpus so when looking at a a traditional uh cpu uh based pipeline right this is like uh yes how cpus work definitely so you imagine a a bustling city with only a few narrow roads uh the traffic will definitely crawl and everyone will be frustrated that's how typically you know on a cpu uh the processing will take place when you are we are trying to run such a massive workload we look at uh so bwa mem this is the step which uh aligns the dna reads to a reference genome which is like organizing the pieces of the puzzle then we have the uh gatk uh the toolkit which uh finds variations in the dna uh sequence like which is spotting the unique pieces uh in the puzzle uh but uh doing it on uh cpus definitely have limitations so it is uh all of it the processing is really happening in uh in a sequence you know one after the other yeah and uh if if i have to give an analogy it's more like uh you know you're driving uh on a uh on a highway which is uh again it's it's a single lane road right so you will you you will have to say that yes you are just behind the uh you know the car ahead of you you can't uh you can't cross so that's that's how uh working with cpus is and uh again in terms of resources uh if you if you just look at cpus um in terms of the in terms of the uh number of cpus that it will it will need ultimately to even uh reach that kind of compute that's a lot and with that the the requirement of power and cooling everything uh multiplies and of course uh it takes a lot of lot of time you know there's such kind of an analysis can uh can even take weeks which uh which hampers you know the progress on the research and getting you the clinical insights in time and that's where uh that's where we see that if we uh instead use gpu acceleration um and that's where if we bring parabricks like what happens all of it so again going back to the road in the car analogy so yeah you're not driving a single lane uh you have a super highway with multiple lanes allowing the traffic you know to move quickly and freely and that's the power of gpus and nvidia parabricks massive parallel processing the gpus can handle a number of tasks simultaneously and that's where we are able to accelerate the processing of analytics specifically we already have seen it as the algorithms there that are used are already optimized there are specialized that are already fine-tuned for gpus which further boosts the performance and ultimately what we are looking at is uh you know up to you can 25x faster uh variant calling so whatever we are trying to do so instead of days you are getting all of those results in hours and uh in terms of cost again because uh you are since you are able to run all of it it is so so fast and so efficiently it is being done at almost one twentieth the cost so 95 percent of low cost making and what this makes is this makes the genomic analysis more much more accessible and affordable and of course uh on the accuracy side as well uh this has better accuracy much higher accuracy than the traditional methods and hence the reliability of the results is a lot more again looking at uh sensitivity so it's with this approach we are able to identify even more variants uh and and hence is we are able to generate even or uncover uh more insights than what typically would not have been possible in terms of numbers what it means really when we say faster and cost effective so what paravix really enabled so in terms of uh in terms of speed you see that uh it's a it was almost 25x they speed up at each step as compared to uh uh to cpus and what you see is really it was it was done by using even less than a week of uh you know the uh we use the gpus for less than a week for for doing all of it and in terms of the cost again what was the projected cost that if you have done it the traditional manner it would have it would have costed us almost 35 000 and we brought it down to almost to one point just just 1.7 000 which is almost 95 percent reduction and what it means is uh specifically in a in a research setting and when you are at a hospital research the money that is saved here is available for better uh purposes for doing more research and that's where uh we see that you are getting a different kind of acceleration as well the resources that were blocked just for one project are now available for running more and more and and again in terms of accuracy as well as compared to the traditional methods uh here the accuracy is also also much higher which is uh to ensure reliable more reliable results so uh when we talk about uh the accuracy side so that's where we see that in terms of uh sensitivity so here for finding out these variants so it's not that uh we were able to uh we were able to identify more variants uh these possible variants again where we are even better than the uh than otherwise what we have we would have got and here the kind of insights that we were able to draw from uncovering new variants that we would have missed if we would have chosen the traditional approach approach now these additional variants simply could hold uh the key to understanding uh disease mechanisms in a much better way or even coming up with uh much better treatments so that's where we see that as we as we get uh more data uh we are able to analyze uh we are able to analyze larger data sets we are able to uh generate more comprehensive insights and since we are able to uh accelerate this analysis it means that we are able to lead uh to much far even faster discoveries and a much quicker turnaround times for clinical applications and in the end all of it happening for a much lower cost than what has been happening if we uh look at the overall impact we are not just talking about uh just the speed and the cost but it's it's leading to um leading to even smarter uh genomics as well right and um this is uh what we what you're seeing on the screen this is a result from an analysis that we conducted uh from the uh again from the uh clinical trial of the degenerative neurodegenerative disease and this is uh what we uh with the studies with genome-wide associated studies that uh that was done here uh the goal was to identify uh the mutation biomarkers and we were able to uh identify a mutation in the in the serpent a3 gene that correlated with a better drug response in a in a specific subset of patient so even being able to get to that specific subset of patient that in itself was a was improvement and uh in this uh in this gene we could see that mutations in this gene had been previously uh reported to be associated with risk for neurodegenerative diseases so whatever that came uh it came out uh it came out very much in line with uh what was expected and uh in the end it uh what it leads to that we are able to find we are able to uh get uh um a specific uh subtypes of uh getting a specific selection of patients who would uh respond much better to any given given treatment identifying again these uh specific uh genetic subtypes so this is another uh analysis where we uh we simply focused on the genetic subtype and patient stratification so patient stratification meaning that we were able to use the data to analyze and get to a specific cohort of patients who we could see that here there would be a higher probability of responding to a specific uh treatment based on you know all of the markers and here we used advanced algorithms for grouping patients based on the genetic profiles and we were able to identify patients that would uh that would be suitable for a for a more targeted therapy what you can see here is uh patients in the genotype cluster one had the best response to the uh to the treatment compared to the others uh this could potentially be due to uh different disease characteristics or differences in drug metabolism defined by their genotype and and this was what we see uh where precision medicine is that instead of uh instead of simply coming up with one treatment and looking at a scenario that that that same treatment will work for all patients um across all types of genotypes and so on and instead of instead of that paradigm coming to where uh we will be able to figure out uh any given treatment what's the uh what set of patients will it have the highest likelihood of giving better results and that is now uh possible that we we are able to tailor the treatments to specific uh genetic makeup and hence it maximizes the effectiveness and it also minimizes the side effects because uh it is already it has already uh gone through we already know that what effect what this kind of treatment will have on you specifically on on on on a specific patient with a specific genome uh genomic uh profile and that's where we see that uh gpus uh and and we are able to get used gpus to power the ai and machine learning algorithms and we are able to uncover so much so many insights and able to do all of it fast and in the end this is what we see that it's going to lead to a future where um where we are able to um bring a lot of precision in diagnosis we are able to develop much better and and more targeted therapies and in the end we are able to improve the patient outcomes by uh by an order of magnitude and that's where that's what is it is uh what the personalized medicine is and we definitely see that uh we are moving more um faster towards a future where medicine is truly personalized where based on your genome uh your treatment plan uh will be subscribed or prescribed to you and in all of this again yes definitely gpus are uh definitely accelerating this revolution helping us to unlock the full potential of the genomic data for transforming the healthcare the the key insights what we did and what we learned was in the end we are able to accelerate uh genomics by doing this rapid analysis of large whole exome sequencing data sets to identify uh precise uh biomarkers uh biomarkers response biomarkers we are able to bring in a lot of efficiency and hence we are able to enable faster discoveries while bringing down the cost on the on the computation side and in the end uh improving the patient outcomes by accelerating the precision medicine advancement uh for for much better healthcare solutions right so all of it what we are doing at uh inoplexis uh this is something which keeps us going every day because we are able to we are able to see the impact of what we are doing um we really are able to see that everything that we are doing in here um using ai and gpus it's really uh it's really impacting patients on the ground we are able to improve the lives of patients and in the end we are able to save a lot of lives potentially and this is where we see it's the it's a true impact of what we are doing at uh inoplexis uh here are my coordinates uh please uh feel free to get in touch and uh we are looking forward to have any questions please feel free to type in in the chat and yeah thank you you