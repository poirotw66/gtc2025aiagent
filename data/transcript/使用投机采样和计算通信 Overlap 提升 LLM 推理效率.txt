大家好,我叫肖斌,来自百川智能很高兴能参加这次GDC2025的分享本次分享的题目是使用投机采样和计算通讯Overlife提升LM的推理效率首先我们将从投机采样开始讲起投机采样主要是利用Decode阶段的过程算力冗余使用额外的技术生成多个候选Token同时输出给大模型并行验证充分利用算力且不会增加额外太多的延迟如左图所示,这个阶段主要处于memory bound即使增加到一定的barasize它的耗时势并不会显示增加只有到了一定拐点之后才会出现现行增长我们主要利用在这个阶段的特性同时生成多个候选Token让给大模型进行并行验证传统的AutoRegressive的结构是输入一个Token然后扔给大模型输出一个Token然后再扔给大模型输出一个Token这样的Regressive的执行投机采样的主要原理其实是输入一个Token之后会过一个Draft ModelDraft Model并行生成N加2,N加3,N加4的Token然后同时扔给大模型进行验证验证的过程中会按照这个数型按照数型的依赖进行验证比如这个输入这个是Token Next加1它会输出N加2N加2需要验证下一个Token输入是否是一致的如果是一致的话说明这条路这一个N加2的输入是正常的直到验证到某一个Token和它下一个Token的输入不对齐这一路就无在采大而使用最后一个Token进行采大就是输入一个Token然后并行验证了其他4个Token并且最后一次性通过了4个Token但在零端的时候其实我们是要考虑大Batch下的场景的Token数量的限制独立采样的主要难点是为候选Token的产生方式称为Draft Model需要命中率高代价低 易为户在我们刚开始选行的时候大概主流的方案有以下几种第一是独立的小模型还有额外的大模型额外的子结构还有外部数据增强独立的小模型的话它需要训练一个跟大模型对齐的模型命中率是比较高的但是缺点系统会比较复杂需要大小模型协同两个系统之间会要不断的频繁交互大模型额外子结构呢其实是在大模型的基础上增加一个特殊的结构命中率比较高系统相对而言会简洁很多但是同时需要强有的模型结构设计能力外部数据增强的就是传统NRP如Ngram等存储方式或者外部数据库运行代价其实比较低但实际上为此要构建一个命中率也相对低些构建整个过程也相对复杂云端的时候我们就是优先考虑大模型的外子结构当于一系统的时候可能会考虑独立强模型云端屯统是大败的场景需要考虑命中率与候选择困数的折衷但是许多论文中大部分考虑是BiteSight的一亿的形状意然后模型之初呢当时只有美都沙可以重新参考我们就在此结构上进行改进性升级可以看到美都沙它是在这个Norm之后呢然后并行拉出N条数据流然后并行验证Next加2加3加4实际上从直观上的感受来说大模型到最后这个Norm之后仅仅包含N加1相关的信息很难包含N加2加2加3加4的通文信息在这个基础之上呢我们尝试了一个方法能不能把这个这个引出的这个Hitden State的信息往前移能从中间的一些层上获取到更多的全局信息但是我们发现其实效果并不好然后我们经过一些调研然后发现其实有一些大小模型系统时候会用到CrossTension来学习大模型跟小型的信息交互同时也参考了美读莎V2的一些方法它会整个重训练整个模型的权重然后可以想到我们能不能增加一个额外的整层Transmolayer来学习这个N加2N加3N加4的信息呢我们发现增加这一层的之后呢美读莎的命中率提高了很多在此之外呢然后我们我们尝试说美读莎它其实是一个并行验证的过程并行验证的过程中呢其实它们之间每个Head之间是没有任何关联信息的这时候呢其实从直观上感觉就是如果前一个Token的信息没有辅助下一个Token的信息这样子是非常浪费的而且后一个Token的预测就变得更加不准确命中率会更低在此基础上我们想进入这种序列的依赖我们最常见的方法序列依赖的话就是把这个HeadnState的信息跟这个上一预测Token的Embedding已经concateconcate起来之后呢然后过一个MLP然后预测下一个Token其实我们可以从这看出来这个什么类似一个RNN结构了输入一个输入是HeadnState然后同时输入一个新的Embedding然后更新这个HeadnState然后预测下一个Token但是我们发现直接concate其实效果并不好在手机上我们尝试设计了一套类似的Attention的结构HeadnState呢按照HeadnM维度进行NomeNome之后作为Query然后Query之后呢新的Input的Embedding作为这个KNValue然后不断的HeadnState不断的Query这个KNValue的信息融合至这个HeadnState里面辅助下一次的预测由此我们就得到了我们第一版Clover的这个结构Clover结构呢大概有两个地方第一个地方是TransformerLayer它是负责收集更全局的信息第二个是这个LegistRN的一个Attention结构RN的Attention结构它负责不断的吸收新Token的信息到全局信息辅助下一Token的预测有了模型之后呢我们其实就要上线了上线的时候其实前面提到两次我们是能够是大Batch场景下的投机有效它Batch场景下投机有效其实相当于是我们不能到那个性能拐点必须在不让它不能让整个模型处于memory bound这时候呢线上的Batch如果比较大的时候这个后选拖空数要求就会比较严苛比如说我们拐点是64Batch再等于64其实要求我们线上的Batch再等于16的时候只能后选投空数为4然后经过我们先衡量的情况下我们大概要选择了这个后选投空数为4的情况下此时呢其实三拨策略是影响比较大的在当时的场景下大部分的这个三拨策略是有比较固定的范式因为大部分论文的时候呢它都是Batch再等于1它是不是尝试了很多这种全排列的方式比如说Head0的Top1和Head1的Top1Head2的Top1这样子进行组合但是呢我们之后选拖空总共只能有4个不可能通过这种每举这种方式因为每举方式没有几个然后我们采用了一种动态构建Token数的方法是一种较为贪心的思索策略这次类的核心来说的话就是首先你大模型输出最后的Token会过DraftModel它会生成Next加2的TokenNext加2的Token我们还会按照这个Top0.8的进行阶段如图所示发我们就会把0.5TokenA和TokenB分别概率是0.5 0.3的留下剩下的丢掉在这个这样子的话就能保证整个TokenTrade的深度往下继续发散深度然后在这基础上我们会过第二再过一遍DraftModelDraftModel会分别预测A.1 A.2和B.A B.R这时候呢我们要考虑联合概率就是当前的概率同时乘以负节点的概率然后把整个4个Token进行排序然后再选Top2这个Top2来点是说因为我们要总共要保证整个后面Token数是4所以说负节点已经选了两个Token情况下我们会在选此节点的时候会减去负节点已经选过的Token数所以说导致第二层的话只能再选两个Token然后这样子的策略之下的情况下我们最大的就是TokenTrade所有节点加合的话就是4然后同时呢这个TokenTrade的结构可以变宽也可以变深是根据这个DraftModel预测的概率来动态变化的这样子能最佳的方式命中更多的Token比如假如就是一个数的概率非常非常的致性这时候呢它会向深度延伸这时候呢命中的Token数会变多当遇到一个Token它非常致性的比较低的时候它就会放宽度延伸提升当前Token的命中率在这种情况下我们就拿到了Clover第一版本的这个性能收益在各个任务数据上都超过了每多的Token命中率大概50%以上的提升端端端的提升速度在30%的当时Token没有优化到最优7B的收益波动很大但是150B的模型收益比较稳第一版本上线之后呢我们其实还做了很多模型结构的升级因为我们当时发现了就是业界有很多EGO的业界有很多因为我们说业界有很多因为我们发现呢其实业界有出了很多新的模型比如EGO也是比较火的然后我们仔细研究了EGO跟Clover的区别然后EGO的话相当于是把整层Transform layer作为每个Head的Draft Model是模拟大模型的整个架构然后每次呢相当于把这个把每一个层所减成1EGO呢相当于是把每一次这个Head的信息和这个Nect Token信息过Embedding然后进行Concate然后Concate过AMLP之后扔给这个Transform layer然后进行一次输出然后同时呢下一TokenF使用类似的方法然后不断的循环这个Transform layer也就是说他预测多少个Token他就要就要loop多少次这个Transform block然后Clover呢其实我们当时也说了其实它是一个类RN的架构只有在第一次跑这个Transform layer的时候会跑一次Transform layer后面都是一个RN的结构相对而言会比较轻量从理论上来说的话EGO可能效果会更好一些但是呢他每次每个Head会跑一次Transform layer如果是5Head相当于每次Decode外跑五层Clover使用的是RN架构相当于比较轻量对我们还是在想能不能在Clover机场进行改进让效果尽量逼近EGO由于更加轻量能做到端端端性能会更好一些在这种愿景下然后我们开始对Clover结构进行升级第一个就是Loss优化Clover1都是在内部SFD中文为主的数据集上训练的数据风度非常高并且没有发现任何过拧合的现象但与EGO对比时使用的是相同英文数据集上训练了20个Epoch就会出现严重的过拧合现象然后分析发现呢其实是一方面英文相对比中文简单另一方面是为了追求极致性能EGO训练很多个Epoch我们尝试了很多路很多方法然后最终在EGO论文中就注意到了一个regressive loss其实是将transform block输出的hidden state与主模型输出的hidden state用L1Loss论文中其实并没有提及太多的原因刚开始的时候我们分析的时候是认为整个transform block需要重训练在之前做了一个MLP且未做残杂连接一个主要功能就是为了训练出手法的稳定性但随着Clover固定核问题的研究深入我们发现Loss这个Loss能很好的抑制Clover固定核问题仅仅根据Crossentropy预测token的偏续情况下可能会导致一些高频token增强概率的情况这个Loss能使得Clover能够关注跟主模型的一致性上而不是走捷径第二个是Clover预测token信息的前置其实在Clover1的时候我们可以发现就是transform layer其实是非常好的一个信息提取器但是我们发现我们在加入第一个Head的信息的时候其实是在这个transform layer后面的其实这个模型是相对而言是比较对称的其实相当于是这个后面都是RN然后前面是个transform layer相对而言比较对称但实际上呢第一个Head的这个信息其实是可以提前加入到这个transform layer里面的然后在这种驱使下我们就把第一次第一个Head的信息提前加入了transform layer之前过也是过了一个我们设计的RN那个结构然后再扔给啊再扔给transform layer这样子的话相当于是提前一个step把信息进行汇聚然后利用transform layer的这个信息提取能力能提前一个step把信息进行汇聚然后由于这项的修改呢我们第一个第一个呃next加的Head这个地方就用了直接用mlp不再需要RN的信息的汇聚了第三个改进呢其实是我们相当于把RN的整个结构从原来的直接用heiden state预测下一个通文的信息改为了啊先过一下这个啊过一下mlp改为了不仅仅要考证heiden state的信息同时还需要考虑这个input的信息扛开大家一起啊再过给mlp集中输出output这个有点模仿于模仿于慢慢的信息结构主要原因是啊有些通文变动还是比较大的使用query啊然后kv这样的方式的话嗯它不能很快的扭转这个heiden state的信息的变化第四个改进其实是相当于是我们增加了整个transmary layer的层数呃呃刚才前面也提到了我们所做的这个transmary layer是位于第一个heiden之前跑一次增加层数不会导致多跑但是eagle的话你想增加层数的话它跟hide是成成书关系的如果你eagle这地方增加一层的话相当于是呃每个hide都需要增加一多跑一次所以导致这种这个性能差异还是比较明显的所以我们在eagle这个在clover的时候增加这个层数提高整个这个transmary layer的信息提起能力当然就是这地方会增加一些参数量其实可以通过一些共享参数啊共享kvh啊window天使方式来优化呃对于端脑动来说增加一层的整个代价其实相对于还是比较低的比如7b的话大家是计数是3%啊70b的话是呃70b的话就相当于是只有1.几模型大的话其实增加一层就也就是损失也不是很大然后在这优化基础之下呢我们就呃做成了clover这个结构clover虽然是以rn的结构基础但是其实在很在这种训练上呃人能打败eagle这种deco的呃类尔为主的模型结构上在各个数据上都有超越eagle最大命中的提升7.7%端端端最大提升速是9.3%对不对足以框量都是跟eagle开源的代码是一样的呃但是虽然也是一个低效的实现然后我们就所有的这个模型结构设计版本之后呢我们就开始这个工程结构的实工程结构的落地了其实大语言模型输出了呃大语言模型大概分为两个阶段第一个阶段第一个阶段preview阶段第二阶段decode阶段然后preview阶段完成之后会输出一个token这个token呢就会直接过draftmodeldraftmodel会负循环每个hide然后然后动态sample出一颗tree然后把这个tree然后进行拍屏然后扔给decode进行并行验证并行推理推理完成之后呢然后进行samplesample就进入验证阶段验证阶段然后验证阶段找到最后一个无效的token然后再扔给draftmodel然后继续构建这个token tree然后改进的点呢就是说我们draftmodel需要实现然后需要实现这个模型的预测呀输出构建呀然后多batch下怎么高效的高效的sample然后构建那个tree mask然后decode的阶段呢需要支持并行验证然后它是需要支持tree mask然后就是tree mask好kwcash那些管理呀mlp部分呀是要并行跑然后sample这块的话呀要做一些新型的优化然后这个验证模块呢要根据tree mask进行验证功能性的优化呢第一个点呢是我们当时做了一个token的改进主要是tree mask上的结构的一些优化正常的这个token tree mask的话它是应该是一个二伪的矩阵但是我们发现可以通过一些简单的算法可以让它把它做成一伪的这个算法的实现就是说我们刚开始按照这个tree的层进行初始化0层就是01层就是12层就是23层就是3然后同时呢把所有呃右节点的这个总数都加到组节点上比如说这个整个呃a点b分之等共有4个节点就是把它左节点全部加上4然后在这样的算法情况下我们使用这个先序病例的时候这个左节点都会在右节点之前但是呢左节点这个数都会比右节点大我们就通过可以比较大小来判断这个这个tree mask的结构就跟如图下锁是吧就是比如说这样加完之后呢a就是0然后a点a呢就变成了5a点a就变成6a点b就是1依次类的1423乘是我们只需要向前比大小比如说到a点b的时候我们比大小只需要跟0做tention而这个a点aa点a是5和6他就不需要做tention需要做mask这样的结构呢我们是节省了一些存阻量的第二改进呢就是主要是算子方面的就是所有后远通空呢其实是嗯空前前缀的kvcash然后我们总共实现了两个版本第一版本是基于kuda core的实现会从那个读取kvcash到share memory然后doquery呢进行b型计算但这个缺点呢就是稍大的后远通空数就会导致这个性能下降比较明显同时gqv的引入会更快的导致这个attention算子进入computer bomb于是我们就有第二版本是tensor core的一个实现同时节省用了窗个prefuel在gqv的场景下收益也是比较明显的然后kvcash一块呢需要问一下然后呢需要问一下我们可以问一下这个tention算子这个tention算子这个tention算子这个tention算子这个tention算子这个tention算子这个tention算子这个tention算子这个tention算子这个tention算子然后kvcash一块呢需要我们其实在计券呃后远拖拖的时候是按pagetention这样子进行连续存储的在呃在forward整个tention计算的过程中呢也是靠我tremask来实现之间的这个这个关联性的一些处理但sample完成之后呢我们需要把这个命中的token然后拷贝到连续的空间方便下一次decode继续使用在sample之后我们也做了一些优化就是一方面是并行化sample这块呢一般会包括成功处罚的一些机制重处罚的机制呢它需要包含历史的已经选中的一些token但是实际上在我们到这个阶段的时候呢还没有到最终的验证这个时候呢这个token到底命中是无法预知的然后这时候呢我们最传统的方式其实是按照这个tention那我们用了一个vorexpill的去sampple但这种效率是非常低效的然后我们采用了一个比较暴力的方式就是把整个sample参数按照trade这个结构直接复制nfile这样子的话扔给这个整个sample算子的时候就是完全独立的n个输入这样子呢就能进行并行的sample然后同时呢我们后来还支持了一些动开token数的一些操作在一些低流量的时候后我们就可以把后显动温数调大一些提升命中率提高抵护的速率接下来我们就进入第二部分的通信优化部分我们在一些低端卡上可以发现这个通信耗尸占比非常非常大这个时候呢这个时候呢通信的时候呢GPU处于大部分的闲置这是非常非常乱性算力的然后我们在这个基础上也调研了很多很多方案希望能把这段通信过程中的GPU利用率提升上来我们也调研了很多方案就正常的Pepeline来说的话就是HiddenState过Tension然后过这个Auto过MRP过Tension然后实际上呢常规的一些方案来说的话就是GIM Overlap就是把Auto前面这个GIM进行Fuse Kernel然后计算一部分MRP计算一部分GIM的时候然后另外一部分在通信来提升这个计算通讯这个Overlap的情况但实际上呢在很多场景下这个Auto会比较时间比较长然后这个前面那个GIM的耗时比较短他俩这样的Overlap其实收益并不是很大很有可能还是带来负收益另外一种这个开源时间比较多的方案其实是这个呃两个request之间的进行Overlap然后两个request的Overlap其实就是模拟训练过程中的那个PP阶段就是一个请求在这个一个请求在做AutoJuice另一个请求做Tension一个请求在做计算的时候另一个请求在做通讯然后这样子进行这个计算通讯 Overlap但是呢其实对于线上的请求来说的话呃有很多的要求限制第一个要求就是说你所谓的也得有两个请求才能做好这个Overlap第二呢你两个请求的尽量是均衡的这样子的话这个两个请求之间Overlap的时候它的收益是最大的在此基础上呢我们就研究了一些方案提出了我们自己的一个想法我们当把一个请求然后切成两个呃两半比如说我们有32k的一个请求前16k呃先进行Otun计算Otun计算完之后呢他会把KFC写入然后他接着进行Oreduce然后这时候呢呃后16k呢就可以进行Otun计算了Otun计算呢Otun计算呢就会读取前16k的KFC同时计算自己的16kKFC其实本质上呃转送block的Seql的依赖主要提供在Otun计这一块Otun计的依赖呢只是每一层有个依赖然后只要我们保证每一层的前16k在先进行计算后16k后计算就可以保证整个序列的计算是嗯跟原始的这个计算是完全不同的这个计算是完全是一致的相当于是我们在暗层稍微错开一位之后就可以可以在Seql内维度进行嗯很好的Overlap然后有了这个方案呢其实我们发现了一些很多极端的case因为实际上我们现在发现的嗯比如说有一些低端卡是这个呃PCIE卡它其实通信占比非常非常高而有些那个比如说NVLINK看的话计算比又特别特别低针对这两种场景呢我们做了一些小的优化第一个针对于这个计算密集型的时候我们会通过嗯量化传说的方案然后让整个通讯占比尽量接近把通讯占比更低一点让这个计算通讯这个比例越接近越好这样子它的话计算通讯Overlife的收益越大啊对于这种计算占比比较高的情况下我们尽量把这个计算通讯Overlife的时候GIM跟这个通讯之间的这个Overlife的时候的这个呃同时并行的算子的时间减少嗯主要做法呢其实相当于是我们把比较大的比如说QKV的计算或者是那个GITUP的这个这个计算通讯这个比例越接近越好我们把比较大的比如说QKV的计算或者是那个GITUP的这个Mix的计算把它拆成多段这样子的话在Kernel Launch同时Kernel Launch的时候就是它AOIDOS只会影响第一个计算啊第二个大的比较大的计算的时候不会跟AOIDOS进行Overlife这样子的话它能充分利用全部的SM进行计算然后除了以上两种极端的情况下也可能出现这个MLP通讯和AOIDOS进行Otention这种呃这种不平衡我们也为此设计了一些可能的优化手段第一个呢其实可能是Otention计算量的一些不平衡Otention就是SequenceR第二段的计算往往会比第一段阶段需要高的这时候我们可以通过一些策略调整整个第二段的这个计算的这个Token的占比比如说第二段的Token可以计算60%第一段计算计算40%我们同时也可以结合Token Proofill技术然后让两个段计算Otention计算的这个开销不至于差异那么大第一还有一种就是OtentionOtention MLP通讯之间的不平衡在一些情况下如右图所致中的情况下Otention的耗时会比较长但MLP的耗时是最短的Otention其实是位于中间这种情况这时候呢其实每一段的耗时都会出现一个短板这样的情况下我们怎么能做好这个计算通讯overlap呢相当于是Otention的话是位于中间长度Otention比较长MLP比较短这时候有个策略其实相当于Otention跟MLP进行合并然后跟Otention一起相当于是把Otention比较长的计算跟MLP比较短计算进行合并计算然后O264跟O264进行合并这样子的话相当于是他们两个加起来的耗时跟他们两个加起来的耗时是几乎平衡的这时候呢是需要设计更复杂的签生方式然后如下图所示吧会把它切成4个MicroBatch这样子的话就可以做到Otention跟MLP合并计算然后两个O264Otention跟MLP合并计算然后两个O264做到这样子的均衡总体上的收益还是比较好的在一些PCE卡上4卡能到40%收益8卡到25%可以明显看到8卡的时候通讯占比更高了所以它是短板效应收益的只会到25%NVLink卡的话4卡是10%8卡是15%NVLink的时候8卡的通讯会占比更高一些然后刚才也说了结论计算通讯占比越均衡的话收益是越大的然后最终收益上限是取决于最小的占比具体的测试一些数据可以看一下录文以上就是这两项的分享很感谢大家感谢大家