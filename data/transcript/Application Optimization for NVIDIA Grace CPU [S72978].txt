 How to get your application running on NVIDIA CREASE Right, thank you. So I'm going to show you some things how to get your application running on NVIDIA CREASE. So this here is the first, and I promise only, marketing slide. I'll just assume you've already bought your own CREASE. So just very briefly, CREASE is our ARM CPU, and it's actually a family of CPUs. So I'll describe the CREASE CPU, which is the building block of a superchip. Each of these CPUs has 72 ARM Neoverse V2 cores, which has four 128-bit SIMD SV2 or Neon instruction ports. And all of these cores are connected together with a very fast interconnect with up to 3.2 terabytes of bisection bandwidth. The CREASE CPU is connected via NVLink C2C to either another CPU or a GPU. It'll give you up to 900 gigabytes of bandwidth between these two things. We have high bandwidth low power memory, which will give you up to 480 gigabytes of data center memory, and give you up to 500 gigabytes of memory bandwidth. You will get this all at 16 watts. And this is actually more or less the main point of CREASE. CREASE is very power efficient. It will have to do up to 2x per watt over two days service. Right. So we already got the first step done. You've bought your CREASE. Now what? So the idea is you have your application, you have the hardware, you somehow have to get the application on your hardware. And people can be quite fearful of that. Because there's this idea that getting your stuff running on ARM can be difficult. Well, it's actually not. The expectation is it's just going to work. Because right now the ARM ecosystem is very much mature. And most things will actually work out of the box. And there are very few cases you need to do some slight fine tuning. But typically you're fine. But then, this is the second step. Now you have the third step. You want to optimize. So it will run out of the box, but it might not run with optimal performance. This is what this talk is about. And when you want to optimize, you see this is actually an iterative loop, so you have to do many, many small steps. And you want you in each of these steps to focus on correctness. And then on performance. So, let's see what the workflow is like. The idea here is to not do a lot of work on your own, but rather rely on other people's hard work. On the left you can see some very simplified workflow for what you can do. The main idea is to get the basic things right first. This will get you 80-90% to the goal. And that means use the correct compiler and use the best compiler flex you can get. Enable optimizations. And finally, use optimized libraries if it makes sense for you. There's another step that you should tune the runtime arguments. Use 72 cores or 144 for a super chip. And then only if these steps fail, so once you've done all the basic things and still not happy with performance, then get your hands dirty, optimize. And the idea here is that you do this data-driven. So, you gather data for your application, you try and figure out what is the bottleneck, and then you improve the code. And again here, get the basic things right. Do as much as you can in the high-level language, which might be C++, might be Fortran, might be Python, whatever. And then if that doesn't work, chop down one level. And again, let me emphasize that the idea here is that the gold standard is of correctness and you have everything in the data-driven way. And this is essentially the roadmap for the rest of this presentation. We just follow the graph on the left down. The first thing is the compilers. And the story here is more or less simple. Just use a compiler that actually supports Grace on Neoverse V2. So, here is a table for the minimum version that you can use. But of course, if you can get a newer version, use this. Always use the most up-to-date compiler that you can get. And then you might want to check and update your compiler flags. So, we recommend the compiler flags minus L3, optimizations, minus mcpnative for tailoring your code, your generated code to Grace. And FFP contract fast to get best loading point performance. So, the whole story is a bit more complicated. But this will get you, in 90% of the cases, to the goal. So, on the first bullet point here, I've got the complete flags. That is the flags that mcpnative will actually resolve to. If you use GCC, you might need to use these flags because GCC will not detect the correct flags by default of native. I'm not going to read the flags. It's mcpnative. It's mcpnative. It's mcpnative. Plus a lot of, like, alphabet soup, which will just activate the crypto extension. The reason for that is that people sell niaversv2 calls without crypto, and we sell it with crypto. And then, if you have a code that uses a lot of floating point math, you might want to tweak the last argument. So, you can use minus-o-fast, which will enable fast math, which will do a lot of things that are a bit, let's say, shady. They work in most cases, but if you're unlucky, they can really introduce very dangerous bugs. They can destroy your whole numerics. So, for this case, really check the correctness. And finally, if you need identical output, you can use ffpcontract is off, which tells the compiler that it should not fuse multiple operations together. So, it will not fuse an add and a multiplication together. This will actually be good for accuracy, because when fuse multiple add, it will round down once, and the sync operation will round down twice. We will not give your bit device identical output. And then, you have some other flags you can tune. But these are not as important as the one that we have here. For example, it's minus-flt0 to have run a link time optimization, which will be, for some codes, be a huge improvement, for some will not be a huge improvement, but it's still significant. Grace benefits from code locality. So, typically, if people use profile-guided optimization, it will improve performance, but on Grace, it might even be better than on x86. And then, if you have an application that depends on the assignedness of chars, you either need to pass fsign char or funsign char. And finally, if you're Fortran, you might want to use fnostack arrays. And that's all the flags. There's a last point here, is that if you want to use Clang, NVIDIA has an own build of Clang, which is more or less top-of-trunk LVM, which optimizes compile speed, and it's actually validated for compatibility with our other technology. But you can also use, of course, normal Clang. Then, to the libraries. If you have a code that uses a lot of linear algebra, so HVC code, or maybe code that uses machine learning, the idea is that you should use the Netlip Plus, Labhack, and FFTW interfaces. These are standard interfaces with operations. If you have that, you can just plug in another implementation for these routines. And here, we recommend, of course, NVPL, the NVIDIA math libraries, or the new performance libraries, which are optimized for NVIDIA CPUs. They will keep being optimized for these CPUs. And they are, again, a drop-in replacement. Just compile the application again, and ideally, we'll get a huge speed-up. And finally, you can also use ARMPL, which is the ARM version of that. It's also pretty good on Craze. But also, open source libraries, such as Atlas, OpenPlus, Plist, they will work. They are tuned to some degree in Craze, but you should probably first try out NVIDIA. So now you've tried the basic things, but are still not happy. Your boss might not be happy. They need to get your hands dirty and do performance analysis and improvement. And the idea here is that you need to know three things. You know your hardware. You need to get to understand what Craze can offer you and your code. Then you need to understand the code, which hopefully you do, but you'd be surprised. The idea is that you know what your code is supposed to be doing, where it should be spending its time, what kind of algorithm data structure it's using. And then you should know your tools. And these tools will help you to do the first two steps. So a tool will help you to understand your hardware and your code. And you should use different tools. You might have your favorite tool. They will work. But I'll also show you later some tools that are pretty good on Craze. And then with that, you should have a solid baseline performance. So you start at some point with your optimal basic things, compiler, flags, libraries, so on. This is your baseline. You start from that. Then you go out and optimize. If you optimize it, it's always good to have an idea what speedup you can actually get, how good you're doing. So for example, you can do, if you do HPC, you can do a roofline model or another modeling. It doesn't be complex. It's just a good idea to have some kind of idea where you end up. And finally, if you have a large application that runs, so if you do like modeling simulation, you have a simulation that runs 30 minutes, that's not really good for this iterative process. You might want to try and find a small example or a proxy application that kind of resembles your application's performance. And if you have that, you can drop in and do the loop on the right. The loop is you compile, you check correctness, you benchmark, you profile, you see what hotspot you have then, then you optimize again. And then you follow this loop until you're happy or until you run out of time. And this loop will be data-driven. So in each step, in the profile step, we will gather the hotspots. You will know what to optimize. We will also check in this loop whether what you've done was right or wrong. So the first step here is understanding your hardware. And that can be a pretty complex topic. So I will just point you to two sets of documentation. The first set is the NVIDIA documentation. Here, mostly the Cray's performance tuning guide, which will have probably more detail than you want. It will give you details how to measure things, what metrics we have, or some things about setting up your system. And finally, if you want to get more information about the core, you can use the NIOverse V2 soft optimization guide from ARM to give you things like instruction latencies, instruction throughput. This will be a lot of detail. You probably won't need it. But for some things, it's good to double check. And then, to better understand your hardware, Cray's has multiple performance monitoring units, PMU. And these PMU tell you what the hardware is doing with your code. You have the core events. A core event measures everything that the NIOverse V2 core does. For example, you get things like how many instructions executing, how many cycles it's doing, how many branches it takes, and how many it misses. So many are mispredicted. And the same thing for cache references and misses. This will tell you a pretty good story of what the code is doing and where it's failing. And then, you have the opposite to some degree of the core events, the un-core events. And un-core just means everything that's not happening in the core but outside of the core, in the un-core. And there, we have two main things, two main sets of events. Both are captured on the socket-wide level in contrast to the core events, which you get for each of the 72 cores. You get the SCF, the scale of the cohesive fabric, which is the mesh that connects the cores. This will give you details about the last-level cache, about the traffic to memory, the traffic to remote memory, so traffic between sockets. And you also can get the PCIe events, which will give you traffic to PCIe. A nice use case for that is, for example, monitoring I.O. when you're using NVMe disks. There's a third kind of events that Gray supports. And that is a bit difficult to understand, but it's not really straightforward. That's the ARM Statistical Profiling Extension, SPE. An SPE is something that's measuring in the core, but it's not part of the core events. It's a completely separate unit that sits in the back end of the CPU and will sample all instructions. And this has a limited set of events, but if the event is supported, for example, if you have memory load in stores, it will give you a lot of detail about that. It will give you a completely accurate instruction pointer or program counter. So the other events, when you're sampling events, you will always have some kind of skit. So if you have a sample, it might not be the exact correct assembly instruction. With SPE, it will be exactly the same correct instruction. It will also give you addresses for data. So which data is your code accessing and also which latencies you have. This is very valuable data, but the tooling is actually at the moment quite limited. You can get it running supported out of the box on many Linux systems, but you might have some more difficulties getting it to actually work because it's pretty new. And again, for more details, you can either check our documentation with the Crace Tuning Guide or the Arm Universe V2 PMU Guide. And if you have both, it will be a pretty good overview. So now you've got these events, but you need to measure it somehow. And here, the idea is to give you a very brief overview of what tools exist. And that's, of course, not a complete overview of all tools that exist. There are very many profiling tools. The hope is that most tools support Crace already. And you should use a combination of tools. Each of the tools has a strength and a weakness. And you might have your favorite tool, might have a very specific tool. If it works for Crace, test it, use it, it's fine. Don't need to learn anything new. However, I would always recommend that you learn a new tool because, again, they complement each other. So the main recommendation here is to use our NVIDIA-developed NVIDIA N-Site Systems set of tools, which I will show later in more detail. It'll give you a nice timeline view of your code. It's a nice GUI. You can directly, officially see what your code is doing and when it's doing it. You can also annotate your code. And if you used the CUDA tools before, it will be familiar to you. You can also use open source tools. So the main thing here is Perf, which is a Linux-based, a Linux-included profiling tool. I'll also show you one or two examples later. You can gather the hardware counters that I showed earlier. You can also gather profiles, you can gather call stacks, things like that. You can actually do a lot of things. Then if you're into HPC, you might have used Liquid before, which has gray support. This also has hardware counter support, has support for editing regions, has benchmarking, a lot of other tools. Finally, there's a whole set of tools using eBPF, which is a set of kernel tracing primitives. And one of these tools is BPF trace. And this tool can trace anything happening in the kernel and outside of the kernel. It's very hard to use, because you need to know what the kernel is doing, and most people don't. I don't either. It's very difficult to use in practice. But it can sample anything. And there is also some example tools that you can use straight out of the box. And of course, finally, if you have a commercial license, a profiler like Linawo Map, for example, which many data centers have, like many HPC centers have, this one works on trace. It's very good with parallel code. Of course, you can use it. Let's go to the first example. That is actually, well, I shouldn't say this, but it's my favorite tool. Because it's very straightforward to use. You can use it nearly every time. That's Perfstat. That's a tool to count events. It has basically zero overhead. If you're not counting too many events, you can use this any time you're running your benchmark. And that will give you an average or a total of your program's execution encounters. It will tell you, for example, the average frequency. So it's cycles divided by time. That's usually because CPUs might change the clock frequency during execution. This will make your benchmarks more stable. You can get IPC, so how many instructions you do per cycles. You can get some things like benchmarks. And the idea is that it will not help you to really find your bottleneck. It will help you to characterize your program's behavior. It will give you a first idea of what it's doing. It will give you an idea of whether it's bound by memory traffic, bound by what the core can do, things like that. An advanced example is, you can compute the time series of each events. And here I'm showing an example of un-core events. So events happening on the fabric. And this example is essentially collect these events. NVIDIA-SFPMU0 is the measuring unit for the socket 0. CMEM read data, data read from memory. Like data ticks read from memory. And CMEM WR total bytes. It is bytes written to memory. And this says, collect this every 100 milliseconds in the JSON file, perf.json. And you can use this file then with Pandas and the other tool to read in. And it will give you a nice metric, and that is the gigabytes per second. Your application is writing or reading from or to memory gigabytes per second. And then it will tell you on a first glance whether using the memory traffic to its fullest degree. It will also tell you whether you have some phases in the application that's going to read a lot of memory. Because you might have something like an visualization phase where you read your data set from memory and you operate on this data set in memory. In cache, sorry. And you can see what your application is doing. And that will give you a very coarse overview. If you want a more detailed timeline, you can use N-Site systems. That is the screenshot shown here in the slide. You can get it from the NVIDIA website. And you can essentially do it in a two-step way. You first run on your trace. You run answers, profile, application. With some additional flags, of course. This will write down the profiling file to disk. You can then open this file either in the same machine or on your local laptop. It will give you this nice GUI. So Perf is very difficult to use because it has some documentation but it's hard to read. This one here is much nicer and much friendlier to you. And it will give you this nice timeline. So you see, for example, here the utilization of your CPU over time. It will tell you when your system is idling or when it's at 100% usage. It gives you a nice visual overview. It also gives you on the top, on the bottom here a call stack view. So which functions are called. It can do even more. You can mark up your code. So you can use NVTX and get at, on GitHub or as part of the CUDA toolkit for C++, C, Python and as part of the NVHC toolkit also for Fortran. And that allows you to annotate regions in your code. And these regions can be over multiple functions. And for example, here we're showing some kind of solver which has multiple iterations. And this will allow you to visually look at your iterations and see what you're doing in each iteration. And all of these things I'm showing here can be combined. So it's zoomed in on some part. But here you see that you see the CPU utilization on the top and you see the NVTX regions on the bottom. That allows you to visualize what the code is doing in which phase. But it gets better. You can also gather core metrics. These are events happening on the core. You just say ANSYS profile CPU core. Metrics equals to something. For example, you can see here how many stores you have for the backend or the frontend. And again, this works with the NVTX regions. You can directly see everything visually in the timeline and also in your region timeline. You can also get UNCOE events. So here I'm showing the local CPU memory read bandwidth per core. Again, you have this nice NVTX region on the bottom. Tell you in which region you're actually operating in. You can see, of course, if you scroll up, you can see the CPU utilization and you can see the core events. So everything here can be combined in one view. Sorry. Another nice thing are flame graphs. And flame graphs are the visualization on the right. If you haven't ever seen a flame graph, they're essentially a visual overview of what the application is doing, not in time, but aggregated over time. You can read them essentially from bottom to top. So each bar has a width, and the width is the fraction of cycles, in this case, with the application spending its time. So the bottom one is 100% of the time. Then, for example, you have a function called crow at one lattice. And you can see if you just compare the width of this bar with the width of the bar below, you can directly see which fraction of cycles is spent in this function. And then you do this iteratively up to the top. And if you look at the functions on the very top, you can see the top functions that are called. And every slice to the X and Y dimension is essentially called stack. You can generate these things with either inside systems or perf or actually also other tools. I show the script here, but essentially what you need to do is you need to run either annsys profile or perf record. You can write a perf record or a performance record to disk. Then you can use a script from Trenton Crack to generate a flame graph from this. And then you get a nice SVG file. It will be interactive and look like the thing on the right. It's a very good first overview of your application. So now you kind of know what your hotspot is, what your application is doing. You need to know what your compiler is doing, what you can do to optimize the code. And here we would recommend that you use something like Godbolt, like the Compiler Explorer, which is a visual overview of what a compiler is doing to a code. So on the left here you can see the code, and on the right you can see the compiler generated assembly. And you can have multiple compilers next to each other. You see what your compiler is doing with your code. That will allow you to compare different compilers, compiler flags quite easily. And it's much simpler than actually compiling yourself with multiple compilers disassembling it. If you have a small code snippet, so a small extracted hotspot, this is the tool to use. And then finally we are at the range where we want to optimize ourselves. I keep this very brief, because the idea is that you don't have to do this normally. But if you do, I'll show you two things that might get you started. The first thing is SIMD. As I mentioned earlier, the Cray has 4x 128-bit SIMD ports, either with Neo and SV2. And the idea again is that you follow the graph on the left from top to bottom. And again, basic things first. Most compilers will generate pretty solid auto-vectorized code, whether it be GCC, LLVM, NVIDIA compiler, Cray and so on. You can get some compiler directives, for example, OpenMP, Pragma, OMP, Parallel4, SIMD. Tell the compiler, you can use SIMD, it's fine, I trust you, or trust me, it's correct. And that will give you, typically 90% of the performance. You can combine this with libraries, so if you use NDPL for your matrix operations, it will be vectorized already. If your application calls in these libraries often, that's the way to go. And if these two things don't work, and only if these two things don't work, we would recommend that you use intrinsics or assembly. So intrinsics are the thing on the right, it's somehow, you write the instructions you want to get, more or less, but you still have the compiler to generate the assembly for you, which means that you don't have to do things like register allocation for yourself. And that is typically the best thing to do for 99% of cases. And if you really think you can beat the compiler with register allocation, which you might be able to do if you're very smart and spend a lot of time on it, you can drop down to assembly. And there are some links on the slide to get you started. There's a lot of documentation, but it will be a hard journey. But that's not because it's ARM, it's just because it's just a hard problem. That's why we have compilers. Very briefly, if you have a compiler, you can get nice diagnostic reports, tell you what has been vectorized and what has not been vectorized. Gotbolt and Compiler Store have that. You can drop a few hints, you can use restrict pointers, tell you, hey, the pointers are not overlapping. You can use Fortran, which has no overlapping pointers. And you can drop in some pragmas to tell the compiler, hey, please vectorize it, it's okay. Finally, the last possible, not the last organization I want to show you is something people might not think about when going with ARM. So ARM has a different memory model compared to x86. And we say that the ARM memory model is weakly ordered. And that means that the CPU is able to reorder load and stores during runtime. That means that writes from one thread or from multiple threads may not be visible to other threads in the same order. And that's very good for performance. The CPU can just reorder things, which can make things very fast. But if you want to pass a message between threads, you actually need to ensure that things are ordered as you would expect. Otherwise, you have race conditions. And if you have a correct C++ program, you will already have done that. You're already done. So if you use, for example, STD Atomic with the default memory order, which is often memory order sequential consistency, it will be correct. However, if you drop down and use a weaker semantic, so if you tell the CPU it can reorder some operations with some restrictions, for example, release acquire semantics, you might improve performance. So here, be careful. It's very easy to introduce bugs with that because the memory model is tricky. You need to think very hard about what you're doing. And again, if in doubt, don't think. Use other people's work. Use a mutex. Use a library. This will hopefully be optimized at this point. And this is only one slide. It's a very difficult topic. For more contexts, I would recommend that you use a watch a talk, for example, Herb Sutter. So, where are we? The main summary is, most things will work out of the box. Don't worry about getting things running. And then, you can optimize. And here, the idea is that you can get the basic things right, and you will be mostly there. The things are, choose a good compiler, choose good flags, and choose good libraries. And if that doesn't work, optimize by yourself. And the idea here is that you should verify correctness, and you should use data-driven tools. Then you understand the performance, you understand the bottlenecks, and you can optimize these bottlenecks. So, that concludes my presentation. The last thing, we have a CWE today at 3, how to run optimize your workloads on the NVIDIA Cray CPU, which will have more time to question than this session. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. So, now we have a small, quick Q&A session. If anyone has questions, I would just roll out the mics. Perfect. So, I'll start with him, and then, later to you. Hi. Thanks. You showed, sorry, you were showing how to calculate the memory bandwidth using PerfStat, and then you showed the same metrics in N-Site systems. Mm-hmm. Does it do the same math for you that you showed on the slide? In N-Site? Yeah. It does? Okay, good. With Perf, you have to do the math yourself. That's great. Thank you. Okay. So, I have two questions, if that's okay. I actually only had one, but I wanted to make a comment on what you said about always using the latest compilers, because we have been having problems, and we've been working with Brent Laback, and he said that from series 24 on, the NV Fortran has started to use LVM Mm-hmm. So, one comment is, is it still relevant to separate out LLVM and what you called NV Fortran, because it's all the same thing now, and does that change your slide? Is it? So, and I can tell you, when you say use the latest compilers, that 24.7 was fine, 24.11 was a complete nightmare, and 25.1 seems to fix stuff again. The LVM background. So, what he has just told me that he uses the LVM backhand, but it's not the same frontend. Is that right? I'll talk to you later, but my real question, which I wanted to ask, was that I come from the point of view of the ARM64FX, so I do code optimization for Fujitsu, and we were wondering whether it would work straight out of the box, because I want to get hold of Grace. Yeah. Because it would be fun to, because compilers, as I said, is a very important part of the performance optimization, so I was hoping that, you know, I could run binaries on both things, and run them on both platforms, and see how they're compared. But in your opinion, do you think they'll work straight out of a box? Binary compatible? They should. So, A64FX has a different vector length. Yes, but according to the ARM specs and the Grace specs, they should be the same, but I've never tested it out. So, if you've written correct SV, it will be fine. Okay. That's more or less the answer. Okay. It will be easier to get performance on Grace than on A64FX. Yes, that's why I wanted, because I have to performance optimize on Fujitsu. If I can play with Grace, then that might give me insight, because Fujitsu optimizers aren't that very... Yeah. So, just to... I would like to ask maybe a question that goes in a similar direction, but maybe a lot simpler. Is there any... I mean, you now interchangeably refer to GCC and Clang, and you didn't really put out a version or something. I just want to ask, do you have... What's your experience like? Which compiler should be used for Grace? Which is the best combination? It depends. That's the answer. So, typically for data center, I would say compare both. I would personally start with Clang, because it's a bit nicer interface in GCC, but... Is there a minimum version that you would require for... I don't know. It's the one on the slides, but typically used the most recent one. It's very... Right. I think unfortunately, we are running out of time. I would like to end this question... the questionnaire. And if you have any questions, feel free to ask him one-to-one outside the room. Yep. So, thank you so much for your time. It was an amazing session. Lots of insights and lots of good questions. So, thank you. Thanks. Thanks. . . . . . . . . .