 Thank you. Thank you. Hi, my name is Janos and I'm working as a data scientist for Siemens since two years. And I am specialized and responsible here at the company for adapting large language models to our engineering domain. Right. And today I will just give you some some insights. How we use the specialized LLMs to optimize processes to save some money. And overall, how we introduce them to our teams and work with them. I have some expertise in the domain specific adoption of these models. So I was first in touch with the with Word2Vec 2011. And here during my PhD time, I adapted I adapted language models to the pharmaceutical domain. And what I do, I'm using my knowledge to adapt it to this engineering domain, which is quite different. So let's start. We have more or less a big data problem. This is big data problem exists probably in every huge company. So the project volumes in our case increase over the past years. And to be honest, the most of the company's documented knowledge is in the form of text, which is not only the case for us, but also for other big players in the world. So the number of documents as a side software comments increases exponentially. In our case, it is because we have to maintain our systems, maintain our trains over decades. That means that older versions, older architectures have to be documented and we need to search in these documents. So for example, if a customer wants to have some details about the systems that we introduced in 2002, then we have to answer his questions. And we have to answer his questions. And we have to answer his questions. And we have to answer his questions. And we have to answer his questions in these documents that are archived here. And the problem is that with this increasing numbers of documents, the access to the knowledge is quite problematic and it's getting worse and worse. So to give you a feeling here, some of the questions, our domain experts, our system architects, need sometimes 45 minutes to answer more or less one question. Right. And this is a lot of time that we can probably and hopefully save in the future. We are the engineering domain, which is quite close. You can just imagine that the secrets of a company won't be published in the Internet. So the architecture, IT security things and all the details and ideas are quite close. And I give you a feeling about the kind of documents we deal with. So on the one hand, we have this architecture documentation. So for each system, we have sometimes hundreds or even thousands of pages, which contain really in detail how the system is constructed, which hardware components and which software components exist and how they work together. Right. And sometimes the systems are also connected with each other. So this is a kind of complex, complex documents, I would say. So this is not the easy PDF file, which contains only text. Right. Then, of course, we have customer related documents. So for example, we get also train log files. And you can just imagine every 200 milliseconds, one of the trains is logging. So many systems we have on the wayside and on the train side. And you reach quite fast petabytes of data. And we have construction plans, which are also quite complex. So you can just imagine that the models that exist outside of our company. Of course, if you want to draw a nice picture of a cat, then it's not a problem because there exists a lot of training data. But these construction plans are so complex. And of course, it's hard to train models to find enough training data on the web. And we have also system-specific code libraries, which means we code a lot here. So we develop the most of the systems by our own. And all the documents contain company-specific information. And I will give you a feeling. What does it mean? How specific this language is? So it's hard to understand, even for me. And I'm working since two years here, some of the texts. So it's hard to me to get out the information if it's correct. It's what kind of systems exist and how they are connected with each other. So only domain experts that develop the systems and maintain them roughly understand them. Right? So this is one of our huge problems. So these data collections will probably never be included in a traditional pre-training or fine-tuning. Because this is closed. So I've mentioned that during my PhD time, I have investigated LMS and LLMS for the adoption of the pharmaceutical domain. Right? The pharmaceutical domain is quite beautiful because you have all the databases that are online. All the documents are online. You can find databases with entities like drugs, genes, or diseases, and how they are connected with each other. You can find all the publications here. Right? So there's a lot of curated data that is very valuable. Right? And at that time, we understood that you need this domain-specific adaption always to increase the accuracies, to satisfy the domain experts, the doctors, the medicines, the other pharmazeutical stuff, to use systems like that. Right? And here's the problem. Of course, two years ago, we started to introduce large language models to our domain. And like the most other companies, you start from the bottom. So you use more or less the details, the standard stuff that exists. So you use services to get your large language model. You use external vector databases to index your data to building REC systems. So we started to index our architecture documentations. And of course, we made some marketing and the feedback was quite hard, to be honest. So for rather simple questions, seems to work. But if you ask the domain experts, they are just really disappointed. So more or less, they are really hard in the feedback that the systems are useless for them for the daily work. Right? And I mean, I've heard that already from the pharmaceutical domain. I was in touch with researchers presenting them my output. And always precision is all they need. So there has to be some confidence that the output makes sense. Right? So that they can use it. The information should be correct and comprehensive. It means so we have to reduce the hallucinations, right? Drastically. And of course, there exist some approaches that we can use to reduce it. So I will present you now some of the use cases that we already developed here. Some projects and I selected them not randomly. I believe that the three use cases that I will present in the following. So they are more or less, they are something that every company is dealing with, with the problems that we try to solve here with the use cases. And the first use case is a rather, well, classical problem. It's a multi-label classification problem. And we get from the customers tenders that contain requirements. And here's one of the example tenders that you can see. It contains 44,000 requirements. In addition, you have also prosa text, which extends the tenders. So you can just imagine how many pages we have. And the process is following. When we get the requirements, you need some humans, some experts that decide who has to answer the requirement, who has to estimate the costs, who has to estimate the risks for a special requirement, right? So you have a lot of departments here that are responsible for the different systems. Let's say 120 around the dot. And you as a human has to decide, okay, who will be responsible to answer this requirement, right? Well, and this was more or less my first project. I just said, okay, multi-label classification problem, it's easy, right? I know that from the past there are millions of, not millions, but there are a lot of approaches here that we can use. So given the requirement, we want to find the department that is responsible for that, based on historical data. And here my journey began. So I tried out the basic approaches for this problem. And then I went through and I just also tested the large language models, fine-tuned them for this problem. And at the end, I have to say that we are working at the moment with BERT models here for the classification problem. So they reached the best accuracy here. And here you see the interface of the tool is called Extender in our case. So there's this proposal, what kind of class should be assigned to this requirement. And in addition, also the users can search within the requirements. So this is more or less, they can use semantic search or keyword-based approaches. And what I did is also to extract for each domain some specific keywords. So for example, IT security, you will have a bunch of words that are correlated with the domain. Let's go on some details for this use case. So as I told you, I have more or less the historical data of 100 tenders, which are more or less 350,000 requirements and pros attacks. So I started a pre-training of the BERT models at first. And afterwards, I've fine-tuned the model for the classification task. And in addition, of course, I try out to improve the results. So for example, using BERT topics to extend the text with some information. So I mean, if you deal with machine learning or in general AI stuff, you know, you get the first results. And then you start using some tricks to improve one person here, three person there to reach an accuracy, which is more or less good enough for our human experts, right? So at the moment, I am also exploring the idea of the combination of BERT models and models like LAMA. So the idea here is more or less to use LAMA after the classification to make some summarization, to write some comments about risks and for filtering data. So these models seem to do the task quite fine. So because BERT, of course, is quite strict, you just learn a bunch of labels and the LAMA models are quite flexible. So afterwards, I can use them to reassign maybe the data if it's needed. This can happen because the domain experts, for example, say that they just need for a certain project, for certain tender, not the 120 labels, but maybe only 10 or 12 will be enough for a certain project. So the top K accuracy, I mean, this is one of the measures that we have here. The top accuracy, so given that I'm predicting per 10 around about 2.7 labels on average, the human predicts 2.2, so around about quite near to the reality. So the accuracy, we reach on different test tenders and accuracy here between 0.8 and 0.9. So savings, I mean, I told you already it takes a lot of time and this is boring work. So this is something that, of course, probably nobody wants to do going through all the lists and just assigning the whole year here the domains. And for me, this is more important that I'm reducing more or less the stress of the people. So we can save you up to 1440 hours per year. We try to calculate, we try always to calculate rather conservative and then we just, well, see if it's working as expected by applying some tests in the reality. So comparing the tool, comparing the teams that are using the tool and comparing the teams that are not using the tool. So we try at the moment to extend it because we have different kinds of departments here. So we are here, mass transit. But we have also the main line and which is another department where the domains are a little bit different. More or less there are 10 departments that are interested in that here. Well, generation of compliance statements. This is our actual work. So this is quite interesting. So given the requirement, the domain experts have to estimate if we are able to fulfill this requirement. Right. And they write down the text and the comments. So we are compliant. We are partially compliant or we are not compliant. So you get, you get texts from this domain expert. And this is, this is a huge risk, not finding the non-compliant statements. It means, given a requirement, we say, okay, we, we are compliant, but this was a wrong estimation. Then usually the customer comes to us and say, okay, please fulfill the requirement. But we say, oh, it's, it's, it's, it's a little bit hard. We need to develop something. And the customer says, well, based on your contract, based on your compliance statement, you said you will do that. And it's your problem now to pay for the extension. Right. And this is a huge risk here. So our idea is more or less to, to use exactly this, this approach is to find exactly this risks and give the humans a hint. So we generate texts here, give them some hints why we are not compliant, for example. And of course we are up to date. So our team tries to be always up to date. So what we are applying at the moment and testing is group relative policy optimization for the different tasks that I will present here. And at the moment, yes, we try to measure it. So the next, the next use case I want to present to you is code generation. And, well, of course, the most big players, they have development teams and they, they caught a lot. And in our case, it's in some parts, some teams, we have a domain specific code. So this is not traditional Python code or C code or Java code, right? So this is a code for the simulation of our, right? Which is quite easy to, to reproduce to, with the, with, with, with the internal Siemens GPT that we have here. Right. So for example, I'm, I just want to write a loop over a data frame, please write me a Python code for that. And we will get it. But this is something else here, what you see. So this is a code for the simulation of our, of our systems, of our trains. And we have own libraries here. And just believe me, we have 10,000 of tests written already. And when I met the team, I, I didn't expect that it's so complicated. So we have testers here, they do nothing else as described. What happens within the tests. So this is quite technical, what we see here, but it's about the, the control units of the trains. And this is a reaction code. So how the codes should react, how, how the train should react. And then they use this really specific libraries, functions, that we have written, developed, DRAMATIC library as well,c examples of our Trafinbar she's Geo, Login locations. And things are really special. Now we have one And land participants, help me to inject send tekn 옛via's notesaroo. I'm going to do more to talk about the manual 부 vanity and write a that we need in multiple options. Divinity at how you suggest you know that you can cause hastening data to in therapy. And each nurses are thumbily teach � gasket meter and network reporter Kursaf работает project. I was really surprised because this is complex. So what we did here is, of course, using the large language models and we are able to generate the code. So at the moment, we are also testing it. So this is already online. So this is a screenshot from our chatbot that we did. So given just this text that you see at the top, we generate the most probable code. And in addition, we show the user also the most semantic similar code snippets that exist. Because this is how the developers work. I mean, I interviewed them and said, well, you know, when I start to develop new code, I just try to search in a file what is quite similar. So I just put the pieces together. And then at the end, I will probably have a test that I need. So these are some functions that we implemented in addition. So this was one of the reasons to extend. So, of course, generate the action code. So I have an action. I have a reaction code. And then, of course, this semantic search features that you need. And, of course, some details here. And so what have we done here? The techniques that you applied. On the one hand, we use continued pre-training, fine-tuning of language models. And in addition, which was quite interesting, is the fine-tuning of embedding models that you use for your vector database. Right? So this is something that is quite important if you are so domain-specific. So here you will get, well, a lot of improvement. Right? So this is also a combination of, this is more or less a specialized REC system that we applied here. And we used for the evaluation code bird score. So we reach here 0.91, which is quite nice. And from teams, we get already nice feedback. And we will run here also on reality test. It means one team is working with the two, the other one not, to see how much time we really save. So this is the savings. This is also, I mean, this was a surprise for me because I thought this is a small use case. That we save 40%. This is a conservative estimations. But if you just imagine that we have 40 testers, I mean, that do nothing else, then you can imagine how much time you can save with this kind of approach. Right? So this is a lot. And to be honest, I was really surprised. And this is more or less a good example for low-hanging fruit. Right? So because we did not spend too much time and effort to bring that online. And then taking a look on the savings, the possible savings here, I was really surprised. And this is some use cases that inspired us also to develop a process, how to find them quickly. The low-hanging fruits, less effort and a lot of savings. So actual work, as I told you, user evaluation. And of course, we play around here also with GP, GRPO. And yes, this is the progress. So the last use case I want to present you today is more or less high-precision search and expert systems for our documentations. We have tons of them. We have tons of them. We have tons of them. And if you take a look on the text, I mean, if you are familiar with language modeling and language semantics, then you will see that the text is full of abbreviations about more or less relationships between the entities you never heard about. Right? And the documentations and entities are connected with each other in different documentations. Right? So, and this is more or less one of a simple questions that our domain experts ask and have to answer. For example, I use this as an example because the answer is rather small. Some of the answers reach two to three pages because they are really detailed. And while this is something that we need definitely. So, this document search, right? Getting and extracting the information that is based on this growing, this huge amount of text data that we want. We cannot deal with them as we do at the moment. Right? I told you that it is growing and some of the questions to be answered, they have to be answered. And the expert in 45 minutes, well, it's today. But regarding the growth documents, you can imagine, maybe it's tomorrow, in five years or three years, it will be maybe three hours. Right? So, also, we have some feedback here and comments to get more information, what the user needs and how good the answers are. So, applied techniques and savings. We have fine-tuned and betting models. There's also REC systems. Also here, continued pre-training and fine-tuning of the models. And we, in some cases, we go even step further. So, we extend the token vocabulary, as you have seen, a lot of abbreviations here. And, well, more or less, we are trying to do also some research to calculate the vector representations by when we just include the new tokens. So, more or less, we will try to get a more domain-specific, semantically optimized vector representations here. So, this is based on my research in the past. So, there are some traditional approaches, how we can deal, how we can move the dots in these high-dimensional spaces and how to test them. Savings. At the moment, it's hard to estimate. We have more or less, I would estimate thousands of users at the moment. At least 3,000 users tried it out. And some of them are using that. And what we want to get out here, so who is using that? How good is the quality? Which is not easy to estimate. So, the actual work is evaluation, evaluation, evaluation, which is quite hard. So, you don't have this golden set, right? And it's really hard to get it. So, what we do is we try to get all the online helpdesks and we go manually through it because the domain experts write in some of the online groups what they are searching for. And we collect the answers, right? This is more or less the gold that we need to test our system and to evaluate it. And this is quite important. And if we don't have it, then we can't measure improvements, right? If we get a new model, right? And to be honest, for this domain specific problem, it's really hard to do the evaluation. You know, so again, the example from the pharmaceutical domain, I have all the database data, all the facts exist somewhere. So, if I get an answer, I can just evaluate it somehow automatically, right? Yes. So, at the moment, evaluation. Also, this query analysis, which means we are collecting question-answer pairs. And again, I mean, don't want to repeat it, but it's a group relative policy optimization that we also try to bring into the game. So, these were the three use cases. I believe, well, it could be really interesting for you because probably from my experience, my last 25 years in the big companies, more or less, they fit more or less in each of these companies here. So, the next, what I want to present to you is a quite cool process, I believe, to find these low hanging fruits that I was talking about, right? So, a lot of savings, less effort, right? So, what we do is we have more or less five steps here to identify them and bring them online. So, at first, sometimes we just get an idea or a problem from domain expert or a group. And sometimes we do also hold workshops with them. So, we prepared some slides, so, and they have to work a little bit to write down the ideas and bring everything to paper so that we get an overview. And as always, so, when we do the workshops or the interviews, there's always one AI expert in the interviews, right? So, I just can immediately take a look on the problem. I get some rough information about the data, if it's structured or not, if it's in a database or in thousands of words, files and PDFs. And then I can immediately estimate, okay, this kind of model should be fine, this kind of approach should be fine to solve the model. So, I see a chance here. And if this is possible, if there's a goal from the technical side and also from the management side, so then we get to the next step, which is more or less an analysis step. So, if the decision is go, then we go to two. If not, we just get rid of it, right? So, the analysis step goes more in detail. So, what are these data in detail? How can I get the data? What are the stakeholders? What are the potential users here? And approaches that we have the potential to solve the problem? And here, rough estimation about the efforts and the savings. So, and if everything is fine, so we have enough users, it's not only one, which happens quite often, that some users are just interested to have a solution for their own problem, but not for a group of users. And then we just go on. We have this effort cost analysis, which is more or less a matrix. And here we can identify the low hanging fruits, which means we have on the effort side, less effort and high savings. So, this is the region we want to reach. The other one, possible use cases, we just discussed then afterwards, right? So, the worst case is high effort and low savings. So, we just get immediately rid of this group of problems. So, the next step is done by us then. So, in our team. So, the proof of concept or hypothesis, do they work? Do the model works? And of course, this is more or less a dump rule. So, we get everything together and then we try to evaluate the accuracy. So, we see we have some metrics then and then we see, okay, so the first approach, the first proof of concept gave us 45% of accuracy. For an example, just right? Then we see, okay, from 45% to the desired 80% to 90%, this will be a hard way, right? And then we decide at this point if we go on or not, right? So, often if the accuracy is not high enough and we tried already a bunch of approaches, the older one, the newest one, and we cannot improve, then we get rid of that. But then there are some other cases where we get with the first shot, let's say 0.7, 70% to 80%, now 70, 75% of accuracy. Then we go on because we now adding some additional models using some data extension techniques, manipulation stuff. So, we know that, of course, each approach gives us some progress for accuracy, right? 2%, 3% is always the same. Afterwards, applying some of these approaches, more or less we reach this desired accuracy that we need for our domain expert to be accepted. So, if this is done and we develop the plan, the whole pipeline of approaches to solve our problems, we need MLOps and web developers to bring everything online. So, at the moment, for the proof of concepts and chatbots, as you have seen, we bring them also online. But then you have an ongoing project, right? And, you know, users, they just want more and more and more. And, of course, you need a team that is then responsible for the project so that users are approaches, automatize everything, also the evaluation and also the developers that extend the user interfaces and the new features of the solution. So, this is more or less what we do. So, the infrastructure for short, we have an object store where we store the pre-process data, the models in our adapters. So, we are using adapter technology to have many flavors of our specialized language models. At the moment, prototyping, training, and analysis happens on four NVIDIA H100 GPUs. For inference, we are using four NVIDIA L40s GPUs. We use here models up to 70B. And in order, and I'm quite happy about that, we will get more or less than a few weeks and months, eight H200 GPUs. So, we just go on. We can try to work with bigger models beyond 70B. Okay. The takeaways I want to do. LNMs out of the box do not lead to satisfying results in a complex domain. The selection here of use cases is crucial. So, this benefit cost analysis you should do. This low hanging fruit exists in every company you have. And, of course, you need the older approaches to improve the quality. So, like topic models, clustering, progression-based approaches, and what you have seen ever before. I would say start with 70B models for applying in this domain specific problems. I mean, it depends on your language. But for us, we gave up to trying to solve the complex tasks with smaller ones. Stay up to date with your models and tools, which means more or less every week we try out new embeddings. We tried other vector databases, new approaches. So, everything that is new, we try to do it. Consider that the evaluation can be very complex, but there's no way around. So, you have to do it, right? And you will spend sometimes more time on the evaluation than on the development of the solution. Well, for on-premise solutions, you need a team of data scientists, data engineers, the MLOps engineers and web developers. So, this would be a desired start for me. I mean, we started smaller, but now we have this kind of guys on board. And you need technical leaders that resist the guidance, the PowerPoint, the army of the three months AI experts. I mean, it's exploded, right? I mean, within two years, there are now in the company hundreds of experts, but to be honest, they are more or less, they are learned everything more or less a little bit online. And they have no clue about the older approaches, for example. In the most cases, they don't want to criticize the people, but they are sometimes dangerous because AI can't be evaluated. Yes, I mean, this is not true. And if they try to force you to believe that, so this is for me something that is dangerous. So, I'm done with my talk and thank you very much for your attention. I hope it was something that was interesting for you. I will be at the DGC and of course, just connect me for further questions. Thank you very much. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Bye-bye. It's a beautiful beginning of this último series ofABix. Thank you. WOOOF 410084674、 Incredvy 9504. assumption out there can GlueDeck. I realized that the pes .. We discussed theINGS to the pulver cycle three miles made. Thank you. That would probably be a good link to you. Thank you, 2020. Friends, I think theakes of the jerk. Sure quoi. There was a great group of Scouts Paradigmists for rep�cdage Lion.նEAWComEmgere琴, the dehydration is brought back in the cup and favorite República option on the Clara các perturbac sidebreaking sua emos on the Reese'sRşaet. Thank you. Thank you.