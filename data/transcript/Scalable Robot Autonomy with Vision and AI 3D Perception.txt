 Hi, and welcome to the virtual GTC session, Scalable Robot Autonomy Everywhere, All the Time, with Visual and AI 3D Perception. In today's session, we'll talk about the multi-trillion dollar opportunity around robotics. We'll explore the essential role of visual perception for physical AI. We'll discuss the physical AI information platform and how it actually helps solving real-world problems that affects the scaling of robotics. We'll show concrete examples of applications leveraging it today, and we'll discuss the future of intelligent machines and intelligent sites leveraging physical AI. So we all heard about physical AI, but what exactly is physical AI? So generative AI works on text. It started in 2021, where text-based operations could be handled with large language models. Large language models would use text from the digital world and will apply actions and additional data on it, still on the digital world. Then pretty quickly later, generative AI started to operate also on images and videos. Generative AI could use images and videos and text as input. And with queries, large video models or vision models can use that and generate new digital data. But still, that's all in the digital world. The next big revolution is physical AI. That's using large language models and generative AI to use data from the physical world to make actions in the physical world. So that's what physical AI is all about. Now, the opportunities for physical AI are huge. The numbers here are just numbers that we gathered. I would believe other numbers as well. But the opportunity is obviously huge. Practically, physical AI could impact any type of machine anywhere on planet Earth and beyond. Any machine could leverage physical AI to perform better. When we look at the history of machines, machines started with control automation, meaning that the machine had very fixed structure, very fixed mission, and it could operate only within very small margins. Think about AGVs as an example, where the AGV has to follow a certain line. It can move. It can stop. It can move a little bit to the right and the left, but it's very limited. The next generation had programmed automation, and that's what's usually called AMRs, autonomous mobile robots. Autonomous mobile robots would be programmed for different applications. Their capabilities could be much wider, but still, every specific behavior was programmed by an engineer, and the machine is as smart as the set of programs that were defined into that specific machine. The next generation will have generative automation, and that means that there will not be any more need to program any specific behavior. The machine will be smart enough using physical AI to understand the surroundings, to understand the physics and the behavior around it from the real world, and to define the right behavior. The change is huge because it will enable machines to do much more than what they were initially designed for, and it enables much faster scaling, because now there's no need for a human being to do all the programming for each individual behavior of a machine. So that opportunity is huge, no doubt. But you cannot believe that that can happen without seeing. Visual perception is the information that is needed for physical AI. For physical AI to happen, it needs to see and to understand the environment around it. It's really connecting the physical world to the digital world. For that to happen reliably, the visual perception needed has to learn and adapt to changes, and it has to learn and behave in different environments. It also has to be robust and reliable. People don't like machines to make mistakes, and every small mistake in understanding the world around the machine could lead to a misfunction of the machine. So it needs robust and reliable perception that works everywhere all the time. So in today's discussion, we'll talk exactly about that. We'll talk about the physical AI information platform that enables all that. How we can use data from different machines, collect 3D data, position data, and semantic data, all together to make the physical AI information platform that can leverage to make machine autonomous and smarter, to make machines do smarter behaviors, even if still human operated, and to make sites behavior smarter, applying physical AI to the full operation within manufacturing lines or sites. We'll also talk about the capabilities beyond that. So in Argo, we focus on enabling mass adoption of intelligent automation with AI. We power machines with reliable human-level perception, allowing them to see and to understand the world around them and to be able to adapt to changes in their surroundings. The perception engine provides the information layer for physical AI. It provides intelligence to all mobile machines. It enables autonomy that can walk everywhere. Right? That's what we want. We want machines that can operate everywhere that we operate machines. And it provides continuous 3D and semantic mapping that leads the information layer. The perception engine learns and adapts to changes. It is based on vision, of course, AI, and sensor fusion. It's easy to integrate and deploy. And it's proven in production and ready to scale, making the development of future robots and machines easier and faster. The perception engine itself is a full-stack localization and perception software. It's tightly coupled with low-cost camera-based reference design. And it's mission specialized for real machines. It is trained and tested on data from thousands of kilometers traveled, making it very applicable to many different types of use cases and behaviors, environments that are indoor, outdoor, and in many different applications. The data itself includes localization, 3D understanding, so geometric understanding of space, and scene understanding, which includes semantic understanding of space. All of that built to walk anywhere, anytime. So let's see a few examples of the use of that technology in real-world application. You can see just three examples of different robots operating and leveraging the perception engine. On the left, you can see Onward Robotic Platform, platform designed for e-commerce application that is capable to operate in dynamic, structured, and even in very narrow aisles. We also see the wheeling platform operating in manufacturing lines and automating cars. And the Kappa robot that also takes automation to outdoor operations. All of these show the capability to operate in dynamic, unstructured environments, indoor and outdoor, on uneven surfaces, and really taking robotics from the classic green field areas developed for robotics into the brown field. Real-world applications that exist today, where machines operate today, and need to become more intelligent. Real-world applications that exist today, and need to be more intelligent. Real-world applications that exist today, and need to be more intelligent. A second example is a platform from Kappa Robotics. That platform is designed for intralogistics, with a unique design for the robot mechanics that allows it to operate even outdoors, on uneven surfaces, on wet terrain, and even in snow. That platform can even climb a few steps. To operate both indoor and outdoor, it uses the perception engine as the main source for positioning. And as you can see in this video, the platform can carry goods between different warehouses, carrying, for example, goods from the storage warehouse within a manufacturing line, into the manufacturing line itself, and being able to feed the machinery within the line. The camera in the front is the camera that you can see in the front. The camera in the front is the camera that you've used by our perception engine. So we saw some applications of visual perception and potential for advanced robotics. But there are also applications on human-operated machines. Every type of machine can become smarter with physical AI intelligence. By combining the algo box with any type of machine, could be a forklift, a tractor, a construction machinery, mining machinery, any type of machine. And by aggregating data from location, speed, time, obstacles, semantics, and map information. In algo, we generate the spatial information that can be used for different applications. Some of the use cases can make the machine itself more intelligent, providing information at the machine level. For example, driver assistance guidance, collision avoidance, route planning, enhanced productivity, and quick onboarding. Or site level intelligence, making the site more efficient, showing real-time visibility to all the machines, giving alerts and notifications, smart orchestration of different machinery, could be different types of forklift, different types of machines, or robots together with human-operated machines. It can also enable goods tracking and track not only the forklift, but also what the forklift is carrying. And also fleet insights and analytics. Overall, the observability is the first step for intelligence. Once you have the observability into what's happening within the site, now smarter actions can happen at the site level. There are many different applications. You can see a few examples. Outdoor operation with the perception engine enabling machines to operate outdoors reliably in different types of environments, with or without GPS, and also forklift tracking applications. In this example, you can see the simple installation of the Argo Robotics box on a forklift. Just adding cameras to a regular forklift, very simple installation, making the forklift trackable, and enabling the site to have all the insights on the exact position and exact operations of all the forklift within the specific warehouse. You can see in real-time the operation, and you can also have queries checking the history of what's happened. So for example, I'll configure a heat map for a certain period of time. And then you can see exactly the congested area and what happened during this case, the morning in that site. Observability is the first step for smart insight because with that information, advanced AI can provide smarter guidance and maybe change the routes or change the location of goods within the warehouse to improve productivity. So we've seen some of the applications done by perception in different use cases. Visibility and fleet analytics, of course, but there are also applications like speed zones and no access zones, and controlling better the operations, tracking of goods that I mentioned, and also providing some smarter insights. We'll see a little bit more about that in a few slides. So we talked about what can be done. Let's dive a little bit deeper into the physical AI information platform. So as I said initially, the physical AI information platform gets the information from the different entities and return information that could be used by the different entities to operate better. But it can also provide intelligent automation at the site level. It can provide observability and analytics, and it can generate a digital twin. The real world representation of the physical world in the digital world that could be used for different simulations and tryouts of different methods within the warehouse. With all of that, insights and smart decisions can be made. These operations are leveraging some of NVIDIA's newest tools such as the NVIDIA Metropolis, the NVIDIA NIMS for generative AI, and the NVIDIA Omniverse for generating the digital twin. So how does that work? The physical AI information layer includes data that is collected in the 3D global map. The 3D global map gets perception data, 3D information from applications such as NVIDIA blocks that creates a coherent and up-to-date 3D representation of the world. It includes additional perception data such as semantic that is added on top of the same map, and the context of everything that happens. That information gets into the physical information platform and can be used by AI-based queries. So using AI or generative AI interface to make small decisions and smart questions on that data. The same data can be ported to digital twin using the NVIDIA Omniverse. And over that digital twin, we can have real to seeing. So taking the real world, converting that into the digital twin, and then operating different commissioning over that data. So trying different methods. Think about trying to implement different number of robots in a certain site, seeing what is the best approach for that, and checking all of that, not in the costly manufacturing environment, but in the digital twin. Being able to run 10 times or 100 times more simulation of different cases until the site can choose what is the best configuration. All of that can apply virtual commissioning. So testing the units in the digital twin, and then applying that back to the real world. With site commissioning tool, that can get back to the fleet management system or the warehouse management system, and apply the actual route and instruction for all the different machines. So as I promised initially, I want to show you a little bit how that can solve real world applications. So the intelligent 3D global map takes the classic 2D map that is currently used. So most sites are using just a simple 2D map generated by a laser scanner. The 2D map is two dimensional only, right? It sees the height and the obstacle at one given height. It takes a long manual process to generate. It requires on-site engineers, and usually some experts, because it's not that easy to generate it. And then it's prone to errors, and it is static. Any significant change in the site requires remapping and recreation of the map, or at least a manual walk to improve that. So that's the common way to do that today. And that's a real bottleneck in robotics, because as we've been hearing from many robotic companies, one of the biggest bottlenecks is the ability to scale and their ability to deploy in many more sites simultaneously. When the installation itself has to be done manually, and you need certain experts to be able to do that, scaling is a real challenge. So we're targeting exactly that problem, enabling faster commissioning with an autonomous process. So we are automating automation. We're automating the process in which robots can be entered. We do that with the intelligent 3D map. The intelligent 3D map has a simple 2D map representation, like the regular 2D map that everyone knows. But the underlying data is actually in 3D. All the information, all the structure of the warehouse is generated. On top of that, we add semantics. So understanding of the different types of objects, being able to classify each and every one of them. You can see here an example for a forklift, the fork of the forklift, the box, different carts, and many other things. We can also add signs to that representation. So the intelligent map really has a deep understanding of the site itself. It includes rich 3D and semantic information. It is automatically generated. So brownfield applications coming to an existing site and generating that map from scratch. And it is continuously updated, which is very important again, because sites tend to be changed over time. And remapping is as heavy burden on the on the companies. There's no need for any special skills. So we were able to lower the complexity of the problem to something everyone can do. You don't have to be an expert in robotics to be able to do site commissioning anymore. OK, so together, Argo with NVIDIA, ISAC, and NB blocks software are enabling a faster deployment, faster commissioning, and automating automation. I want to show you an example of how it looks in the real world. So this is a real warehouse that we've mapped. What you're seeing on the left side is the 3D map that we generated by running one time in that site. At the center, you can see the 2D map that was generated by that. And then we have a very basic tool that enables seamless commissioning with very simple operation and something that everyone can do. So you can see this map. The map is a three dimensional map. You can see it when I rotate it and you can see it from different angles. The map includes not only 3D information, but also all the objects that we've identified. So we've selectively selected a movable object. And now we can automatically generate the free space with the click of a button. We understand exactly the area in which the robot can operate. And we can move and switch between the different objects we identified as dynamic objects and mark them as free space. You can see this card. It's free space. It's not really blocked and it will be moved probably soon. Here's another card. We can see where it is. And it is likely to be moved so we can mark it as free space. Very simple process, quick and can be done by anyone. In this case, this shelving system is probably static, so it has to be marked as blocked. With a click of a button, that all translates back to the 2D map and makes a 2D map that the robot can navigate it without the need for any experts. Now we can look at signs. So this map already includes signs. I approved signs 1 to 3. And you can see here some additional signs that we can decide if we want to add to our system. These signs are important because people like to see signs and understand which eyes they have. And now we have an automatically set map that already understand which eyes is which. As you can see in this map, the eyes are already numbered based on the real world information that we've seen. And if someone will change the numbers of the eyes, we'll automatically see that and change it. All of that then can be transformed to a map and just published with a click of a button without the need for an expert. Okay, all of that shows solving a real world problem by using the advanced AI capabilities that the NVIDIA tools together with the Algo Perception Engine can enable. So we talked about the problem of commissioning. Another known problem is the digital twin. The concept of having a digital representation of a manufacturing line of a warehouse is exciting. It enables running different types of simulations and different types of actions in the digital world without paying the cost of doing that in the actual site. And then applying them right back to the warehouse. The main problem today is that the digital twin, if generated, is generated usually based on CAD data, on information that is at the beginning at best true for the initial setup of the of the warehouse. But over time, warehouses and manufacturing lines tend to change. And there wasn't a way to keep the digital twin up to date. So at some point, most companies abandoned the digital twin and go back to the classic way of doing the testing in the actual site. What we're showing here is a full reel to seam seam to reel application. So on the left, you can see a robot. That robot is running in the real world. That's a small, that's a video showing the robot in the actual site. On the right hand side, you can see the virtual warehouse. So it's a 3D representation of the warehouse that we've integrated into Omniverse. So Omniverse showing the real warehouse in the digital world. And the simulation is one to one correct with the warehouse itself. At the top, you can see the Algo control center showing the operation and installation of a robot in that warehouse. It running in simulation based on the exact data that was generated in the real world. So let's see that in action. Again, on the left, you see the real world with a real camera. On the right side, you can see the Omniverse digital representation of the real world. And on the top, the installation of the perception engine tool showing the machine running in that space. You can see how the Omniverse representation, how similar it is to the real world, how natural and real it looks. Allowing different types of simulation and different types of application to run different simulations and testing in that digital environment. So let's see a bit where it can take us and what's the next step when we are thinking about using AI for robotics and automation. One of the challenges that most sites have is that it's hard for them to understand what's happening in real time. For that reason, we've implemented the Insight Engine. The Insight Engine uses that information layer, the physical AI information layer, in order to enable an easy to use mechanism for the site manager to have real world insights, actionable insights on what's happening within their site. What's happening here is that we're using the data that we have to generate a context information, including 3D data, mapping data, location and context, and visual information that we collect all together and we feed all of that into generative AI, LLMs or LVMs. The end user can make real world queries with natural language, whatever question they want, using that tool on their specific site and on the specific information that they see in their site. Now, on the left, you see the map of the site. And now I can ask a simple question. Show me images with unsafely stacked objects. Okay. Typical query for a warehouse. On the left, you can see all the places in which the Insight Engine has identified as cases of unsafely stacked objects. You can move between all of them and see all these cases. All right. The information that you get here also adds an additional response explaining the exact reasoning for each one of them. For example, here, there's a stack of items on pallet wrapped in plastic. Yes, it's unsafely stacked objects. That makes sense. Okay. All that data is connected to the real world positioning. So the exact position and exact angle in which these objects are. So the site owner can take and make decisions based on that. Now we've taken it a step further and we actually implemented some actions based on that. You can use that information in order to make smarter decisions for path planning, for sending missions to different machines or different robots. With this tool, we can select these specific points and add them as specific rules for path planning. So for example, we've asked it to find images where IELTS becomes narrower. Okay. Sending the query and you get the list of all points in which we've detected the IELTS that are narrower. Now we can skip between these different places and we can choose which ones are relevant for us. And we mark them with a check mark. Okay. That one looks correct. Let's see this one. Yep. That's another case of an narrower IELTS. How about this one? Let's choose. Yeah. Maybe another one. Let's see a few examples of where the IELTS are becoming narrower. Now we can go to the location rule and define these as low speed zone. Okay. That can automatically translate into commands saying that within these areas, the speed of the forklift or a robot should be limited. Same can apply for IELTS that are blocked. So here I'm asking for blocked IELTS. Okay. You can see some of the cases. Some of these could be the same as the error aisles because it's not always clear when it's wide enough. I can again mark these points. And then define them as a location rule avoid. And then these points will be points in which we want to avoid them in our task planning or our task management system. That example shows how we can use the information layer for physical AI, apply that with physical AI and provide real world insights that can really change the operation within an existing warehouse, making it smarter and enabling it to make smarter decisions in semi-autonomous way by choosing the cases in which we want to automate. So to summarize, we've shown that visual perception is a critical enabler for physical AI. It's an enabler for robotics and automation for all machines. We've shown that adaptive, flexible and affordable visual perception is fundamental for generating physical AI information. Physical AI information is really the basic on which physical AI can operate. Using NVIDIA tools together with Argo Perception Engine provides a foundation layer for physical AI. And we also showed that it provides immediate proven value that can be used already today. It can be used for simplifying commissioning. It can be used for improving safety, reducing cost, of course, and also improving productivity at the site level, even in existing sites. I hope you enjoyed this session. I'm very happy to get additional questions and to discuss this further with you. You're also welcome to look at our website and contact us at argorobotics.ai. Thank you.