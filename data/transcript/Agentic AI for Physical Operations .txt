 Hello, everybody. We'll be talking about agentic AI for physical operations. I'm also from the Metropolis team, and I can see a larger Metropolis team in the crowd who is actually responsible for building this. Second. The agenda today will talk about what is Metropolis doing with automating facilities, the modern warehouse challenge, Metropolis spatial AI agents, how are you building these Metropolis spatial AI agents, DataFlywheel, which is a critical part of any agentic AI solution, and call to action. So, Metropolis agents enable automated facilities. We are building a diversity of visually perceptive and interactive AI agents. So, as we all know, the manufacturing industry is huge with 10 million factories and nearly half a billion employees as part of a 50 trillion manufacturing industry. There's an opportunity to use physical AI to automate robots, self-driving cars, and now industrial spaces like factories and warehouses to perceive, understand, and perform complex actions in real physical world. Agentic AI systems operate autonomously, executing tasks with minimal human supervision. In industrial settings, visual AI agents work at three levels. We start with the, at the mini level, what we call it. AI agents help find and classify defects to improve product quality. At the micro level, we have agents train workers, act as co-pilots, and automate checks in standard operating procedures. At the macro level, this is where we think physical AI is used to transform visual AI agents into facilities operational agents for the facility as a whole, for warehouse as a whole, for example. This is, Metropolis provides the outside-in view, where an AI agent helps with warehouse wide challenges, example, worker safety and space utilization. So, physical operations automation is still challenging. Let's talk about the modern warehouse challenge. Complex environments, we don't just have humans and inventory, we have dynamic inventory, robots to manage, humanoids and workers. What we need is real-time awareness, spatial and contextual understanding of the space. Space utilization still has inefficiencies and leads to underutilized capacity. We need semantic understanding of a 3D space, object relationships and so on. And the last one is the management systems gap, with inefficient processes, safety and zone violations. We think that a real-time spatial integration, spatial AI integration, will help with the flow of human and robots and dynamic inventory into the warehouse. Moving on. Metropolis spatial AI agents and how do we automate operations. So, the critical part of a spatial AI solution is a physical AI. And that's what you see at the top there. This is enabled by our solutions with NVIDIA, Omniverse and Cosmos platform. And there were many announcements at GTC and many sessions on the physical AI solutions. And what we do, the spatial AI agents, we bring in a lot of Metropolis tech, which is real-time 3D spatial AI, multi-sensor perception and fusion, which perceive, understand and reason about 3D environments. We analyze objects, faces, geometry and spatial relationships in real-time, improving operational efficiency, safe space utilization and safety. And since it is an agentic AI solution, we also have GenAI integrations for Q&A, real-time Q&A, and generations like report generation. So, all this is powered by 3D perception and understanding. And these are some visuals created by the brilliant Metropolis teams. We incorporate bird's eye view, models, AI, multi-view camera, and 3D bounding boxes and trajectories, distance measurements. So, the idea is Metropolis agents are tracking a 3D or a space in real-time, all the time. This is another example which is generated by the agent. Two examples. The one on the left talks about spatial understanding to improve space utilization. It's showing the last step of the agent where the agent recognizes the free spaces where a pallet can fit. And we'll walk through some examples later in the presentation. The one on the right is showing safety bubbles around humans and humanoids. You can also see an agility humanoid there if you have seen the agility digit booth. So, dynamic safety bubble reduces safety accidents and it creates alerts. And this is an example of an agent-generated dashboard which enables enabling AI-driven decisions. It has space utilization in a warehouse, buffer zone utilization, and also safety violation alerts. Safety, we do look at both proximity violation between human and humanoids, robots and humans, and also zone violations. All this is built using the Metropolis stack. We call it the Metropolis spatial understanding and reasoning stack. It's built with Metropolis 3D NIMS, blueprints, and agents. For agents, we use the agent IQ library that was released earlier in the week at GTC. And blueprints. You've heard a lot about blueprints and the new reasoning LLMs and reasoning models that were released as well or announced. Okay. So, I have a video here which walks through the end-to-end just explaining what I mostly just talked about. And showing some examples of the agentic trace and agentic Q&A and reasoning steps. This is also available at the booth. We have a B10 booth that is referenced at the end of the slides. And you can possibly go to the booth and see the trace much more clearly. There's no voice to this, but it's just saying that Metropolis visual AI agents or spatial AI agents are tracking the warehouse all the time. So, that's a dashboard. That's a dashboard. And this is the agent in action. And I'll be talking more about the types of agents we use to provide an accurate estimation or monitor a warehouse space. So, we've done through four queries in the video. Two are related to spatial utilization. One using a spatial aware 3D VLM. One to provide historical analytics and the other one for safety violations in a warehouse. And this one is for the safety violations. The agent at the end uses a visualization tool to spit out a visualization on visual image overlaying with the objects and so on. Let's move on to the next slide. Next slide. So, this is a section where we deep dive into some of these agents, AI agents. They're all 3D agents for various operations. Let's see. So, we all know agent-a-KI is a four-step process for problem solving. Perceive, reason, act, and learn. Perceive. When you want to perceive a space, you need 3D perception and tracking in real time, all the time. And this is powered by our Metropolis 3D LIMS, MultiView, Metropolis Microservices, and Metropolis Blueprints. And reasoning. For reasoning, we have the agent's reason. We also have an orchestrator, which I'll show later in the slides. We use the regular instruct models, LAMA models, the Nemo Tran reasoning model. We use the agent router, and, yeah, GenAI LLM instructs for tool calling. Act is enabled by tool calling or function calling, and we use quite a few tools here. And this is where the whole agent collaboration is most seen. In the sense, we use the USD search to talk to warehouse dimensions, right? This, the whole agents were built using simulation in a warehouse digital twin. So, USD search gives you every possible object size and dimensions of buffer zones and so on. So, which an agent can process to work on or reason about. And then segmentation. We use the SAM2, and I'll go a bit deeper later. And we have a video storage toolkit integration, which is a microservice which deals with real-time videos and images of a space. And the data flywheel, which we'll dive into a little bit later, which synthetic data generation is a huge part of it. And fine-tuning and eval. Okay, this clicker. Okay. So, the way we have developed this, it's a mixture of 3D agents, right? We have two, this shows four agents solving all the types of queries that I just mentioned related to safety and space. But we categorize them into two buckets. The top one is called spatial awareness, and the later one is called spatial estimation. The difference is, for the spatial awareness agent, we use a spatially aware AI model, which in this case is spatial RGBT. And it can answer queries like what is the distance between leftmost palette and the rightmost buffer zone, or what is the distance between a humanoid and human at this particular time, and so on. Right? The space estimation agent, safety agent, and metrics agent, they're all based on 3D CV, real-time detection and tracking, and an analytic solution. So, what these agents do is, for example, the space estimation. Where can I fit another palette? It has all the data. It is tracking in real-time where there is space. It calls the respective agents to analyze and provide the result. And also provides an overlay visualizer image to show you where there is free space. Safety agent, again, the same thing. It depends on our real-time detections, tracking, and spatial analytics. Same goes with metrics agent. Let's talk about the architecture a little bit. So, this shows two blocks, again, of those agents. We call the top one, we call the 3D spatial awareness agent, which uses, as you can see, four models and a tool to actually create a prompt for the 3D VLM. Right? That's the power of the agentic AI pattern that you can use, augment an LLM with more spatial information. The 3D spatial analytics agent is actually deriving its data from the lowermost block there, which is our real-time CV detection and tracking pipeline. And we use PARSE 4D there, and I'm going to talk about that a little bit ahead in the slides. Let me move to the next. Okay. Skipped a slide. Sorry. This thing is... Oh, God. This is... Need some help. Can't move the... I skipped a slide. Yes. Thank you. So, let's talk about the 3D spatial awareness agent, which uses the 3D VLM for a bit. Right? This diagram actually shows it's a flow chart. It uses a lot of other models for accuracy, right? Or accuracy to augment the spatial RGBT model. It talks to... It uses SAM2 to get the segmentation masks. It uses the VLM prompt generation. It's its own tool, which actually uses the segmented object identifiers and the user query to create a prompt in a format that spatial RGBT understands. Right? And all this is possible because of the reasoning and the agentic architecture. And the visualization tool is there to actually create the visual for the user. This is a flow chart for a 3D CV agent. The same thing. But the 3D CV agent, it is tracking the place or the space, a warehouse space, all the time and detecting objects. And we have three agents. It supports three agents with all the data it's analyzing and the analytics. So, again, the sample query is where can I fit another pallet? Or how many pallets can I fit in a particular buffer zone? And so on. You can see some examples running at the booth. So this, as you can see, USG search tool is again common between all the agents. They do rely on USG search to get some of the warehouse or static dimensions of the space. The analytics API tool is actually querying our analytics database to get the relevant metrics to compute further. And then in the safety agent, again, safety agent is also tracked. For example, right, if you can see in the booth, we have an example of the analytics engine, the Sparse 4D, and the analytics engine tracking and detecting humans and the agility humanoids all the time. And they can actually show you the distance or the maintained distance between every other human and humanoids to generate alerts, safety alerts. Next slide. So we connect all these 3D Spatial AI agents with an orchestrator agent. And the reason why we need an orchestrator agent is because, again, like I mentioned, we have a spatial awareness model, and then we have the spatial analytics solution, which is powered by Sparse 4D. We, in all these spatial reasoning or task planning that we have, not both agents are capable of answering all the questions. Right? The spatial, the SRGPT is good at talking about spatial distances, relationship between objects, and the orchestrator is fed some descriptions and few short examples to route the queries to any particular agent that is capable of doing it. In the future, you could think about this orchestrator agent that can advance to maybe routing the query to both agents and using a reward model to score them, and then picking an answer for the operator. So the agents that we just talked about, this is just an example to show what kind of models are used and also the reasoning models. The orchestrator reasoning and tool calling uses the LAMA 3.370B instruct, and the recently announced, this is either one of them is needed. Basically, the recently, at GTC was the 49B, which seems to be very promising in terms of latency and accuracy. 3D-CV agent is based on Sparse 4D. It's continuously, this is a BEV model, which is continuously detecting and tracking in multi-view. And the 3D-VLM agent requires an ensemble of all these models for a spatially accurate answer. Depth anything, which generates, does the depth estimation for spatial RGBT, and SAM2, which feeds the segmentation mask. This is our, it's a view on how these spatial AI agents are integrated into our end-to-end physical AI solution that was, or a blueprint, that was announced at GTC at the keynote as well. It's called the Mega Blueprint, Mega and Omniverse for physical AI, for robotic, for actually training robotic fleets. So what you see here is agents, again, the same pattern that we were just showing, but the same architecture diagram. But in this case, it is the real-time simulation streams are coming from the Mega Blueprint, which is a robotic simulation system for training robots. The rest of it remains the same. The agents talk to our analytics database or the NIMS at the top, which is the 3D, VLM and 3D, CV NIMS, and the generative AI models. Data flywheel. Data flywheel. Data flywheel is a very critical piece of any agent or AI flywheel, or you call this agent flywheel. Let me see. And you need a data flywheel because agents need to continuously learn and adapt if they are going to be autonomous. They need to learn. They need to improve. And this slide actually shows the NVIDIA Omniverse and Cosmos fine-tuning workflow. What you see here is a simulation platform based on NVIDIA Omniverse, a digital twin, which also provides a digital twin, or you can generate synthetic data from it. There is Cosmos platform. These are services. Cosmos predict and transfer. And then you're looking at the NIMO microservices for fine-tuning. That is the NVIDIA NIMO curator running on DJX Cloud and also the NIMO fine-tuning microservice. So in this data flywheel, the BLMs, fine-tuned BLMs, are later plugged into the Metropolis Spatial AI agent blueprint or workflow or whatever you want to call it. This is a zoomed-in version of the simulation or synthetic data generation block that you saw earlier. This uses the IsaacSim replicator object and replicator agent for physical simulation. And this is a three-stop process. Scene creation, data generation, data outputs. For example, our team recently used this to actually add humanoids to the warehouse scene, specifically the agility humanoids. And a lot of this data is available for, for example, the USD search to understand the space for the agents. This is a snapshot from our team who trained these models. For the 3D-CV agent, it's Sparse 4D and 3D-VLM agent, Spatial RGBD. It's just a snapshot that this is an ongoing process. The Sparse 4D, as the team tells me, that they need to do another round of larger data set. They're collecting a larger data set to pre-train for further improvements for accuracy. So this is an evaluation workflow. An agent evaluation has two types, final response and trajectory evaluations. And this is a workflow that shows you how you can do this with the AgentIQ library that was recently announced. And this whole Spatial AI agents use the AgentIQ library and tooling. So what you see here is again a multimodal eval data set, which we have started building. We have a smaller version of the data set, but we are trying to build it to cover a lot more variations of Spatial AI, for Spatial AI. And the evaluation and scoring is done by the AgentIQ evaluator. The AgentIQ framework also provides you with telemetry information and the output response. One thing we're doing is we're trying to get annotated data sets out of the annotated output responses with the human in the loop, which doesn't exist today, but that's where we want to be. And what you get from this is an evaluation score. This is a snapshot of the observability platform with OpenTelemetry. This is a Phoenix dashboard, which comes with AgentIQ. This is just a screenshot of a 3D VLM breakdown. As you can see, it uses a lot of tools. And the way to think about the evolution of this is as we get... This is a 3D VLM, right? As we get better, as the 3D VLMs get better, we will be dropping some tools because the VLM will probably not need that augmented additional info for higher accuracy. So that's the beauty of breaking down this problem solving into very small steps with many tools, which you can possibly iterate on as the AI gets better. Right? That's the advantage of this architecture. Call to action. Try the three components of spatial AI agents. We have blueprints. The one VSS blueprint, which is our video search and summarization. It's available on build.nvidia.com to build video analytics AI agents. The mega blueprint, this is again the robotics simulation blueprint, which I just talked about, to build digital twin simulations and training your AI agents. Right? Training your... We call them brains, the robot brains, which are nothing but the 3D real-time detection and tracking, and also training your AI agents. Agent IQ library to build the multi-agentic system. The value of using Agent IQ library is getting all those insights, integrated observability. And one thing that we could not, we ran out of time to, we didn't, we could not add it into the slides, was the Agent IQ profiler, which, which if you went to the Agent IQ talk, or catch that talk later, you will see that you can get profiling data for all your agents. The completion token, the generation token, and how to optimize it. So that profile data, which Agent IQ gives, you can use that to optimize your tool usage or try new LLMs, reasoning models, and so on. It makes your development and iterations faster. To learn more, these are some featured talks. And there is a... The first one is, of course, this talk. Then the next one is the Agent IQ talk. There is a... There was a talk on video VSS, the Video Understanding and Summarization Blueprint, and many others. And our booth is at B11-011, and it's right next to that agility humanoid. So, yeah, people are crowding to see the agility human right beside... humanoid right beside us. That is all I had. And this is a list of another featured talks for Computer Vision, Metropolis, Video Analytics, and Partner Talks. I think we're good on time. We'll get Q&A. Thank you very much. Thank you. Thank you. This is Rupa. Everybody, thank you so much. We now have time for a couple of questions. We have some folks running around with microphones. I have one. You have a question right here. Let's start right here. Hey, just on this flywheel agent thing, what happens if you upgrade the model? Like, do you end up with a problem of having to start completely from scratch if you change models somewhere down the line, or can you carry the data over? You can carry the data over, and that's why the evaluation... I think having that evaluation data to get an evaluation score so that you can iterate on that process is very critical. Yeah. Does that help? Okay. All right. We have other questions here. Hi. I had a question regarding the real-time aspect of things, because a lot of the models that were being used are, like, inferenced over time. Like, for example, SAM is, like, inferenced over a set of, like, images over time, and so is, like, the summarization aspect of things as well. So, like, when you say real-time, are you basically making changes to the architecture of these models to be more real-time? Or are we talking about, like, inferencing in, like, 30 minutes later, you have data? Yeah. So, it's a mixture of agents, and some are real-time, acting in real-time. SAM2 is not in this picture. The whole 3D VLM agent pipeline is not real-time. They are all using on-demand. But the advantage of having the real-time pipeline, the way it works is, which I probably missed explaining, is when the operator queries for a particular buffer or a spatial reasoning question, right, you... It contacts a real-time system to capture an image, to get an image, sends it to SAM2, and depth estimation, and so on. So, it's all based on an image, on-demand image. But that's... We'll be improving that as well. We'll slowly move to videos, and so on. Does that answer? I might have missed. Yeah. Okay. Further questions? Just raise your hand if you have anything. Oh, we have one back there? Oh, upstairs. Yeah. Does anyone have questions upstairs? I see one waving, but I don't know if I have a microphone up there. They do. After, we do. Perfect. Oh, no, I was just waiting. Like, we're up here. There's a whole Metropolis crew sitting there, which they can answer. Okay. Okay. Okay. Well, I guess with that, we are going to close the section. Many thanks to Rupa for her wonderful presentation. Thank you. Thank you.