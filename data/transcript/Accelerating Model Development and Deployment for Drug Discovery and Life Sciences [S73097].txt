 Thank you very much for being here bright and early. I really appreciate everyone's attendance and I hope to give you some exciting updates on the things that we're doing in the digital biology space on the product side and also talk a lot about our important partners who make all of this possible as well. So I wanted to start with a slide that you may have seen in Kimberly Powell's, our VP and GM of healthcare's talk on Tuesday afternoon. And the reason I really want to put this up here is that we started in healthcare and life sciences in areas where there were clear applications for us to verticalize what is otherwise a horizontal computing platform for healthcare. And so it's probably no surprise this started in areas like imaging, radiology. These are sort of now bread and butter canonical types of applications of artificial intelligence in healthcare. But as we have evolved over the last 10 years, we've found incredible applications for AI technology in all sorts of different areas. And today we're going to talk about two in particular on the product side, which you see here, lots of products here on the bottom, Bionemo and Parabricks. And these are products that we build in the digital biology team. But the reason I really wanted to put this up here is that the breadth of the NVIDIA platform and computing in general that is relevant in biology is getting broader and broader and broader and broader. And so actually, if you actually look at this, and I'm not going to be able to point all the way over there with the laser, but if you start from the right-hand side and you look at the inference acceleration platform in TensorRT, you'll see examples of this today where that was applied to AI models that we built in biology to get huge speed-ups, both for language models, which is where you probably typically think about things like TensorRT, but also in structure-based models and design for drugs. We base a lot of the work that we do in Bionemo, as an example, on the innovations that have come from the last seven or eight years in learning how to scale foundation models. And that's based on our NEMO platform, right, which is applied to large language models, applied to imaging, but we can inherit a lot of that core technology into what we do in the Bionemo space, AI for biology space. Rapids, GPMSA, these are examples of technologies that have applications that are very broad, but increasingly important in understanding how to do accelerating alignment, accelerating data processing for genomics. We'll talk a little bit about that, or in your experience actually using models like AlphaFold, a lot of people in the world doing that, and accelerating the data processing pipeline before you actually get to the model inference itself. So I won't go through all of the rest of these, but I think it's fair to say that there isn't a single box up here that we don't take advantage of in one way or another in the digital biology space, which is super exciting and incredibly fun. So I want to, this is another slide that you might have seen before, and I like this slide a lot because what it teaches us is how we started in generating the core ground truth information that we need to understand biology, as well as how we're taking advantage of that in a big way today with the types of AI models that we're building. So if you look down on the bottom left in the 2020s, or sorry, in the 2000s, you see our early work in genome sequencing, the revolution that was CRISPR, a lot of the work that we're now scaling up in single cell workflows and single cell data generation, hugely important applications for generating data for things like AlphaFold and cryo-EM and other ways of determining protein structure. And now in the future, the application and sort of the integration of all of these tools with AI to enable things like autonomous labs. So we have this really foundation in generating and interrogating the data that comes from our labs that is really fundamental to our ability to then take all of that information and build useful representations of biology that can make useful predictions. And so AlphaFold was the first of those models, and I hope everyone sticks around for the 10 o'clock talk because Max and Sergey from Isomorphic are going to be here and talk a lot about their past and current and future work, I'm sure. But then, you know, as we've moved into new domains, AlphaFold took advantage of a PDB database that had lots and lots of structures, but now what you're seeing is lots of models that integrate information that come from all of the different technologies that you see at the bottom of this panel. So it's really an exciting time, and I'll tell you about how we support this over the course of the next half an hour or so. So we think about this, and I'm putting this slide up mostly as an organizing principle for everyone in their mind. There are lots of variations on this theme of lab in the loop, but the fundamental piece here is we start with we're going to generate a lot of data. You're going to see a lot of people generating exponentially more data, and this is not getting less of importance simply because we have been doing this for a long time. It's actually becoming more important, both in the real-world setting and even in the synthetic data setting. We then train models that integrate information from a variety of different sources and modalities. There's technology that we need to build there to make that fast and easy and performant. We then can use those models to make useful predictions, either as silver standard in silico predictions, or in some cases to even generate synthetic silver standard data. And we usually take all of these types of models that we build, and we string them together into entire workflows. And so how do we optimize something that's end-to-end that takes advantage of a lot of different models that need to talk to each other in some way? And then once you've done that, you've made some useful prediction. That'll drive either your decision-making on what the next lab experiment is that you do, or you'll take that information and get gold standard labels from some lab experiment, whatever it is that you're measuring, and feed that back into your training loop. So this is really a stage-setting kind of slide to just give you a sense of, like, almost everything we're going to talk about fits into this flywheel. And I think it's reasonable to say that you can fit a lot of the development that's happened in the space across applications in AI for biology into this paradigm. So first, I'm going to talk to you a little bit about updates that we have in our video platform for genomics. And so this is a platform, an extremely mature platform at NVIDIA. We're very lucky to have committed to this and built it for a number of years here. And our goal has been to, you know, accelerate high-throughput analysis in genomics data science. And we've done that through a technology called Parabricks. And so this has been the core of what we've done in secondary analysis as an example to accelerate all of the work that people do in genomics data science. And increasingly what we're seeing now is as we generate all of this data, as we're able to process all of this data and make it useful, the connection between the generation of the data itself and AI technologies that we attach to that data is becoming more and more prevalent. So if we look at our genomics ecosystem, we work across all of these goals across our entire ecosystem. So we are in every single genomics instrument platform and work very closely with those partners. We are in all of the ISPs and cloud platforms that you see here, and this is not certainly an exhaustive list. And then we partner very closely with industry and with research organizations that are consumers of the technologies that we work with our partners to deploy. So one really incredible example of this and an example of how the work with, for example, our instrument partners continue to accelerate is work that was announced, and this was in Kimberly Powell's keynote as well, on the SBX platform from Roche. And so this is really amazing. I mean, if you get a chance, I couldn't put a video on this, but if you go to YouTube and just type this in and actually look at the videos of how this technology works, the first time I saw it, it's really mind-blowing. But we've really taken what was an incredibly expensive activity in gene sequencing and reduced that into something that can be done, you know, many genomes in an hour at reasonable coverage. And so this is the next frontier in the types of data that we can generate in genomic sequencing. And our partnership with Roche has integrated work that we're doing in Parabricks, as well as other accelerations like TensorRIT, into the platform to enable this to work quite as fast as it does. So the Parabricks platform or the genomics platform for Omics, and I'm going to not restrict ourselves to Parabricks, and that will be obvious, I think, in just a moment, is our GPU-accelerated software suite for Omics overall. And so if you know the platform well, you will have seen lots of work that we've done in secondary analysis. This is our bread and butter here. We have incredibly fast alignment. You'll see on the left-hand slide, again, I'm indicating as if I have a pointer when I don't, but on the left-hand side, we've committed a lot of resources in this most recent release to make a lot of the aligners much, much faster. We have incredible variant calling technologies in there as well, and you'll see how the interaction of those two we've put together into Blueprints, which we'll talk about in a little bit. And then we're, importantly, and as I mentioned before, engaging with a much broader set of technologies in NVIDIA, to enable the AI plus genomics revolution that we're seeing happen as a consequence of the incredible data and data processing capabilities of the industry as it matures. So in Parabix 4.5, like I said, there are three main areas that we focused on. I think you can, on the left-hand side here, just have a number of items. I won't go through all of these in detail, but focusing on core performance improvements for the technologies that we already have and extending those in the middle here to our Blackwell platform. So I'll show you a little bit about there, both faster and cheaper on Blackwell for Parabix for many workflows. And then I'll give you a little preview here of additional acceleration that we get for a workflow involving both giraffe and deep variant. So this is an example of work that we're doing with our next generation of Blackwell GPUs, our RTX series of Blackwell GPUs, so-called RTX Pro 6000. And so I show here two different vignettes of the acceleration that we're getting. So in the middle, you see the performance that we get versus L40s, our last generation of these GPUs, for a Smith and Waterman alignment. And then on the right-hand side, you see a full germline workflow, which in our case is 135 times faster now on a 4x RTX Pro system compared to a 96-wide CPU instance. And we get incremental improvements over the previous generation of this platform as well. If we look also at the work that we're doing in giraffe and deep variant, we see compared to baseline on our compared to L4 and L40s, a six times faster runtime on this platform for variant calling and alignment. So great, great improvements to core functionality in Parabricks that we released recently and continue to invest in accelerating. I think this is super cool. And this will lead us into a conversation around the work that we're doing in single-cell genomics. But there's a library that we have that is in collaboration, actually developed by SCVerse, called Rapid Single Cell. And it's an incredible library, huge speed-ups. If you're familiar with Rapids, which has been our platform for accelerating data science in the PyData ecosystem for quite a long time. So you may be familiar with libraries like QML and QDF, which are drop-in replacements for things like Pandas and Scikit-learn. So Rapid Single Cell takes advantage of a lot of what we've learned in doing that to extend and verticalize those for common types of workflows in single-cell biology. And so these are GPU-optimized versions of what's available in ScanPy. And so the figure on the left just gives you incredible speed-up numbers, like UMAPs of almost 700 times faster, nearly two orders of magnitude faster PCA. And the figure here is actually an 11 million single-cell data set, not a 1 million. But it's an incredible figure to look at. So we see end-to-end performance runtimes, runtime performance of approaching 200x, as you see on the right-hand side, with much faster individual component acceleration that we've been working on for some time through the work that we do in Rapids. Okay. So a few things there on what we're doing in extending the Parabricks platform and some really important, in some cases, revolutionary performance improvements for the tools that we have and continuing to invest in new tools. One of the themes that I'd like to lean into for this talk is how the work that we're doing in genomics relates to the work that we're doing in all sorts of other areas in the company, especially as we're applying AI technology, classically applied to other areas, perhaps, of biology, into this space. And so we've had a lot of announcements around this, and I won't be able to get to all of them in this talk. But the intersection, the multimodality intersection of work that, for example, we might do in Moni, right, for imaging, for pathology, for high-content imaging, work that we're going to do in BioNemo, and that I'm going to lean into quite a lot in the next section of this slide. With AI models where we're taking the fundamentals of how we accelerate these transformer models, we're extending those into the uniqueness of the architectures that we need to process and understand biological data, and then we're integrating those learnings through embeddings, as an example, into a larger and more complete understanding of biology. And I mentioned before, you know, the work that we're doing in rapid single cell, which is really fundamental to processing the data that is required for putting this all together. Okay. So moving on from our platform from Genomics, I'm going to talk about a lot of work that we're doing, and this is platform from biology, is in our platform biology. So we're going to talk about BioNemo first and foremost to start with here. So this figure is intended to give you a sense of the entire platform that we have for AI and biology, and we call that BioNemo. BioNemo has been around for just about two years, and we've done some really remarkable things with this, and maybe I'll play this again for everybody so you can just see the cool animation. But there are three major components of this that are important for everyone to just get your head around. The first is that we have libraries and a training framework, right? And so these are accelerated libraries for the types of unique challenges that we have in biological data. So you'll see a couple of examples of that, certainly not exhaustive, as I move through the talk. Those are used within a training framework, which is our perspective on how you take advantage of NVIDIA's acceleration. A lot of work that we've learned over the last seven or eight years, as I mentioned earlier, on how you take these types of transformer models and increasingly novel and more exotic architectures that are particularly useful for problems in biology and train AI models there. Then we have, from there, an efficient way to deploy those models. And so we are very much a strong member of our NVIDIA inference microservice ecosystem. We have launched many, many, many NIMS across a variety of different types of use cases. Some of those are the types of models that you can train in the framework, which today are primarily sequence-based models. But we also have NIMS across protein structure prediction, protein generation, design workflows, multimer or complex workflows, pose prediction, things like diff doc. And I'll tell you a bit about how our libraries and our framework teach us how we can make that entire NIMS ecosystem much better, faster, and more performant. And then as we move over to blueprints on the right-hand side, individual NIMS can be useful. They can be useful at scale. They're often very useful when you're generating lots of data from these NIMS. But real-world workflows tend to integrate lots of different models and different technologies together. And so we integrate all of these workflows into blueprints, which we make available. And we make those available in lots of different ways. I'll tell you about that in a little bit. And we're now doing this across everything from genomics to chemistry to biology and even some work with some partners that really extend this into reasoning and language model type of operations, which I'll mention in just a moment. So what is Bionemo framework? Bionemo framework extends and improves on a legacy of training very large-scale models. So how do you take an AI model? How do you break that up across a large cluster? How do you then how do you train that model? And what are the specific things that we need to do for biological, chemical, genomic data that are different from how we do things in, for example, the large language or imaging space? And so we can train models up to many billion parameter scale. We recently released work with the ARC Institute, EVO2, which is a novel architecture that extends up to 40 billion parameters for an AI model, which is a very large model in biology. We build custom architectures bespoke for biological applications. A lot of the types of things that are required there include support for long sequence and reasoning along very, very long sequences. So very large input context is one of the things, certainly not the only thing that is important and is unique about the reasoning over biological data. We approach this in a modular fashion, which means that we recognize that this is a very fast-moving field. we know that we need to find things that generalize so that we can accelerate everyone who is building these types of models and not just commit to a specific model architecture. And so we try to find, every time we figure out how to make something faster, we try to figure out how to expose that at the right level so that everyone building models can take advantage of that. And I'll show you a couple of examples of that in concert with how we deploy these tools, which we have many ways of doing, both open source, available in PyPy, available as containers and available as enterprise versions of this as well. We're also quite community-driven. This is an R&D field, and we work with many people in the room, many people I know and work with here that are here for the talk. And we get feedback from all of you that teach us what direction we need to go, what's going to be the most important problem that we can solve for you in the future. And part of that means that we're developing this in the open so that you can contribute, you can tell us exactly what we need to do next, and we can do that in a collaborative way. And we're gradually building working groups around some of the work that we're doing in the framework to make sure that we're meeting those needs formally in the future. Okay, so what does this actually mean? So let's take just a very straightforward example of a model I'm sure everyone knows, which is ESM2. On the left-hand side, you have the 650 million parameter variant. On the right-hand side, you have the 3 billion parameter variant. And what you see is quite nice and equivalent speedups across a range of model sizes for ESM2 on our A100 and H100 systems. So roughly a 1.7 or 1.8x speedup on A100 and over a 2x speedup compared to the standard implementation that you might get from the original GitHub implementation, the baseline that we interpret here. We also scale incredibly well. So the figure on the left is normalized to individual GPUs or nodes, and we get incredibly high-performance scaling, basically perfect scaling in Bionemo compared to what you might get from the standard implementation that you get from GitHub. And we also implement all sorts of improvements that allows you to go out to larger batch sizes. So they're really functional improvements, both in terms of performance and in terms of capabilities that you get when we engineer something on a really robust and repeatable platform like Bionemo framework, which is based on a legacy of many, many years of investment and understanding how to scale these models. We also then extend what we can do in the transformer space or in the AI model space for sequences. And so this is an example of an architecture, Stripe Taena. Stripe Taena was perhaps first used in Evo 1, a model that was released by the Arc Institute, last year, yeah, last year, and Evo 2, which we released together this year as a collaboration between our teams, which is extremely fun and taught us a lot about how we build new architectures into the Bionemo framework. And this is really important because these architectures are particularly useful for what we need to do in biology, which is reason, in this case genomics, which is reason across very, very long sequences. And you see here a number of performance benchmarks on A100, H100, and B200 where we're getting evolutionary performance improvements in the Bionemo framework, again, with extremely good scaling and really good performance as we move out to very, very long context length. So this was published together with a few authors in the room here today. Earlier this year, we did this jointly between the Arc Institute and ourselves in addition to a number of academic and industry collaborators that were also on the paper that we published. And it's available completely open source, right? And so the consequence for the community is now you have a really repeatable, robust platform in Bionemo. You can go and get Evo2, train checkpoints at 7 and 40 billion parameters, and you have a platform to train, adapt, fine-tune, and then eventually deploy these models. And we give you an example of that in a deployable NIM that contains the Evo model as well with an API that we've designed with the Arc Institute. So great new architectures on top of Bionemo that enable us to do things that we otherwise couldn't do before. Just briefly getting into the way that we think about the modules in Bionemo. These are two examples, and they are not exhaustive. But what we want to do is we want to find, like I said before, we want to find the common accelerations and pain points that we can contribute to for the community and make those accessible for people for developing models on top of. And so one of those is a package that we recently included in the Bionemo framework but was also independently installable. You can just pip install that, and that's why I had that little thing up on the top right pulled from the PyPy website, which allows you, it's an interpolate library, which is particularly useful for building and exploring, architecturally, diffusion and flow matching models in biology. So these are famous in the text-to-image generative space, but they're also incredibly important in sequence and structure design. You'll see a lot of models later in the talk by the time I finish here that are taking advantage of both flow matching and diffusion-types approaches for molecular design. So we support continuous and discrete models here, and so you can use this in the framework. You can also use this as an independent module for designing these. We also build incredible data loaders, and so this is a library we released. It's actually, we've been investing in this space for some time, similar to what you've seen in things like Rapid Single Cell, and so we have a library called Bionemo SCDL, which is particularly useful for loading single-cell data for the purpose of training AI models in Bionemo. And so, again, it's independently installable, and is a standalone, very memory-efficient library for doing this with basically a one-to-one map to the end data API and speedups of about 5x. So you can see, you know, example of how to do this. For those of you that are familiar with this, this will look very, very similar to what you've already done, but we've implemented this in Bionemo and sped it up considerably, which is important as you're trying to figure out how to feed these models as you're training, but then also as you're referring at large scale. So lots of tutorials to get started. We have a ton of documentation online. There are many ways to get access to these, which I'll show you in the next slide, but please go ahead and take a look online in our documentation, look at the tutorials, go to GitHub, take a look at what we're doing. You can see what our engineering team is doing in real time. That's how open and transparent we are about the framework that we're building, and please come talk to us and give us feedback and tell us what we need to do. to make this framework even better. So you can consume things in a variety of different ways. I mentioned the open source GitHub route, which is, as everyone expects it to be, developed completely in the open with a very permissive license, an Apache license. We also have an open source version that is packaged up on NGC. This is very convenient because we take care of the entire dependency stack for you, and you just launch the container and everything works. So highly recommended. There's no additional cost associated with this. You can go download it and use it. And then if you do need enterprise support for the framework, we do provide that, and you can get in touch with our sales team or come through us, and we can give you full stack and business critical support associated with the framework and the container that we built here. Okay. So second to last section here is, okay, so we have a framework for you to build these types of models. What do we do once we actually adapt models, and how do we take advantage of these models at large scale? And so we've been a part of what we're doing in the NVIDIA Inference Microservice Space since its inception. We're one of the most prolific publishers in this space, actually. And NVIDIA Inference Microservices really are the full stack solution so that you have an optimized model with all of those dependencies, all as a single container that you can download and deploy with an API that makes sense for that model, right? So we take lots of pains to optimize the models. So for example, in Evo 2 or for ESM or for a number of models you'll see later where I have very specific performance figures, we ensure that the model works, the model is as fast as NVIDIA can make it, and that we define an API and make it easy to deploy at scale and then integrate into your workflows. And we have examples of what those workflows look like and those are called blueprints. Okay. So just moving very quickly back into the genomics space, this is a new motion for genomics in developing NIMS and blueprints. We released our first two NIMS for genomics this week. The first is an alignment NIMS, a microservice that is easy to deploy for standard alignment, and we released a deep variant NIMS for variant calling. So these are downloadable NIMS. You can get them on our website. All of the same types of things that you've come to know and love about the NVIDIA inference microservice ecosystem applies here. And then we've also integrated these into our first blueprints for genomics analysis and for single-cell analysis. So the left-hand side, you see an example that integrates a lot of the technologies that I've already talked about here today for a single-cell that includes rapid single-cell into an entire NVIDIA blueprint. And we have a similar story on the right with Parabricks and deep variant for end-to-end genomics analysis. And the critical thing to keep in mind when you're thinking about these blueprints is blueprints for us are references. What they do is they teach you how to take all of these accelerated microservices and build them together into a useful workflow, but they're also flexible in the sense that you can take other containers, other technologies, and plug them into these workflows so that you can string together exactly the blueprint that makes sense for you. Very briefly, before I tell you about some of those examples that we worked on with a few partners, I just want to plug this. You have a couple of days left to scan these QR codes and get some free credits from our partner, Crusoe, where you can deploy the single-cell analysis blueprint and the genomics analysis blueprint and get access to some compute so that you can play around with the technology that we've built here. So I think that this ends, I guess, by March 22nd, and so please do, and there's no reason not to. Okay, so in the NIM ecosystem, we've also published a lot of things in structural biology, in small molecule design, and so here's where I'm going to get a little bit more technical and just tell you about some of the incredible things that we can do to make these models much more efficient and in some cases much more performant as well. So we've worked on models like AlphaFold, obviously very famous at this point, other models that are probably, you know, getting to that level of fame, things like DiffDoc, models that some people might know here like RF diffusion for protein structure generation and GenMole, which is the new model which I'll tell you about in a second. And the important thing is these are all available as NIMS, as NVIDIA inference microservices, but in every single case what we've done is we've taken that model architecture and we've learned how to make it way more efficient, right? One-to-one, you get exactly the same results, but for example, in AlphaFold, we have now a NIMS that is 5x faster for a single GPU inference than the standard implementation of AlphaFold that you might get publicly. In DiffDoc, we've done incredible work here and we've actually encapsulated this into a library called Coequivariants to accelerate all types of equivariant model architectures and we're pushing seven times faster in the DiffDoc version and there we've actually invented new data sets, trained next generation models and improved the accuracy and performance of these models as well. RF diffusion, we're about two times faster in that model. GenMole is a next generation model and so I won't compare it in quite the same way in terms of its performance, but there's an entire paper out. This is going to be featured at iClear later this year and is achieving state-of-the-art accuracy for fragment-based molecular design, which I'll mention in a second. Oops. Okay, we got it. So I want to just talk about three partner integrations that we're extremely proud of and that we're working very closely. So we worked very closely with Biovia to deploy a a a a mole-mim. So you see mole-mim as the nim here on the top left and the important thing here is that what Biovia implements is a choose-your-own oracle kind of approach to doing molecular design. So we have our mole-mim we have the optimizer that allows you to sort of take an external oracle so some prediction that you make that guides the evolution of the model and how it generates new candidate molecules and feed that back into mole-mim but you can do that in a flexible way on the Biovia platform. And so they've integrated very quickly our mole-mim nim into an end-to-end blueprint a customized blueprint not the one that we publish but one that takes advantage of the same technologies in order to quickly stand this up for their customers and enable this generative chemistry design on their platform. We've also worked with with Sapio Sciences this is a super cool really amazing implementation that takes a lot of these technologies which are typically very difficult for people to access and makes them available in a totally no-code platform. So they integrated mole-mim alpha-fold diff-doc into their electronic lab notebook solution and the result of that is that as you're working with your own data you now have instant access to workflows for chemical design for structure prediction and other pieces of this blueprint that they've implemented. And last certainly but not least we worked very closely with Cadence and OpenEye on their Orion molecular design platform integrating Bionemo. They took advantage of a number of NIMS that we've published recently most perhaps notably alpha-fold to multimer this is the multiple extension of alpha-fold in addition to MolMIM for their molecular design platform as well. Okay so I've got just a few minutes left and I want to tell you the next generation sort of work that we're doing and give you a sense of what's coming next. We've been working very hard on research work that we can expose in the products that I've told you about today. One of them is a model called GenMul. This is the next generation generative molecular design platform that is fragment-based. There are a lot of advantages to this. They use a slightly different grammar but the result of this is that you can really ground the generation of novel chemistry in reality and reality can be defined as is it synthesizable as an example. But you can also start with mass elements that you want to fill in. So you can ground the generation in something that is understandable by the individual user with a model like GenMul. And so a lot of people are taking advantage of this now to extend the work that we've been doing in MolMIM. We're also releasing new NIMS this week. Those include MMSeq and OpenFold2. These are models that are actually we've worked just like we've worked with the other models and made them a lot faster. So our MSA Search NIM, which is there at the bottom, is I think 23 times faster than the standard implementation in AlphaFold2. And our OpenFold NIM is now seven and a half times faster than the AlphaFold2 that we had previously discussed. And so end-to-end, you have really state-of-the-art performance in structure prediction facilitated by these. And we have little blueprints here for you to try out and examples of how you might stitch these things together for the purpose of structure prediction end-to-end, as well as the next generation of our virtual screening blueprint, so-called V2.0, which incorporates much faster alignment in MMSeq, much faster structure prediction in OpenFold, next-generation fragment-based design in GenMull, and improved performance and accuracy in DiffDoc2.0. So a really important generational improvement in the way that we build blueprints for virtual screening. Okay. Last but not least, I have two minutes left. This is really important. We've also been, you've heard a lot from me over this talk about the way that we take accelerations and we so-called drive them down the stack, which means we find things that are critical bottlenecks for the industry and we figure out how to expose those in ways that are really developer-friendly. And so we have been working with all of the groups that are developing foundation modeling in biology, and these are just examples of important models that have been published in the recent past. And what we want to do is find ways where we can develop common accelerations across a lot of different types of models in this space. And so one of the things that we did recently is we partnered with MIT and with their collaborators at Genesis Therapeutics to accelerate a model called Boltz1. And Boltz1 is a fully open source architecture. And what we were able to achieve through work that we did with TensorRT, so I've mentioned that library a few times, very, very important for us, as well as custom kernels that we've built with the CoupDNN team. Yes, that same CoupDNN team that sort of defines early performance in AI models. To get almost a 5x speedup in inference performance based on those two technologies in the Boltz1 architecture. And so what we're doing now is we're thinking about, okay, so we know how to put these things together. And we know about some of the early acceleration that we're getting here. How do we actually take this and make it general across all of the next generation work that's happening? And so this is a little bit of a teaser for the future. Please come back and see us at other conferences where we'll find ways of making sure that this is available to the entire ecosystem of next generation biological foundation models. I'm really pleased to tell everyone about collaborations that we have with Vantai and Dynotherapeutics. These are great partners of ours. Vantai, I believe tomorrow, is going to announce Neo1, which is their atomistic structure and de novo design model. It's the first of its kind that does both. And then DynaFold from Dynotherapeutics, which we've been collaborating in this space to understand and to be able to sample from dynamics much, much faster than actually running a physics-based calculation. So really incredible innovations from our tech bio partners here. And this is not the end of this story. We have an entire ecosystem of people that are putting all of this together and really innovating at the bleeding edge of our ability to design molecules, generative design workflows, structure prediction workflows. We're extremely proud of our collaborations with everyone you see here on this screen and in our entire tech bio, pharma, and digital biology ecosystem. So with that, I'll close and thank you for being here and I hope you learned something. And unfortunately, we don't have time for questions, but hopefully I'll see you afterwards.