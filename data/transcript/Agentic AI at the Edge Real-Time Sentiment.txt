 Temple Park Thank you. Thank you. With that said, let me introduce our presenters today. We have Jacques Eistock, CEO of Infuse. We have Niels Cash, Chief Data Scientist at Eviden. And then Kaylin Humphries, Chief AI Architect at Infuse. Welcome to GTC. And gentlemen, please get started. Welcome, everyone. It's so wonderful to see such a packed house. Thank you for coming. We'll kind of do a quick intro here. Niels? Hey, everybody. Nice to be here. Good to see this turnout. I'm Niels Cash. I'm the CEO of Evidenzy Data. I run a group of data scientists. We built awesome applications. Really glad to be here. And I'm Jacques Eistock. I'm the CEO of Infuse. We're a boutique organization. Super happy to be here. Kaylin? Yeah, it's wild seeing so many people here. My name is Kaylin. I'm the founding principal of a company called Infuse.io. And we have really, really good software engineers. Awesome. So let's go on a little bit. Probably not everyone here in the audience has heard of either Eviden or Infuse. And so, Niels, can you tell everyone just a little bit about Eviden? Yes. A word from our sponsors. So, Eviden is a data and technology company. We create and develop data and artificial intelligence solutions for our customers. We have a global team of about 41,000 professionalists. These are AI engineers, platform architects. We're a recognized leader in managed security solutions and high-performance computing. So, Ed Eviden actually lead a group that emphasizes generative AI in large language models. And we specialize in solutions that apply this technology in physical spaces and at the edge. All right. That sounds awesome. And Kaylin, tell us about Infuse. Infuse started out historically as a big data services company. We did a lot of things with Hadoop, a lot of things with Spark. And we had to move data around extremely quickly. So, we have about 100 plus AI native engineers. We're a boutique firm. So, we're able to get things done pretty quickly. And that's kind of our motto is agility over bureaucracy to an extent, right? I love it. So, now that we got the pleasantries out of the way, let's take a quick second here. And you'll see on the left just a prelude of the demo that we're going to show. Kaylin, do you want to just tell us a little bit about that? Yeah. Quickly, the three panels you see on the left are just three different tiers of digital twins in a dashboard application that we use in a real store. And Nils, in a real store, what does that mean? What is the use case? Why did we put this thing together? So, let's consider for a moment here M&W Markets. M&W Markets is actually one of our representative customers that really face industry-wide competitive pressures, okay? So, M&W Markets, for context, is a mid-sized grocery store in the Northwest. It has about 100 stores. And what they're up to is, like, they really have to compete against, like, these big-box retail stores, like, you know, the Walmarts, the Kroger's, the Safeways. The interesting things about M&W is, like, these stores actually serve the communities, right? These are community stores where people come together, build relationships. They, you know, they're nice and convenient. They're local. And so, it's really, like, the hub of the community. So, over the last, I want to say, five to seven years at Evident, we invested heavily in initiatives like building autonomous stores, doing shelf compliance, you know, ordering automation, like, things like that. However, like, these solutions really require substantial investment in new, you know, infrastructure, like, you know, think of cameras, think of cash registers and that kind of stuff. And that really makes them intangible to these stores, both from a cost perspective, but also from an expertise perspective. So, how can we at Evident, like, meaningfully help stores like M&W? And which, by the way, like, this market segment is actually the biggest segment in, like, the grocery market, right? Like, if you count up all the grocery stores sort of in that size, it's the biggest market segment. So, what we did is we actually shifted our direction to really focus on top-line revenue impact by building customer experiences, make these people come back to the store, have a great experience, and make them feel welcome, right? So, this is in contrast to, like, these big, you know, big retail stores, right? So, in this talk today, what we really want to discuss is, like, how does agentic AI fit into this vision, right? What is agentic AI? And what we'll specifically explore is how agentic AI can be used for real-time sentiment analysis and enhance context-aware serving of, like, things like advertising in the store to improve the customer experience. We also pay particular, you know, attention to privacy-preserving technology because that's a, you know, big thing in these kind of environments, and how to actually deliver a solution that is efficient, cost-efficient, and scales to not just one store but all of them. Okay, so before we get into the meat, I do want to dig in just a second about the difference between a customer experience and a customer engagement. Yes. Perfect. So, it's actually fairly simple. As the name suggests, customer experience is if you walk into the store, what does the customer experience as you walk around, right? Versus engagement is, you know, how does the store engage with the customer? How does the customer engage back with the store or, you know, the store owners, the operators? Okay. And so, why is this, why are we calling out this differentiation? And simply put, a positive customer experience is essential for fostering meaningful engagement. And, in turn, engagement drives revenue, right? So, I have, like, a little quote up here. And this is not just limited to grocery stores, right? This is, like, a universal sort of industry challenge, right? So, for example, all of us, like, we flew in to a new city, if you've been here the first time, right? Head to a new hotel. Wouldn't it be nice, like, if we feel welcome at the hotel? They know our preferences. Give us helpful information to navigate the landscape. And just, quite frankly, make us feel comfortable. And this is exactly where agentic AI can help. It allows us, or allows a store operator to express in natural language a goal that they're trying to achieve. And then the AI, with some, you know, magic behind the scenes, goes and executes that goal and achieves that goal. Okay. So, quick show of hands. Who in the audience thinks that they could define agentic? Quick show. Quick show. Anybody? Anybody? Hmm. A quarter, maybe a half. So, Nils, without reading all the words, tell me the difference between traditional AI and agentic AI and why it's good. All right. I'm going to act as your LLM and summarize the slide for you. So, think of traditional AI being like a cookbook. Okay? So, it provides the instructions, but you still have to go out and cook your meal. Right? Whereas, agentic AI is more like a personal chef. It plans. It goes grocery shopping for you. And then it cooks the meal for you. Right? And so, really, agentic AI is kind of like the next evolution to machine learning. And just a couple of things that I want to highlight here. And I kind of mentioned this a little bit before. So, agentic AI is goal-driven. Right? Meaning, you can do things like specify an objective in natural language versus, like, writing code. And then, behind the scenes, it goes and figures out how to accomplish that objective. Right? And at the core of it are these flexible foundational models. Meaning, we don't have to go out and train, like, all these little models to accomplish a particular task. We have a flexible model. We prompt it. And it accomplishes it. And the third component that's really important here is tool integration. So, tool calling stuff. Right? These are kind of like the agents that accomplish a task. And a good example is, like, hey, going out and getting, you know, up-to-date weather information. Like, in the simple case. Right? Or in a more complex case, an agent that actually goes out and reorders, like, inventory when inventory is low. Okay. So, I get how that helps an operator or an owner of a store. But from a development standpoint, Kalen, how does this help you and your team actually make things work? Yeah. So, from an engineering standpoint, in the past, at least, to accomplish what we're going to be showing today, we would call that a data pipeline. And that data pipeline would have a bunch of different stems on it. Some would be synchronous. Some would be asynchronous. Some would retrieve data. Some send data. In other words, a lot of code. Right? A lot of code to maintain. A lot of code that could get brittle over time. So, agentic AI takes that brittleness away for us a little bit and reduces, at least, the amount of code footprint we still have to write. We still have to write code. Right? But it's just a little bit more flexible now with how we use agents. Okay. So, that sounds fantastic. So, let's get into the meat. We're going to show a little video demo. Kalen, before we actually get to the demo, do you want to, like, tee it up a little bit? Yeah. Yeah. With the demo, I don't have a way to press pause. So, we're going to go through it quick and then we'll loop it again at the end if we have time. But what this represents is an operational panel that all runs off of a single Jetson device. So, I work for Infuse, not NVIDIA. But I would say go buy one of these outside at the booth right now. Because they're amazing not only to learn from a developer's point of view, but we also take these and we put them in these stores. We hook them up to kiosks, large screens. We hook them up to multiple cameras, which are in turn really just RTSP feeds into this unit. With the hardware accelerated libraries that we have through GStreamer and these sorts of modules, we can process AI from video frames amazingly quickly. So, I think we can go ahead and start this now. Here comes the AI wrap-off. What we see to start with is our map view. We could have multiple smart spaces or digital twins that we can navigate around and ask questions through our AI companion here. There's a checkpointing memory operation in Postgres. So, we can go on our web browser or our mobile phone and pick up where we left off. The first example is a hotel with IoT device integration where we materialize 3D objects. It's my humble opinion that a digital twin without live integrated data is just a fancy piece of art. We like to make them as live as possible. So, you see a dollhouse view here. We're going to query the AI companion. And anything that we have in this viewport, we can then ask, what are we looking at from any small detail? What color is the floor? What kind of brick is on the wall? Or, what is this? A three-dimensional model, right? We get as much information as we want back. We can summarize that and kind of massage it for whatever your preferences are. We're going to dive into the scene into a first-person view. And this is where the digital twin really comes alive. You'll see a billboard up there that has a couple things on it. Unlocked, lock status, occupancy count, and temperature. These are all sensors inside of these rooms. So, me as an operator could put on a pair of VR glasses, jump into my hotel. I could walk around, do whatever I need. All of these things are done by agent controls, even navigation. Move forward, rotate right, rotate left. It's all through natural language, right? So, if you think of an API layer on top of that, it makes it even more simple for a developer to hook up to something like this and start making things happen. So, we're doing a scene description here where we're just saying, what's in the glass case? I expect it to know that there's a glass case there. And so, it's able to find out quickly what kind of pastries are there. It's a non-technical dream if you look at the command that we just posted there, turn around. That's a command that takes parameters, degrees, right? But the agent and the large language model are smart enough to deduce how to use that tool within that isolated agent to get your task. The next digital twin we're going to jump into is a really fun one because this is an M&W store that we were talking about a few slides earlier. We use DeepStream in order to infer AI from security cameras inside of the store, existing security cameras. And this is where, instead of a large LLM, we use very small pointed classification and detection models. So, that we can get exactly what we need in order to infer the types of coupons, the types of promotions that the people walking through that door should be greeted with first thing. So, as we give it a command to rotate right here, you're going to see a spatially correct, I would say, representation of a live kiosk that we have in the store. You'll see the coupons are changing on the screen. Those are changing through the data that's being inferred through the camera. And then we do a similarity search with PG vector inside of a Postgres database that returns to us back ranked coupons for what it sees in the camera at that point. So, the kiosk is really fun here because this is real. We're mimicking the real world with this. But if we have someone that wants to try these things out and see what it looks like in their store, we can put them, place them anywhere they can put on VR glasses, and their end complaint might be it only comes up this high on me. Maybe I want it higher. That's a really cool thing about VR is to be able to see those things like that. And for the engineers in the room, these are some of the technologies that go into making something like this. A lot of these should look very familiar to you. React 3, Postgres, some of the newer stuff, of course, DeepStream, LaneGraph for agents, and we're going to get into the architecture a little bit more in a few slides from now. All right. Wow. That was a lot. Usually, we go a little bit slower than that, but hopefully, you guys all grok that. Just to kind of pull it all together, you know, at the big scheme of things, we are taking existing infrastructure, existing cameras, through the power of a far-edge device such as the Jetson, and in fact, such as the Jetson that Kalen has right here. We're able to do all of those things in real time so that when you walk into a store, relevant advertisements are available to you, and everything that you're doing throughout that store is tied into and in real time analyzed. If we go a little bit slower and really deep dive into the architecture itself, Kalen, you know, how did it all, you know, kind of get put together? Yeah, there's a lot to talk about as far as technologies that enable a solution like this, but there's a couple that I wanted to point out that have been very, very helpful, in particular, the top-left block there where we see AI and VR and VST. If you don't know what VST yet is, you can think of it like a video management system. It's Video Storage Toolkit by NVIDIA. You hook up all of your RTSP streams, and they make the output of those really, really easy to consume. AI and VR is a companion app. On top of that, where we can find regions of interest, tripwires, things like that, because as someone walks through that door or a group of people walk through that door, at M&W, we're given a super limited amount of time to be able to display the products we think that they want to buy before they turn the corner and their time is invested elsewhere in the store. Those tools help us a lot. And then, of course, DeepStream. We're big fans of DeepStream, and there's a framework that we use that we definitely have to shout out. It's an NVIDIA inception program called Savant. So Savant AI.io provide really good framework and tooling to be able to deploy these DeepStream pipelines in a stable pattern. And then Kafka, et cetera, and we'll talk about that a little bit more, too. Perfect. So now that you see the architecture of how to pull all these pieces together in a small form factor, Nils, how exactly do you put all this together for the use case for M&W to actually solutionize this? Yeah. So remember, we have to deliver a full-blown solution here, right? We have multiple stores. Each store may, depending on the size, might be 6,000 square foot to 10,000 square feet. And so everything that you just heard from Kalen, you'll see on the left here is what we consider the far edge, right? So this is the Jetson device that is interacting with the environment, is getting data from the environment, is performing data analysis, and is actually generating the ads that are being put on displays. The interesting thing to note, everything that you've seen in the virtual space that Kalen has shown in the demo is actually in the real store, too, right? So we have displays there. We have other things like avatars that customers can engage with and get recommendations on products and all this kind of stuff. And all of that is sort of classified as the far edge, the devices that power these things, okay? And we can scale that up, just add more. But the other interesting thing that we have is a near edge, right? And we talked earlier about privacy-preserving technology. So everything that is happening on the Jetson technically is already incorporated with best practices, is doing privacy-preserving stuff. But to be extra sure, on the near edge, and think about the near edge being like another machine sitting there, we can do other interesting things. We can run NIMS on there. We can run Nemo on there and do things like guard railing. So any interaction that is happening in the store, either from the customers or from the store operators or from the agents themselves, we can guard rail. So, for example, let's say an agent is supposed to order something and a security incident happened or something like that. We can inject these guard rail policies to ensure that the agent is still performing the task or kill the task off, right? So same thing with data. If any data that does not fit our policy could potentially make it out, it's killed off at that point, right? And so on the far right, basically the only thing that's going off to the cloud is like these, think of them like BI dashboards, right? Things that a store owner is interested in. It's like, oh, how many people visited the store today? So that kind of like reporting. And then, of course, to tie off the solution, some site management, right? Some statistics on, oh, are all my displays up, all my avatars up, or do I need to send somebody and go out and fix it? Which is awesome. As an old business intelligence data warehousing person, I look at these digital twins and this capability almost as the next evolution of dashboarding, of BI. Not only for operators, but, you know, in real time you can start layering these digital clouds together and do real time checking analysis, etc. Now, having said all that, we didn't just get to this architecture, did we, Kalen? No, I'm actually pretty empathetic to engineers, right? I've been to a lot of conferences over the years, and you see some pretty cool stuff, but you never really know how they got there, right? And nothing starts completed. So the first iteration that we had that seems like years ago at this point, well, yeah, I guess it was a couple years ago at this point, but seems longer, was when we could only do development on these things. If we wanted something to run on a Jetson, we had to do the development on the Jetson, right? Where's the problem in that? I live in Irvine. I don't live in Idaho at M&W stores where these have to be located, right? So I have to then VPN into the store site, and these aren't huge stores, right? So the internet connection is always spotty. And on top of that, we're wasting GPU space by having to run something like Rust Desk for a graphical display. So while that was necessary for a while until NVIDIA came out with new versions and stuff like that of DeepStream, it was a very cumbersome process. So the next slide on phase two is the next iteration of that. You can see here that we smartened up. We wisened up a little bit, and we thought, well, since this device has to be physically connected to a display and cameras, we better just buy two, right? And that way we can have one co-located next to us. We can work on it remotely. We can version our code, put our images in GitHub Container Registry, and then, of course, push those up and pull those down onto the unit itself. The hair pulling out here is some stuff that I think kind of like takes away from actually developing the end goal application for us, right? DeepStream is very, very flexible and gives a lot of freedom to the developer, sometimes too much freedom, right? Because you'll see a configuration for a certain type of stream, but maybe the configuration is an INI file, right? And the next one's a YAML file, and you just don't know which way to go. And so the third phase of that journey, at least, was us using Savant. Savant took care of so much tooling and boilerplate for us within DeepStream that it finally gave us the freedom to put everything else together, right? Because I care about the data being inferred out of those streams, and my collective team of engineers, I don't necessarily want them to be computer vision experts. We're data experts, right? So if that can be taken care of for us, then we can get other amazing things done very, very quickly. So here's the ideal setup that we have, at least for development. As everybody kind of knows, with DeepStream 7.1 now, hopefully you know that you can do development with DeepStream on an x86 now. You don't have to just do it on a Jetson. So we'll do all our local development for that quick, quick iteration just in front on our own work computers, right? I don't know about you guys, but for me, I put a lot of time into my i3 configuration and my window manager. Like, it's all customized. I don't use my mouse, right? Like, it's a more comfortable development environment for me, and I know for my engineers, the more comfortable they are, the better code I'm going to get, the quicker implementation, the quicker turnaround time. So, NVIDIA, you know, evolving DeepStream to a point where you can do that stuff locally on an x86 really, really speeds things up for us. We still have the Jetson remotely on each one of our engineers, so we use that for integration testing because, yeah, you know, it works on an x86. Does it really work how we intend it to on an ARM processor? So that's what we find out with that. So we have a couple different layers of checking and validation that the application and the pipeline, the AI inference pipeline, is working like we expect. As we expect. Once all that's signed off of, we commit our code, we build our images in GitHub Container Registry, and then when we want to deploy them at the edge, no longer are we working through a VPN typing directly into a terminal and xWindows on this thing. We're going remotely into an incrementing Apache version or a minor version and then doing a pull from Container Registry, and that's it, right? It's very, very quick, fast process with multiple checkpoints for validation, which you need. This last part here I thought was a lot of fun. The left-hand side is kind of what we end up with, or it is what we ended up with at M&W Markets, and you can see that pink line there. That pink line is a delineation from what we have running on the Jetson to what we have running an on-prem infrastructure. So because of the privacy nature of that data being inferred inside of that store, we don't necessarily want to go out to the Internet and do anything, right? In fact, we're not guaranteed Internet access in these stores a lot of the time, right? So everything has to be inferred on-site. That's where NVIDIA NIMS comes in for us. We have that running on infrastructure there. Our agents then talk to that, our front-end, our back-end, and then our storage and transit services. Those all kind of surround our Jetson, and it frees up enough space for us to put in a lot of custom models within our deep-stream pipelines and then hook those up to multiple cameras, right? So it's great. The right-hand side of it is the possibility of the cool stuff you can do on one of these units. It was about, I don't know, 10 years ago now, I gave a talk on a 2015 MacBook, and I was running Hadoop, Spark, and I think Cloud Foundry on one MacBook. It felt like it was going to light on fire, right? And now I can run way more than that. I can run everything on this one unit alone. The only thing that we grab out externally or that you can need to grab out externally is your LLM, because, of course, we're not going to run a 70 billion parameter large language model on here, right? But luckily, build.nvidia.com has API endpoints that we can hit from all day. So I just wanted to express with that right-hand side, it's ridiculous how much you can fit on that little tiny unit right there. That's incredible. So one unique thing here that I don't think I hear a lot in architectures is this use of Kafka for parallelism. Can you tell me how and why you guys chose that? Yeah. Kafka, I hate to say it, think of it like a cue, but in the old days, at least a cue, right? First in, first out. You grab one of those messages. You hope to God that your application doesn't have a runtime exception or something that you have to fall back on, because once you take a message, it's gone forever, right? What Kafka enables for us is a log processing instead of a first in, first out process. So instead of our data being pulled once by an application and then us crossing our fingers and hoping it processes right, instead of that, we take data based on offsets. And I can hook up any number of applications to a single topic that's then partitioned out, for example, 144 times. So say I have a topic. That's what the old cues were. I'm putting messages into the topic. Then they're going into partitions. I connect up to a topic with a single instance, and it'll consume by round-robinning on those partitions. But for what it allows us to do in this scenario, in this scenario, is more camera streams without any degradation of quality or any increase in latency, right? Because, again, it's very important to have tuned models in DeepStream to get what you're needing. But I would say a close second to that is a tuned infrastructure, right? Especially for immersive experiences where things have to react so quickly. So the cool thing about Kafka for us is we can scale it up to keep quality great, but also we can hook any number of applications to any checkpoint procedure we have throughout the entire data journey within this unit and that supporting infrastructure. So external applications can be hooked up and see data coming through live as the kiosk is presenting those promotions to people. Okay. So that's super cool. One unique thing, Nils, that you guys have kind of put together is this concept of human in the loop, which I find interesting because there is a lot of fear of AI in general, both from customers as well as end users. Can you tell us about that? Yeah, absolutely. So to kind of speak to the fear factor here, right? So you heard a lot about agentic AI and it's doing things on its own, right? You've seen earlier there are some guardrails in place that are supposed to prevent jailbreaking and this thing going off the rails and all this kind of stuff. But a lot of our customers are asking, like, how do we keep a human in the loop, like provide this oversight process? And for us, like, there's actually two things. So from an operator perspective of a store, right, like I mentioned earlier, these guys are not, you know, techies like we are, right? These are, you know, normal store manager and all this kind of stuff, right? And so even prompting in natural language might not be the natural thing for them to do, right? So human in the loop, like we have a mechanism of escalation where these guys can reach out, ask for support. And funny enough, the agentic AI actually helps with that process, right? Like if it detects that, hey, there's something not quite right here or they're struggling with something, we can refer them to, like, a human customer support agent. Same thing on actually dealing with customers in the store. So imagine somebody is talking to an avatar or is, you know, seeing some sort of advertising in the store. They're not quite happy. They get frustrated. You always want to provide, like, some sort of escalation path where somebody is in the loop, right? And the agents that we employ, like, you know, allow us to do that in a very cohesive and fluid fashion that, in a way, do consider the own guardrail policies of the store of when to escalate. So it's kind of like an optional thing that could happen or a mandatory thing depending on, you know, what is configured in the store. That's awesome. So a couple key takeaways from today, and we'll get to some questions. First and foremost, everything that we show today, this is all real. This is all production stuff that is running, unlike, you know, a lot of conversations that you might have out in the wild. But when I look at the key takeaways, first and foremost, privacy is super important. Eviden is a global organization. GDPR comes into play. We take that seriously. Your architecture also needs to take that seriously. With agendic decision-making, you now have the ability to, as Kaylin had said earlier, not have your developers build brittle code. Instead, you can move faster. You can move better. It also allows us to have low-code, natural language prompting. This makes our developers faster as well. We can get to an end product sooner. And then lastly, because of this, everyday employees can actually operate and use these tools. I'm sure everyone has been the victim of an application that is supposed to help you, but it is, in fact, super unintuitive to use and, therefore, frustrates you. We are Eviden and Infuse. If you scan the QR code or come visit us at the Eviden booth at 2,219, we'd happily tell you more. Infuse.io, we're about 100 folks. We are hiring. If you liked any of the stuff that you see and want to do it, or if you liked any of the stuff you see and want to learn how to do it, let us know. Lastly, I'd say we can't do any of this without our partners. You know, you can go fast alone, but you can go fast and farther with your partners. We have a stable suite of partners that help us through everything. And now I'll open it up to any questions that anybody may have about any of the architecture, the use case, or anything in between. Thank you, Niels, Jacques, and Kalen. Please raise your hand if you have a question, and we will bring a microphone to you. Thank you. What is your frame rate of how fast are you processing images, and is that adaptive, and how many video streams? Is it based on the amount of traffic, or is that dynamic? Yeah, well, it depends on how much you have running on the single unit, right? I can say with this, we've had four camera streams running simultaneously, and we get between 25 and 30 frames a second for each one, which is nice. But that's why you see, like, on the on-prem infrastructure, a lot of that is externalized out to on-prem stuff, because, again, GPU real estate, at least for something like this, is very valuable, right? So you have to preserve what you can to make sure that, number one, the camera processing works, right? Because we care about that. If that doesn't work, then nothing else really matters. And it actually kind of depends on the use case, too. There are certain use cases where you do need a high frame rate, and others where you don't. So, for example, at M&W, they have this hot food display, right? So think of, like, your local 7-Elevens. Like, there's hot dogs rolling. There's, like, fries and all this kind of stuff, right? And, you know, letting the store employees know is, like, oh, when to turn something or when to replenish something, right? Like, you don't need a super high frame rate for this. Like, you know, an image a second is, like, totally fine here. I mentioned one more thing about that, too. It's when you get those low frame rates, doing a little bit more investigation, you tend to run into, well, maybe I don't need 4096 by 4096 resolution on my security camera just to see a person walk through the door, right? So if you can give up on quality and still get back the results you need, do it all day long. Because the more space you can free up on this, as we know with NVIDIA, they're going to come out with something rad in, like, two weeks. You're going to want to put on this, too, to interact with as another agent, right? Hi, I'm Karishma. I'm a recent Georgia Tech graduate. I have a quick question. When you are fine-tuning LLMs on Nemo, when it comes to mapping the user journey, do you face data sparsity issues? And how does that affect the model quality and end results? Yeah, great question. What are you doing later? We're hurrying. So the short and cop-out answer here is that in a lot of cases, we actually don't have to fine-tune. Like, we literally just use foundational models. And, you know, with creative prompting, enable the agents to perform their task, right? So this is, like, the whole, you know, developer efficiency, low-code kind of thing that we're going for with the Gentic AI. If we do have to fine-tune, we can. You know, we train. So this is typically for, like, other industries that we work with that need, like, domain-specific information. And, yeah, there it becomes, like, a data problem per se, right? Like, you want to ensure that, oh, you're not training, like, any bias things into your data set. So my background is machine learning. And, like, my data scientist, like, I make sure to pay particular emphasis that people care about the data that they work with. And we have some checks and balances in place to ensure, you know, data skew is not happening. Hello. Hello. Could you give an example of process that the employees operate with the system? And what's the ROI perspectives of the project? So if I understand the question, how do the operators use all of this, right? And so for M&W specifically, they're using it in order to better understand before they deploy a technology across 100 stores, they like to be able to, in virtual reality, experience what they're about to put in, in the store itself, without having to necessitate getting a bunch of people out on site. From an operations standpoint, at any point in time, anybody can go into a digital twin, actually see the live stream cameras and see the results of what is happening, a la somebody walked in, here are the ads that were displayed for that. We do have a continuous loop, so we know, based on metadata of what's going on and ads displayed, we fine-tune how we do it. And I think most importantly, from a store manager standpoint, the agentic nature of the whole system allows them to say, hey, today it's March 17th, and I want to sell more Coca-Cola. Like, I need to sell more Coca-Cola. The system can take that prompt and actually start to influence the ads that we're displaying based on that prompt, nothing else. And so it really empowers the end users from that side. And I forgot the second question, actually. Actually, Jacques, I'm sorry, we're out of time. But if you have more questions, you can come and talk to our great presenters here, afterward here in the room, and then after a couple minutes, you can talk to them outside. So I'd like to thank Niels, Jacques, and Kalen. And, you know, great round of applause for our presenters. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Bye. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.