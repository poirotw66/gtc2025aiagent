大家好,我是原永启行CEO周光今年是我第二次受邀来英伟达GTC给大家分享最新的思考今天我分享的主题是自动驾驶到AGI下一代自动驾驶技术的演进在去年3月份的GTC上我跟大家分享了我们技术线路的演进从不依赖高晶度地图的研发到端道端的部署上车再到最近的VLA研发正式开始之前我想先跟大家同步一下过去一年的发展首先来说我们公司在去年有两款车型合作楼地今年预计会推出10余款自动驾驶车型在去年我们仅用了三个月就在全省是全国开放了城市和高速的NOA其中一款以车企合作的自家车月销量达到了8000一度成为六座七座的SUV里面最热销的车型在这款车型上圆融成功地扮演了爆款制造机的角色并且在2024年第四季度我们圆融仅仅凭借单款车型市场占预率达到了15%这个也是来自第三方版人会的权威数据在过去几个月DLD模型我们快速部署到了4万多台车获得了大量的优质数据这期间我们也遇到了一些现阶段DLD1.0无法解决的问题从去年9月份我们就开始研发VLA模型将进一步提高解决城市高阶自驾的能力我们可以处理复杂罕见的常规场景接下来我跟大家分享一下DLD2.0以及VLA模型的一些思考首先来说我们先看一下VLA的一个模型架构我们从下一个往上看首先是输入input视觉输入即模型接受这个图像作为输入那图像可能包含了我们各个视角的摄像头也包含了物体场景以及其他的视觉信息那么第二个是一个文本的输入那么这个文本的输入可能是一些指令可以一些些交互也可以是接受到这种多模态大模型的输入跟AI做仓的一个联动以及导航等信息接下来的话是输入之后的话是一个视觉编码器以及文本的编码器那么视觉编码器其实负责是处理输入的图像提取这些高层次的特征这些特征可能包含了用于物体识别的场景理解的以及空间关系的这样的一些特征那么文本编辑输入器的话实际上输入的是一个上下文相关的一个项链那么这个项链它捕捉了文本的语义包括导航包括人类的交互包括车辆来自于更上级AI的这样的一个交互信息随着这些视觉跟文本的特征被融合成一个统一的表达那么这一步其实是VoA模型的一个核心我们讲多模态的这样的一个信息融合起来以及使我们这个模型能够同时理解图像以及文本的一个指令那么当融合完了这些特征之后视觉编码器可以根据这些的一个输出那么文本编码器也根据文本的一个输出最终我们会输出一个是驾驶动作行为以及说为什么说我们会有这样一个行为的一个逻辑推理比如说当前面的这个我们的车辆遇到了右途这样一个场景那么我们的第一来说我们的决策路径会告诉你要做个减速那么我们的文本的话也会输出告诉你是因为有人在过马路那么我们需要做出在那个减速的一个行为那这就是一个模型架构的一个图那接下来的话我们会讲到这个VLA的一个三大优势那么首先来说第一是所有的这个组件的话都会被标记化Tokenization那么我们VLA模型的话整个模型其实也以现有的这些LM大语言模型的范式兼容那么我们其实这样子的话我们可以通过这个大模型的一个架构来升级我们这个模型那同时我们整个的这个这个模型的话也能够享受大模型的scanning law比如说数据的增加那么这个模型参数的增加性能的提升同时的话我们也可以用这个预期训练后续练跟持续训练的方法来改进我们的模型那以及这个人类偏好的这样一个对齐比如说我们在我们有pre training 的一个部分对吧我们也我们在这个这个 post training 的话我们也会用到这个像人类的 feedbackRHF DPO GPE GRPO 等等这些技术其实对于我们的这个这个最终的 alignment 跟 safety 其实还是非常有关的因为这个我们认为说还不仅仅只是一个 imitation learning 就能解决了自家然后另外一个特点的话就是VLA模型的话我们是有这个 chain of salt 有思维链那么它可以使模型能够进行一个 training那举个例子来说中国很多很多那个大城市里面比如说潮汐车道 可编车道对吧这都是用于来减少这个交通堵塞的当我们的一个自驾系统啊遇到这一个场景的话它会识别相关的道路标识那么根据这个道路标识来评估当前是否可以使用这个车道对吧还会根据交通状况啊相邻车辆啊等等预设他们的轨迹基于这些所有的所有数据的一个分析我们也会决策是否加速减速对吧并相应的调整行为那么VLA模型的这个强大之处的话它会其实它会逐步的解释你的决策过程是整个我们的推理透明化而且还能够在整个过程中维持长时序的这样的一个理解其实我们认为这个长时序因果可能是自动驾驶里面应该是最难的而且这个也是跟语言大模型高度相关的我们会随着时间的话会不断的优化我们的决策流程从而确保有个最安全的一个行动方案那当然这个的话我们的VLA模型的话也可以通过强化学习的训练方式来做这个后训练那么这样的话我们也可以判断说这些这个这个是否符合物理规则那VLA的另外一个关键优势的话其实是使这个驾驶行为更加偏向于人类的一个偏好为了使驾驶体验跟人类的偏好相匹配的话那么我们这里的对此技术也是关键对吧我们可以将基于这个规则的偏好然后注入模型同而使我们这个VLA能够做出符合我们人类所预期的驾驶角色那为了使我们这个VLA能够做出符合我们人类所预期的驾驶角色在实现这里我们可以根据规号和人类的偏好结合VLA模型的多模态输出对吧标注良好的形式轨迹这意味着说我们的VLA模型能够识别并优先考虑这些安全的驾驶行为从而减少现实事件中可能产生的不良习惯比如说压实线等等当前面对多过可能的形式轨迹的话对其技术也可以使我们的VLA能够选择最符合人类偏好的路径例如说它会悠闲优先跟随车道线中心行驶对吧这其实是人类驾驶的一个常见行为比如说过马路的话我们也会优先跟着前车一起走也不是说看到一个车我们就照看其实这些也是我们认为的非常大的一个优势但是其实像这个VLA的模型部署也没有一些挑战我觉得最重要的其实是一个实时响应的能力对吧随着其实我们加入了语言层其实整个的模型的这个参数量会变大那么但是实际上我们在这个这个这个这个模型VLA的时候我们其实也是做的是一个一个一个一个一个快速的一个响应所以说我们也需要达到毫秒级百毫秒级能够响应处理所有的信息这也需要做像高效的模型架构对吧以及可扩展这样一个预心链跟微调的这样的策略为了实现最佳的性能的话我们也当然我们也依赖高性能的硬件了比如说像英伟达的AGX平台这些多款的高性能芯片那举个例子来说因为打下一代的AGX芯片具备更高效的处理能力低功耗以及强大的并性计算能力那适合大部分的处理数据啊能够复杂的机器学习任务那同时的话我们认为说这个硬件加速也非常关键那么高效的硬件加速设计那么可以是我们在多任务处理啊能够在时时分期数据中有一个非常好的突出的一个表现那它还支持这个高度优化的计算架构减少延迟并提升响应速度非常适合需要快速角色的应用场景VLA的视觉语言动作模型需要这样高效的芯片来提供高效的数据能力以及实时反馈能力从而更好的实现复杂任务的学习和执行通过高性能芯片的这个支持吧我们觉得VLA模型一定能够并能够在物理视频中能够实施的处理大量的信息能够高效精准的预判这个动作然后进行决策那第二个挑战的话我们认为这个是数据了虽然说现在有一些公司它预来以模拟数据但模拟数据它整体来说还是很优限的而且现实事件中的很多复杂微妙的一些零件态其实这种模拟器是很难以被描述的一句话来说如果说你想完整的去simulate这个事件可能你需要算力需要的这个花费的这个金钱是大于你的现实事件中采集的比如说你要你要模拟一个仇段一个布对吧其实说实话一个布你用这个用这种显卡去渲染可能你需要把一个布皮拆成几十万上百万份才能更精妙的去描述这个布皮那如果我们在现实事件中去采集数据我觉得是更为高效的那个那那想让咱们的VOA模型的话去实现这个scanning law我们需要非常丰富的真实的实体的行驶数据当然这个benefit from我们的量产对吧我们目前的话已经有四万多台车了那整个的这个数据啊我们也是有非常丰富的一个数据那么从数据的一个整理从感知端的对吧从你的这个这个一个标注啊到行为的这个数据啊对吧这些转这些转边的话都是为了让我们有更好的去训练模型从而能够理解跟模仿人类的驾驶偏好安全等等这就是我们为什么会优先使用真实实体的数据尤其是关键态零件态的数据比如说呃路边两个人是吧你说你打你开GTA的时候你可以assume这些人可能呃就是一个NPC嘛对吧他可能就是一个打饼NPC但实际上现实实体的人是非常复杂的比如说这个人可能是两个人在吵架对吧在argue那这个时候其实这个人是有可能能够冲那个冲到路上来的但是如果说两个人是一个stable的人在那里或者在那里对吧可能是做一些非常peaceful的一些谈话可能他也不会下来其实真实实体远比模拟系统要复杂但这些critical的这些critical状态的数据其实才是最关键的那么呃这对我们的虚拟vla模型来讲是无加之宝呃刚才也说了我们从去年8月份到今年2月份我们有近4万台车截止呃2月份底我们的总自家里程的话已经超过了4000万公里那么今年我预计会有20万台车投入消费者市场2026年预计我们会有50万台车搭载我们的自驾系统自驾里程的话我相信也会直速级的增长在中国之外我们也开始了在德国的路车也会加快海外自驾的一个部署那么基于这个海量的真实现实数据进行脱名那么我们也构建了一套非常高效的数据并环那么从这个这个也是一个全自动的一个数据链路从数据脱名回流清洗模型训练发板我们可以做到以周级为单位大幅提升我们的自驾OTA速度加速产品的性能进化持续为用户提供更安全自能的驾驶体验那今年年中我们最新的VLA模型的话预计会部署到多款车上消费者能够享受高阶自能驾驶的功能VLA决策更透明因此我们相信这个新的产品能让消费者对自驾系统更信任也会VLA也会更理人有长时序的推理能力因此决策也会更优秀像内优秀的老司机整体驾驶会更安全刚才我们前面提到了这些复杂的罕见的场景我觉得VLA的这样一个系统你是可以更好处理的近期像TESLA也在中国部署它的V13其实我们可以看到V13的技术能力非常强就像在今天的话在我们的自驾过程中有一些case我们认为是非常难的比如说在路边停的一个车那么这个车子还想出来也许它只是打了一个方向盘但是我们人类实际上是可以知道它会出来但是实际上它在如果说你没有加入这些VLA这样的一个高级的AI体系的情况之下你只是靠一些bunding blocks靠BEV是根本没有能力去处理的那么我们相信说借助这套更新的AI架构我们可以更好的去理解和处理这些场景当然实现自动驾驶大部分量产之后我们每天都有数万台甚至未来会有数十万台车搭载我们的系统在路上行驶这些车辆记得是优质数据对于各位环境的理解能力对于物体的交互能力都会提升我们模型的在空间上的物理事件的一个理解比如说空间层面的旋转不边性叫做推行这扭顿定律等等以及人的一些行为比如说我们人在路上大概会有哪些预期的一些行为再加上世界再加上语言的导航能力我们会应用到不同形态的物体上然后我觉得我们下一段我们公司的目标是实现Rode AGI即所有在道路上的机器人都会用到我们这套通用的Rode AGI但这也是我们大规模量产之后的下一个目标接下来我们看一个视频整个这个视频的话就是利用的是我们的DONONDON技术然后应用在城市的场景里面可以实现城市里面从小区到小区任何小区到小区任何商场的某一个店铺到小区店铺到店铺之间的一个移动能力我们认为这个会对未来的城市物流会产生非常大美丽的影响那个视频中可以展现出来我们整个的这个学习过程是采用的是DONONDONON技术那么整个的这些车辆的这些避障动作都不是通过规则是写的都是基于这个驾驶员的运动我们学的是这个驾驶员的这些避障避障躲闪以及基于导航的一个能力同时的话在进入小区之后我们也可以根据这个周围环境的这种context我们可以自作选择我们去哪个商铺哪一个店到了电梯的话我们也可以通过这个一个手臂去控制这个电梯空子的电梯加入法杉 そう我MEN搅� gost Right这个您 problematicкольку我们可以 models这条件处理请不吝点赞 订阅 转发 打赏支持明镜与点点栏目请不吝点赞 订阅 转发 打赏支持明镜与点点栏目请不吝点赞 订阅 转发 打赏支持明镜与点点栏目在未来的话道路上所有移动物体我们认为都会被AI驱动那么我们的road AGI系统能够不依赖高级路地图实现任意点栏目的移动不仅像智能驾驶车辆一样在外面的道路上可以移动而且我们还能免部署买来就用就跟一个人一样我告诉你说去隔壁的百花小区三栋五单元一楼201房间对吧 共取个东西我们就可以实现通过语言经纵通过语言就可以到那个地方去拿回我的东西可以实现跨石雷外无缝衔接真正实现跟人一样的门道的能力未来的道路当中型我们可以适配各种本体吧在这里的话我们展现的其实也只是一个小车但实际上这只是个底盘对吧 不管是机械狗像外卖小车包括像人型的机器人都可以采用我们这套RODE AGI而且我们也支持语言级别的探索刚才也提到了从商场来个外卖到我家在导航到商场商场内部的导航也是基于大模型来做的最后的话我想用这张图来介绍原容情形的使命我们的使命是打造物理世界的通用物理世界的通用人工智能那么去年开始我们量产了我们今年会我们在中国的量产会进一步提速同时也会走向海外海量的这些自家数据能够持续训练我们的模型下一步当然就是RODE AGI我们相信道路的通用人工智能会在接下来几年快速出现我们的自家系统能够赴能道路上不同形态的移动物体进一步提升生产力最终迈向真正的物理识点通用人工智能最后谢谢大家关注这是我们的NINK IN X 频道也了解人用情形最新的新闻以及进展感谢大家再见再见