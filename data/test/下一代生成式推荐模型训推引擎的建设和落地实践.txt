大家好,我是美团核心本地商业业务研发平台的于磊,负责美团搜索推荐业务的机器学习平台工作。本次由我和同事马池向大家介绍美团下一代生成式推荐模型,迅推引擎的建设和落地实践。整体介绍分成四个部分。第一部分是背景介绍,即为什么我们要做生成式推荐。第二部分是模型介绍,即生成式推荐模型要如何设计。第三部分是MTGR介绍,MTGR即美团GR。这是我们为生成式推荐GR模型所建设的训练推荐引擎。最后,第四部分是总结和展望,我们会介绍未来的相关工作。下面我们开始做具体的工作介绍。首先是为什么我们要做生成式推荐。生成式推荐是推荐系统的一种做工方法。大家在日常生活中,对推荐系统的使用是非常多的。比如日常刷抖音视频,又比如在美团首页,美团外卖首页等多个流量入口,都能看到推荐的菜品和商家。他们背后都有精细且智能的推荐系统在支撑。推荐系统所做的事,就是综合考虑用户信息、物品信息、场景信息之后,从庞大的候选物品库中选择用户可能感兴趣的物品,并推荐给用户。推荐系统的相关研究很多,发展了很多年,已经非常成熟了。沿用之前的模型思路继续提升效果变得越来越困难。在大模型出现之后,业界认为这是一个很好的机会,开始关注如何基于大模型思想做推荐系统。大模型的核心思想之一是Skilling Law,在2020年由OpenAI首次提出,即模型性能随着模型的规模、数据量和计算资源的增加而不断提升,并且遵循着一定的数据规律。那么是否可以把Skilling Law的思想应用到推荐系统呢?这并不容易。首先,推荐系统的模型结构非常多样,什么样的模型结构有利于Skilling呢?这需要做深入的探索。其次,从模型训练的角度考虑,推荐系统模型训练的Token数很大,远高于大模型。考虑到E级别用户,万级别的用户序列,百天级别的训练样本,训练时Token会到10的14次方数量级。这比GPT3模型还要高两个数量级。从模型推理角度考虑,为了不影响用户体验,推荐系统模型的线上推理延迟有着非常严格的限制,大概是几十毫秒级别。这个约束远远高于大模型的推理延迟约束。如何做到低成本高效率的模型训练和推理,这在算法设计和工程实践方面都面临着巨大的挑战。关于这个课题,在2024年3月,MED团队发布了一种新的模型结构,命名为悟空,在推荐系统领域观察到了Skilling Lot的规律。这个模型结构如右图所示。悟空在同等模型复杂度时,模型质量高于其他Sota模型。并且当模型复杂度提升两个数量级时,模型质量也能不断地提高。最终,模型复杂度达到了100G Flops,和GPT3规模相当。悟空模型虽然符合Skilling Lot规律,但是并没有在业务上真正落地。和务工同期,MED团队另外一个团队发布了新的模型结构,HSTU,并利用生成式思想进行了排序和召回。发布了对应的生成式模型,Jannity Recommenders,也就是GIA。GIA模型符合Skilling Lot规律,并且是第一个在业务上落地的生成式推荐模型。GIA对整个建模方式、任务定义等都进行了重构,而且有很好的业务效果。业界将GIA这种生成式模型看作下一代推荐模型。这个模型在几个方面都和传统的DLIR模型不同。在数据组织方面,GIA模型将所有的特征组织成序列的形式,并选择一个最长序列作为主序列。一般而言,主序列就是用户的历史性为item序列,更新很快。在每个item之后,再插入用户对这个item的点击转化等行为信息。此外,还有一个更新较慢的辅助序列。当有更新时,再把对用变化插入到主序列当中。最终生成一个包含用户所有历史交互属性变化的长序列。在任务定义方面,对于召回任务,利用自回归的方式去预测下一个点击item的概率分布,并选择topcake item。对于排序任务,直接将候选target拼接到序列的末尾,再接上不同的MLP hide进行多目标的预估。在模型结构方面,将长序列输入transformer进行信息交互,并对transformer做了改动。将softmax建模改为ponovice建模,并且在QQV作用结果之后,增加了一个learnum和u的点击。这有点类似mask.net的结构,用于实现特征的进一步交叉。利用HSTU结构的下一代GN模型,比原推荐系统模型有更好的效果。如左途所示,不论离前指标还是在线AB实验,GN模型都优于DLM模型。并且,GN模型展现了很好的Skin-N-Out的性质,如右途所示。这意味着找到了一条模型效果优化的可持续发展路径,为模型效果的持续提升打开了新的空间。Meta利用生成式思想做推荐的思路,给了业界很大的启发。现在我们来介绍美团外卖场景下,如何利用生成式思想做推荐。总的来说,我们借鉴了MetaGN的做法,并根据实际情况进行了一些调整。我们认为这和传统的DLM模型相比,可以有效提升推荐模型的训练和推理效率。具体来说,在建模方式上,我们采用Listwise的方式替代Pondwise的方式做建模。在训练范式上,我们将同一用户的多次曝光压缩成一条样本,避免用户信息重复计算,并且无需复材样,避免了对应的额外开销。在推理范式上,我们采用了中UserModel的设计,并且尽量复用KPcatch,可以进一步减少重复计算,提升整理吞吐。在模型结构设计上,我们参考了HSTU的实现,并且根据实际情况,在序列构成和多目标预估上做了一些改动。这里就不做展开介绍了。在模型效果方面,我们将模型复杂度,叫Baseline模型提升了两个数量级,在理线评估中,观察到了近似对数线性的Skin-NOW的现象。此外,线上AB实验也取得了不错的效果提升。并且,通过一系列的工程优化,我们将训练成本降低了20倍,利用有限资源高效支撑了算法的迭代创新。下面,我们会对工程优化工作进行详细的介绍。大家好,我是来自美团的马池。接下来由我给大家详细介绍下我们的工程优化工作。首先,我们整体分析下GR模型的训练和推理面临的挑战。从理线训练的角度看,按照Skin-NOW的指导,模型效果的提升来自训练数据量和模型参数量的增加,从而带来训练计算量的大幅增加,需要很高的训练成本。我们以LM作为参考,其一般都需要数百张GPU卡进行总共几十天的训练。这在之前的推荐场景是无法想象的。同时,由于西数Inbanding规模增加,给Inbanding的分布式存储和分布式通信带来严峻的性能挑战。从在线推理的角度看,模型大小和计算量膨胀显著。为了满足严格的延迟限制,需要更强算力,更大显存的GPU,多张卡并行推理,甚至CPU、GPU联合推理的易购计算模式。鉴于这些问题,我们建设了美团生成式推荐模型训推引擎,MTGR,解决模型计算量和存储量激增带来的诸多性能挑战。主要包括两个核心组件。一、MTGR Training,支持低成本、高效率的大规模分布式训练。二、MTGR Inference,支持低延迟、高吞吐的大规模线上推理部署。下面将分别进行介绍。首先介绍MTGR Training,这是我们基于TouchRack构建的简单易用、性能优异的GR模型训练引擎,支持千亿参数、By GFLOPs Per Example计算量的模型的训练。如图所示,整个系统框架分成三层。最底层是TouchRack,TouchRack是META开源的吸收模型训练框架。我们对其做了一些功能上的扩展,例如支持动态Hash Table、T度累积、通信优化等,满足业务的实际需求。对于模型变形,业界也有很多其他可以借鉴和复用的工作,比如Invidia的MagTrain。中间层就是MTGR Training,我们的主要工作也集中在这一层。其对训练流程进行了封装,包括离线流失数据读取。训练评估流程,断领续训,离在线移支撑校验,Warm Start。同时,给用户暴露简单的模型定义接口。最上层是算法模型层,不同业务根据我们提供的接口定义自己的算法模型。从深度学习框架的选型上,我们选择了PyTouch,主要出于以下几点考虑。首先,大模型社区已经实质上抛弃了TF。搜推广领域也开始出现了许多基于PyTouch的尝试。比如,Meta内部一直在使用PyTouch进行大规模推荐模型的训练,并且在去年10月发布了TouchRack的第一个正式版本。第二,国内大厂,包括阿里、腾讯在内,也陆续发布了许多基于PyTouch的工作。从实际体验上来看,PyTouch简单易用,上手快,社区发展迅速,虽然其动态图的特性导致性能会略逊于TF,但是PyTouch社区一直在努力优化性能。最新发布的PyTouch 2.0及以上版本,在许多奔驰骂格上,性能已经和TF持平,甚至实现了反超。从研究论文的使用框架上看,TF的占比从20年底的25%,下降到现在只有不到5%,PyTouch一统江湖已经是大势所趋。再介绍下我们采取的并行策略。推荐模型的特点是大Spas,小Dense,因此采用Spas部分模型并行,加Dense部分数据并行的混合并行策略。同时考虑到Dense部分会参考大模型的结构进行引进,比如像引入DeepSeq这样大的MOE,我们计划采用PyTouch, FSDP,或者Megatron的模型并行方案,通过TP,PP,EP等更复杂的模型并行策略,支持Dense部分的模型并行。TouchRack支持Spas部分的多种沙丁策略,包括Tablewise的沙丁,Rowwise的沙丁,Columnwise的沙丁,DataParalleled,并且其提供了一个Planet,可为不同配置的模型搜索得到最优的沙丁Plan,我们实测效果也还不错。下图以Tablewise沙丁为例,展示了N张卡同步训练的前项计算过程。从Tunner0到Tunnern为同步训练的多个GPUWorker,也就是从GPU0到GPUn。每张卡读入自己的Local Batch,里面包含特征0到特征n全部的特征数据。按照Tablewise沙丁的策略,不同的特征对应的Embanding表会被Partition到不同的卡上,例如卡0负责特征0的Embanding表,卡N负责特征n的Embanding表。接下来会进行第一次Out-Out通信,也就是图中蓝色的Fature Out-Out,将特征发送到对应的卡上,例如特征0会被全部发送到卡0上,特征n全部发送到卡N上。每张卡拿到自己的ID后,进行Embanding Lookup。接下来再进行第二次的Out-Out,也就是图中蓝色的Embanding Out-Out,将Embanding的结果返回给对应发送的卡。每张卡拿到所有特征的Embanding之后,再慷慨的起来,进行后续Dense部分数据并行的计算。接下来介绍我们在MTGR春年上做的具体优化工作,主要分为两大类,功能支持和性能优化。在功能支持上,第一个就是动态Hashi Table。动态Hashi Table更适合工业界的场景,因为实际应用中,吸收ID是不断变化的,例如商品频繁的上限和下限。动态Hashi Table支持动态扩容,无需提前设置容量,无需提前对ID进行预映射。Touch Rack原生只支持类似数组结构的静态表,虽然社区有人贡献了Dynamic Embanding的插件,但是我们实测性能比较差,因此我们将自研的高性能Hashi Table集成到了Touch Rack中。第二个是梯度累积功能,支持算法进行大Bitch Size的训练。Touch Rack原生也不支持这个功能,其实现的难点在于推荐系统中,Bitch之间的吸收参数不是完全相同的,吸收梯度需要高效的聚合方案。接下来是性能优化工作,也是我们做工最多的地方。第一个是自动合表,我们设计实现了一套吸收源与API,用户仅需定义特征的关键属性,即可自动生成模型的SPAS部分,并且对多个吸收表进行自动合并。开启自动合表后,我们实测端到端训练吞吐提升8%,第二个优化是混合精度训练,这个大家用的都比较多,在此不再赘述,它对GIM密集的结构收益显著。第三个是科能优化,科能优化和实际模型结构相关,我们知道LM引入FlashAttention后,计算效率翻了10倍,GR中最主要的子结构是HSTU计算单元,也可以使用类似FlashAttention的优化思路,在我们的长展下,使用优化的HSTU Kernel,端到端训练吞吐提升85%。第四个是ID Unique,推荐系统中冷热ID的现象非常明显,少部分的热门ID贡献了绝大部分的特征,通过ID Unique的方式,输入的SpathID先去虫,再进行AutoAu通信,极大降低了通信数据量,查到的Embanded,它的数据量也极大降低了,特别是在多机训练时收益显著,因为机间通信带宽相对比较低,在我们场景下,4机32卡训练,实测端到端训练吞吐提升45%。第五个是变长序列负载均衡,这个后面会细讲,它也是针对推荐系统做的定制优化,端到端训练吞吐可提升30%。最后一个是GAUC计算优化,算法同学在训练过程中,经常需要观察某些评估指标,比如GAUC,乃义5的实现会严重拖慢训练的主流程,通过精心编排计算逻辑,将一些CPU负载较重的计算,前一道数据读取进程中,可降低训练进程中的资源竞争,端到端的训练吞吐提升10%。接下来会向大家详细介绍其中一些重点工作。第一个工作就是动态哈记提本。我们根据TouchRack提供的Interface,构建了一套单独的Dynamic Embanding Pipeline,如图右半部分所示。由于使用了同一套接口,API参数基本相同,用户切换的成本很低。同时,为了进一步降低切换的成本,我们实现了静态表和HashitBall数据之间的互相加载,方便用户在不同模式之间进行Warm Start。比如用户使用TouchRack原生的静态表训练了一个模型A,切换到我们的HashitBall实现后,可以直接继承模型A的所有吸收参数,无需重复训练。第二个工作是自动合表,我们以左下图为例,用户定义了四个吸收特征,设置了Embanding表的名称,Embanding Dimension等参数后,框架侧内置的Sparse Arc模块统一创建吸收Embanding层,创建合并后的表,随后进行Lookup操作,最终返回Embanding的字典。用户只需要从字典中获得特征的Embanding值,再进行后续的模型定义,无需关注其他细节。我们的Sparse Arc支持单个特征查询多张Embanding表,如左图中的Food特征,支持序列特征的Pooling,如左图中的UserTopDPC特征,后续我们还将支持特征准入,特征淘汰,Embanding压缩等更高级的功能。如右图所示,数据读取进程负责数据下载,特征解析,特征处理等复杂操作,我们通过多线程的方式提升其处理能力,确保数据供给不会成为平静。此外,我们还尝试了对吸收部分和稠密部分进行流水线编排,增大计算和通信的overlap也取得了一定收益。第三个是kernel优化,通过集成NVIDIA提供的深度优化的Catalyst-based HSTU kernel,大幅提升了Attention的计算效率。单算子性能相较于TreatM版本提升2-3倍。我们可以看右下图,例如在A100GPU上,Catalyst版本相比TreatM版本加速比达到了3.25倍,提升非常明显。在我们的常整下,实测端到端训练吞吐也提升了85%。优化的kernel直接支持变长序列作为输入,无需pading,这对于推荐场景来说作用明显。我们以某个业务的实际场景为例,序列最大长度是3000,但是平均长度只有500。padingfree的特性能去除很多无用的计算。同时,HSTU的计算过程也比较细碎和复杂,通过算子融合技术可有效减少仿存,最大化计算吞吐。除了HSTU kernel的优化外,我们还根据业务需求,对group layer norm等算子进行了定制优化,也取得了一些提升,在此不再赘述。最后是变长序列负载均衡的油画。首先解释一下为什么会有这个问题,主要原因是我们的样本是按照用户维度去组织的,而在推荐系统中,用户序列呈现明显的长尾分布,少数用户的序列很长,大部分用户的序列都比较短,因此固定Batch Size训练时,每张卡的计算量差别比较大,计算量大的卡会拖慢整个训练流程,因为在同步训练的模式下,每个step都要等到所有卡计算完,才能进行梯度的同步。我们采取的解决思路也比较直观,就是引入动态Batch Size,每张卡的Batch Size,根据实际的数据序列长度动态调整,从而保证计算量,也就是总的Token数基本相同。我们以下面的图为例,假设有两张卡GPU0和GPU1进行同步训练,左边是固定Batch Size,右边是动态Batch Size,固定Batch Size等于40,很明显卡铃上的用户序列比较短,是Underload了,也就是负载不够,存在资源浪费的情况。切换到动态Batch Size后,比较短的用户序列会组成更大的Batch,比如Batch Size从4扩到5,从而保证所有的卡都是Fullload,也就是满载的,进而最大化系统的吞吐。如前所述,在我们长整下,端到端训练吞吐提升了30%,这里有个小Trick需要注意,那就是在动态Batch Size下,必须修改梯度聚合策略,按照LocalBatch Size对每张卡的梯度进行加全,这样才能保证计算逻辑的一致性。前面介绍了训练引擎,接下来介绍推理引擎,MTGR Inference。在推理框架方面,我们全面拥抱NVIDIA软件生态,选择Tensor RT作为模型推理框架,选择TrickInference Server作为模型部署框架,它们是NVIDIA官方推荐的推理解决方案,因为性能好,稳定性高,在业界广泛使用。下面是MTGR Inference的系统框图,最底层是硬件层,主要是配Invidial GPU,再上层是模型推理层,基于Tensor RT。中间层是我们引入的各种推理优化手段,包括特征H2D优化,CUDA Graph优化,HashiTable等。其中HashiTable面向推理场景进行了极致优化,阉割掉了写的功能,进一步提升了性能。最上层是模型部署层,基于TrickInference Server进行部署。下面详细介绍部分推理优化手段。首先是特征H2D优化,推荐模型一般都包含数百个特征,将特征从host读到GPU时,有较大的传输耗时。为了解决这个问题,我们采取了先merge,host split的操作,即在hostbuff中预先开辟一块连续的空间,将不同特征拷贝至这个空间,再统一进行一次H2D,从而充分利用传输带宽,传输到GPU上后,再进行split。这个方案虽然简单,但是收益比较显著,整体推理时延降低了37%,从19毫秒减少至12毫秒,吞吐提升了38%。其次是cudagraph的优化,cudagraph的原理如左图所示,通过将多个kernel build成一张graph,减少CPU上kernel launch的开销,从图上我们也看出,原本共有A,B,C,D,E,共五个kernel,有五次的launch,现在只需要一次graph的launch,就可以了。在我们的场景下,使用cudagraph优化后,吞吐提升了13%,不同QPS压力下,平均食盐也降低了17%到52%。第三个优化也比较常见,就是FP16计算。FP16推理具有以下的优势,首先是减少内存占用。FP16相比FP32内存占用减半,使得更大的模型可以在有限资源的硬件上运行。第二是提升计算速度,现代GPU对FP16的支持通常比FP32更好,能够更好地利用这些新硬件的特性。第三点是保持精度,虽然FP16的精度比较低,但通过适当的缩放和使录技术,可以有效地控制精度的损失。在我们的产检下,使用FP16格式进行推理,相比FP32吞吐提升了50%,精度几乎无损,相比FP32打风的Diff呢,能够控制在0.006以内,在我们业务接受的范围。最后一个优化是User Cache,目前我们仍在建设中,这个优化也是针对推荐场景的特点进行设计的。在推荐系统场景下,同一用户的不同Request之间,序列特征的绝大多数是相同的,只会不断open的最新的行为,例如用户9点请求了一次推荐系统,10点又请求了一次。这两次请求,绝大部分的用户特征是完全相同的。如果能复用序列中绝大部分的计算结果,只计算新增的Token,将极大降低计算开销,理论上计算复杂度将从ON方降低为ON。左下图是我们整个推理系统的执行流程。Predict是整个服务的入口,接受请求Request并返回结果Response。Feature Store是特征存储和计算服务,Model Serving是模型计算服务。我们新增了最左边的Cache Server,按照用户力度存储序列的计算结果,新请求到来时,首先通过UserID在Cache Server上查询,到UserCache,再同Fature Store上查询到的FatureTensor,一同传输给Model Serving进行模型的计算。由于大部分的计算结果可以从UserCache中获取,增量计算的开销较小。这个方案引入了额外的存储,且存储量级很大,如果设计不当可能会起到反作用,目前我们仍在开发中。整个MTGR的工程优化就介绍这么多,接下来请我的同事余磊继续分享后面的内容。下面我来介绍整体工作的总结和未来展望。总的来说,为了支持GR模型在美团的大规模应用落地,我们设计开发了一套使用简单,性能优异,灵活可拓展的模型训练推理引擎MTGR,为下一代搜推模型的迭代,打开了算力天花板。面向未来,我们准备在以下方向进行探索。从训练模式看,目前GR模型尚未支持实时学习,后续我们要将GR模型和实时学习结合起来。从性能角度看,通过Embiting压缩,低精度训练等优化手段,预计还有较大的性能提升空间。从模型规模看,我们准备借鉴大模型的训练方法,支持Dense模型的并行训练,进而支持更大规模的GR模型。从底层框架看,后续我们会尝试AOT Compile,通过编译优化手段,进一步提升PiTouch模型性能。借此机会,我们对NVIDIA团队表示感谢,特别是DAVTAC团队,柴斌,刘士杰,SA团队,侯一林等同学。他们给整体项目的建设和落地,提供了很多支持与帮助。关于更多的美团技术分享,欢迎大家扫码关注美团技术团队的公众号。最后打一个广告,我们团队长期专注于搜推场景的机器学习工程建设,包括CTR模型和大模型的模型训练和模型推理引擎,以及对应平台建设等多个方向。团队长期求闲,欢迎感兴趣的同学邮件联系。请不吝点赞 订阅 转发 打赏支持明镜与点点栏目